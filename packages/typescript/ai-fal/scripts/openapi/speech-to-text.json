{
  "openapi": "3.0.4",
  "info": {
    "title": "Fal.ai speech-to-text Models",
    "version": "1.0.0",
    "description": "OpenAPI schemas for all fal.ai speech-to-text models (9 models total)",
    "x-generated-at": "2026-01-26T05:40:51.973Z"
  },
  "components": {
    "schemas": {
      "File": {
        "type": "object",
        "properties": {
          "url": {
            "type": "string",
            "format": "uri"
          },
          "content_type": {
            "type": "string"
          },
          "file_name": {
            "type": "string"
          },
          "file_size": {
            "type": "integer"
          }
        },
        "required": [
          "url"
        ]
      },
      "QueueStatus": {
        "type": "object",
        "properties": {
          "status": {
            "type": "string",
            "enum": [
              "IN_PROGRESS",
              "COMPLETED",
              "FAILED"
            ]
          },
          "response_url": {
            "type": "string",
            "format": "uri"
          }
        },
        "required": [
          "status"
        ]
      },
      "ElevenlabsSpeechToTextScribeV2Input": {
        "title": "SpeechToTextRequestScribeV2",
        "type": "object",
        "properties": {
          "keyterms": {
            "description": "Words or sentences to bias the model towards transcribing. Up to 100 keyterms, max 50 characters each. Adds 30% premium over base transcription price.",
            "type": "array",
            "items": {
              "type": "string"
            },
            "examples": [
              [
                "fal.ai"
              ]
            ],
            "maxItems": 100,
            "title": "Keyterms",
            "default": []
          },
          "audio_url": {
            "examples": [
              "https://storage.googleapis.com/falserverless/example_inputs/elevenlabs/scribe_v2_in.mp3"
            ],
            "description": "URL of the audio file to transcribe",
            "type": "string",
            "title": "Audio Url"
          },
          "diarize": {
            "description": "Whether to annotate who is speaking",
            "type": "boolean",
            "title": "Diarize",
            "default": true
          },
          "language_code": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "Language code of the audio",
            "title": "Language Code",
            "examples": [
              "eng",
              "spa",
              "fra",
              "deu",
              "jpn"
            ]
          },
          "tag_audio_events": {
            "description": "Tag audio events like laughter, applause, etc.",
            "type": "boolean",
            "title": "Tag Audio Events",
            "default": true
          }
        },
        "x-fal-order-properties": [
          "audio_url",
          "language_code",
          "tag_audio_events",
          "diarize",
          "keyterms"
        ],
        "required": [
          "audio_url"
        ]
      },
      "ElevenlabsSpeechToTextScribeV2Output": {
        "title": "TranscriptionOutputV2",
        "type": "object",
        "properties": {
          "text": {
            "examples": [
              "Hey, this is a test recording for Scribe version two, which is now available on fal.ai."
            ],
            "description": "The full transcribed text",
            "type": "string",
            "title": "Text"
          },
          "language_probability": {
            "examples": [
              1
            ],
            "description": "Confidence in language detection",
            "type": "number",
            "title": "Language Probability"
          },
          "language_code": {
            "examples": [
              "eng"
            ],
            "description": "Detected or specified language code",
            "type": "string",
            "title": "Language Code"
          },
          "words": {
            "examples": [
              {
                "text": "Hey,",
                "start": 0.079,
                "type": "word",
                "end": 0.539,
                "speaker_id": "speaker_0"
              },
              {
                "text": " ",
                "start": 0.539,
                "type": "spacing",
                "end": 0.599,
                "speaker_id": "speaker_0"
              },
              {
                "text": "this",
                "start": 0.599,
                "type": "word",
                "end": 0.679,
                "speaker_id": "speaker_0"
              },
              {
                "text": " ",
                "start": 0.679,
                "type": "spacing",
                "end": 0.739,
                "speaker_id": "speaker_0"
              },
              {
                "text": "is",
                "start": 0.739,
                "type": "word",
                "end": 0.799,
                "speaker_id": "speaker_0"
              },
              {
                "text": " ",
                "start": 0.799,
                "type": "spacing",
                "end": 0.939,
                "speaker_id": "speaker_0"
              },
              {
                "text": "a",
                "start": 0.939,
                "type": "word",
                "end": 0.939,
                "speaker_id": "speaker_0"
              },
              {
                "text": " ",
                "start": 0.939,
                "type": "spacing",
                "end": 0.959,
                "speaker_id": "speaker_0"
              },
              {
                "text": "test",
                "start": 0.959,
                "type": "word",
                "end": 1.179,
                "speaker_id": "speaker_0"
              },
              {
                "text": " ",
                "start": 1.179,
                "type": "spacing",
                "end": 1.219,
                "speaker_id": "speaker_0"
              },
              {
                "text": "recording",
                "start": 1.22,
                "type": "word",
                "end": 1.719,
                "speaker_id": "speaker_0"
              },
              {
                "text": " ",
                "start": 1.719,
                "type": "spacing",
                "end": 1.719,
                "speaker_id": "speaker_0"
              },
              {
                "text": "for",
                "start": 1.719,
                "type": "word",
                "end": 1.86,
                "speaker_id": "speaker_0"
              },
              {
                "text": " ",
                "start": 1.86,
                "type": "spacing",
                "end": 1.879,
                "speaker_id": "speaker_0"
              },
              {
                "text": "Scribe",
                "start": 1.879,
                "type": "word",
                "end": 2.24,
                "speaker_id": "speaker_0"
              },
              {
                "text": " ",
                "start": 2.24,
                "type": "spacing",
                "end": 2.319,
                "speaker_id": "speaker_0"
              },
              {
                "text": "version",
                "start": 2.319,
                "type": "word",
                "end": 2.759,
                "speaker_id": "speaker_0"
              },
              {
                "text": " ",
                "start": 2.759,
                "type": "spacing",
                "end": 2.779,
                "speaker_id": "speaker_0"
              },
              {
                "text": "two,",
                "start": 2.779,
                "type": "word",
                "end": 3.379,
                "speaker_id": "speaker_0"
              },
              {
                "text": " ",
                "start": 3.379,
                "type": "spacing",
                "end": 3.399,
                "speaker_id": "speaker_0"
              },
              {
                "text": "which",
                "start": 3.399,
                "type": "word",
                "end": 3.519,
                "speaker_id": "speaker_0"
              },
              {
                "text": " ",
                "start": 3.519,
                "type": "spacing",
                "end": 3.539,
                "speaker_id": "speaker_0"
              },
              {
                "text": "is",
                "start": 3.539,
                "type": "word",
                "end": 3.659,
                "speaker_id": "speaker_0"
              },
              {
                "text": " ",
                "start": 3.659,
                "type": "spacing",
                "end": 3.699,
                "speaker_id": "speaker_0"
              },
              {
                "text": "now",
                "start": 3.699,
                "type": "word",
                "end": 3.839,
                "speaker_id": "speaker_0"
              },
              {
                "text": " ",
                "start": 3.839,
                "type": "spacing",
                "end": 3.839,
                "speaker_id": "speaker_0"
              },
              {
                "text": "available",
                "start": 3.839,
                "type": "word",
                "end": 4.319,
                "speaker_id": "speaker_0"
              },
              {
                "text": " ",
                "start": 4.319,
                "type": "spacing",
                "end": 4.339,
                "speaker_id": "speaker_0"
              },
              {
                "text": "on",
                "start": 4.339,
                "type": "word",
                "end": 4.579,
                "speaker_id": "speaker_0"
              },
              {
                "text": " ",
                "start": 4.579,
                "type": "spacing",
                "end": 4.599,
                "speaker_id": "speaker_0"
              },
              {
                "text": "fal.ai.",
                "start": 4.599,
                "type": "word",
                "end": 5.699,
                "speaker_id": "speaker_0"
              }
            ],
            "description": "Word-level transcription details",
            "type": "array",
            "title": "Words",
            "items": {
              "$ref": "#/components/schemas/TranscriptionWord"
            }
          }
        },
        "x-fal-order-properties": [
          "text",
          "language_code",
          "language_probability",
          "words"
        ],
        "required": [
          "text",
          "language_code",
          "language_probability",
          "words"
        ]
      },
      "TranscriptionWord": {
        "title": "TranscriptionWord",
        "type": "object",
        "properties": {
          "text": {
            "description": "The transcribed word or audio event",
            "type": "string",
            "title": "Text"
          },
          "start": {
            "anyOf": [
              {
                "type": "number"
              },
              {
                "type": "null"
              }
            ],
            "description": "Start time in seconds",
            "title": "Start"
          },
          "type": {
            "description": "Type of element (word, spacing, or audio_event)",
            "type": "string",
            "title": "Type"
          },
          "end": {
            "anyOf": [
              {
                "type": "number"
              },
              {
                "type": "null"
              }
            ],
            "description": "End time in seconds",
            "title": "End"
          },
          "speaker_id": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "Speaker identifier if diarization was enabled",
            "title": "Speaker Id"
          }
        },
        "x-fal-order-properties": [
          "text",
          "start",
          "end",
          "type",
          "speaker_id"
        ],
        "required": [
          "text",
          "start",
          "end",
          "type"
        ]
      },
      "SmartTurnInput": {
        "title": "SmartTurnInput",
        "type": "object",
        "properties": {
          "audio_url": {
            "examples": [
              "https://fal.media/files/panda/5-QaAOC32rB_hqWaVdqEH.mpga"
            ],
            "title": "Audio Url",
            "type": "string",
            "description": "The URL of the audio file to be processed."
          }
        },
        "x-fal-order-properties": [
          "audio_url"
        ],
        "required": [
          "audio_url"
        ]
      },
      "SmartTurnOutput": {
        "title": "Output",
        "type": "object",
        "properties": {
          "prediction": {
            "title": "Prediction",
            "type": "integer",
            "description": "The predicted turn type. 1 for Complete, 0 for Incomplete."
          },
          "probability": {
            "title": "Probability",
            "type": "number",
            "description": "The probability of the predicted turn type."
          },
          "metrics": {
            "title": "Metrics",
            "type": "object",
            "description": "The metrics of the inference."
          }
        },
        "x-fal-order-properties": [
          "prediction",
          "probability",
          "metrics"
        ],
        "required": [
          "prediction",
          "probability",
          "metrics"
        ]
      },
      "SpeechToTextTurboInput": {
        "title": "SpeechInput",
        "type": "object",
        "properties": {
          "audio_url": {
            "examples": [
              "https://storage.googleapis.com/falserverless/canary/18e15559-ab3e-4f96-9583-be5ddde91e43.mp3"
            ],
            "title": "Audio Path",
            "type": "string",
            "description": "Local filesystem path (or remote URL) to a long audio file"
          },
          "use_pnc": {
            "title": "Use Punctuation/Capitalization (PnC)",
            "type": "boolean",
            "description": "Whether to use Canary's built-in punctuation & capitalization",
            "default": true
          }
        },
        "x-fal-order-properties": [
          "audio_url",
          "use_pnc"
        ],
        "required": [
          "audio_url"
        ]
      },
      "SpeechToTextTurboOutput": {
        "title": "SpeechOutput",
        "type": "object",
        "properties": {
          "partial": {
            "title": "Partial",
            "type": "boolean",
            "description": "Indicates if this is a partial (in-progress) transcript",
            "default": false
          },
          "output": {
            "title": "Transcribed Text",
            "type": "string",
            "description": "The partial or final transcription output from Canary"
          }
        },
        "x-fal-order-properties": [
          "output",
          "partial"
        ],
        "required": [
          "output"
        ]
      },
      "SpeechToTextTurboStreamInput": {
        "title": "SpeechInput",
        "type": "object",
        "properties": {
          "audio_url": {
            "examples": [
              "https://storage.googleapis.com/falserverless/canary/18e15559-ab3e-4f96-9583-be5ddde91e43.mp3"
            ],
            "title": "Audio Path",
            "type": "string",
            "description": "Local filesystem path (or remote URL) to a long audio file"
          },
          "use_pnc": {
            "title": "Use Punctuation/Capitalization (PnC)",
            "type": "boolean",
            "description": "Whether to use Canary's built-in punctuation & capitalization",
            "default": true
          }
        },
        "x-fal-order-properties": [
          "audio_url",
          "use_pnc"
        ],
        "required": [
          "audio_url"
        ]
      },
      "SpeechToTextTurboStreamOutput": {},
      "SpeechToTextStreamInput": {
        "title": "SpeechInput",
        "type": "object",
        "properties": {
          "audio_url": {
            "examples": [
              "https://storage.googleapis.com/falserverless/canary/18e15559-ab3e-4f96-9583-be5ddde91e43.mp3"
            ],
            "title": "Audio Path",
            "type": "string",
            "description": "Local filesystem path (or remote URL) to a long audio file"
          },
          "use_pnc": {
            "title": "Use Punctuation/Capitalization (PnC)",
            "type": "boolean",
            "description": "Whether to use Canary's built-in punctuation & capitalization",
            "default": true
          }
        },
        "x-fal-order-properties": [
          "audio_url",
          "use_pnc"
        ],
        "required": [
          "audio_url"
        ]
      },
      "SpeechToTextStreamOutput": {},
      "SpeechToTextInput": {
        "title": "SpeechInput",
        "type": "object",
        "properties": {
          "audio_url": {
            "examples": [
              "https://storage.googleapis.com/falserverless/canary/18e15559-ab3e-4f96-9583-be5ddde91e43.mp3"
            ],
            "title": "Audio Path",
            "type": "string",
            "description": "Local filesystem path (or remote URL) to a long audio file"
          },
          "use_pnc": {
            "title": "Use Punctuation/Capitalization (PnC)",
            "type": "boolean",
            "description": "Whether to use Canary's built-in punctuation & capitalization",
            "default": true
          }
        },
        "x-fal-order-properties": [
          "audio_url",
          "use_pnc"
        ],
        "required": [
          "audio_url"
        ]
      },
      "SpeechToTextOutput": {
        "title": "SpeechOutput",
        "type": "object",
        "properties": {
          "partial": {
            "title": "Partial",
            "type": "boolean",
            "description": "Indicates if this is a partial (in-progress) transcript",
            "default": false
          },
          "output": {
            "title": "Transcribed Text",
            "type": "string",
            "description": "The partial or final transcription output from Canary"
          }
        },
        "x-fal-order-properties": [
          "output",
          "partial"
        ],
        "required": [
          "output"
        ]
      },
      "ElevenlabsSpeechToTextInput": {
        "title": "SpeechToTextRequest",
        "type": "object",
        "properties": {
          "language_code": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "Language code of the audio",
            "title": "Language Code",
            "examples": [
              "eng",
              "spa",
              "fra",
              "deu",
              "jpn"
            ]
          },
          "audio_url": {
            "examples": [
              "https://v3.fal.media/files/zebra/zJL_oRY8h5RWwjoK1w7tx_output.mp3"
            ],
            "description": "URL of the audio file to transcribe",
            "type": "string",
            "title": "Audio Url"
          },
          "diarize": {
            "description": "Whether to annotate who is speaking",
            "type": "boolean",
            "title": "Diarize",
            "default": true
          },
          "tag_audio_events": {
            "description": "Tag audio events like laughter, applause, etc.",
            "type": "boolean",
            "title": "Tag Audio Events",
            "default": true
          }
        },
        "x-fal-order-properties": [
          "audio_url",
          "language_code",
          "tag_audio_events",
          "diarize"
        ],
        "required": [
          "audio_url"
        ]
      },
      "ElevenlabsSpeechToTextOutput": {
        "title": "TranscriptionOutput",
        "type": "object",
        "properties": {
          "text": {
            "description": "The full transcribed text",
            "type": "string",
            "title": "Text"
          },
          "language_probability": {
            "description": "Confidence in language detection",
            "type": "number",
            "title": "Language Probability"
          },
          "language_code": {
            "description": "Detected or specified language code",
            "type": "string",
            "title": "Language Code"
          },
          "words": {
            "description": "Word-level transcription details",
            "type": "array",
            "title": "Words",
            "items": {
              "$ref": "#/components/schemas/FalAiElevenlabsSpeechToText_TranscriptionWord"
            }
          }
        },
        "x-fal-order-properties": [
          "text",
          "language_code",
          "language_probability",
          "words"
        ],
        "required": [
          "text",
          "language_code",
          "language_probability",
          "words"
        ]
      },
      "FalAiElevenlabsSpeechToText_TranscriptionWord": {
        "title": "TranscriptionWord",
        "type": "object",
        "properties": {
          "text": {
            "description": "The transcribed word or audio event",
            "type": "string",
            "title": "Text"
          },
          "start": {
            "anyOf": [
              {
                "type": "number"
              },
              {
                "type": "null"
              }
            ],
            "description": "Start time in seconds",
            "title": "Start"
          },
          "type": {
            "description": "Type of element (word, spacing, or audio_event)",
            "type": "string",
            "title": "Type"
          },
          "end": {
            "anyOf": [
              {
                "type": "number"
              },
              {
                "type": "null"
              }
            ],
            "description": "End time in seconds",
            "title": "End"
          },
          "speaker_id": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "Speaker identifier if diarization was enabled",
            "title": "Speaker Id"
          }
        },
        "x-fal-order-properties": [
          "text",
          "start",
          "end",
          "type",
          "speaker_id"
        ],
        "required": [
          "text",
          "start",
          "end",
          "type"
        ]
      },
      "WizperInput": {
        "title": "WhisperInput",
        "type": "object",
        "properties": {
          "language": {
            "anyOf": [
              {
                "enum": [
                  "af",
                  "am",
                  "ar",
                  "as",
                  "az",
                  "ba",
                  "be",
                  "bg",
                  "bn",
                  "bo",
                  "br",
                  "bs",
                  "ca",
                  "cs",
                  "cy",
                  "da",
                  "de",
                  "el",
                  "en",
                  "es",
                  "et",
                  "eu",
                  "fa",
                  "fi",
                  "fo",
                  "fr",
                  "gl",
                  "gu",
                  "ha",
                  "haw",
                  "he",
                  "hi",
                  "hr",
                  "ht",
                  "hu",
                  "hy",
                  "id",
                  "is",
                  "it",
                  "ja",
                  "jw",
                  "ka",
                  "kk",
                  "km",
                  "kn",
                  "ko",
                  "la",
                  "lb",
                  "ln",
                  "lo",
                  "lt",
                  "lv",
                  "mg",
                  "mi",
                  "mk",
                  "ml",
                  "mn",
                  "mr",
                  "ms",
                  "mt",
                  "my",
                  "ne",
                  "nl",
                  "nn",
                  "no",
                  "oc",
                  "pa",
                  "pl",
                  "ps",
                  "pt",
                  "ro",
                  "ru",
                  "sa",
                  "sd",
                  "si",
                  "sk",
                  "sl",
                  "sn",
                  "so",
                  "sq",
                  "sr",
                  "su",
                  "sv",
                  "sw",
                  "ta",
                  "te",
                  "tg",
                  "th",
                  "tk",
                  "tl",
                  "tr",
                  "tt",
                  "uk",
                  "ur",
                  "uz",
                  "vi",
                  "yi",
                  "yo",
                  "zh"
                ],
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Language",
            "examples": [
              null
            ],
            "description": "\n        Language of the audio file.\n        If translate is selected as the task, the audio will be translated to\n        English, regardless of the language selected. If `None` is passed,\n        the language will be automatically detected. This will also increase\n        the inference time.\n        ",
            "default": "en"
          },
          "version": {
            "title": "Version",
            "type": "string",
            "description": "Version of the model to use. All of the models are the Whisper large variant.",
            "const": "3",
            "default": "3"
          },
          "max_segment_len": {
            "minimum": 10,
            "maximum": 29,
            "type": "integer",
            "title": "Max Segment Len",
            "description": "Maximum speech segment duration in seconds before splitting.",
            "default": 29
          },
          "task": {
            "enum": [
              "transcribe",
              "translate"
            ],
            "title": "Task",
            "type": "string",
            "description": "Task to perform on the audio file. Either transcribe or translate.",
            "default": "transcribe"
          },
          "chunk_level": {
            "title": "Chunk Level",
            "type": "string",
            "description": "Level of the chunks to return.",
            "const": "segment",
            "default": "segment"
          },
          "audio_url": {
            "examples": [
              "https://ihlhivqvotguuqycfcvj.supabase.co/storage/v1/object/public/public-text-to-speech/scratch-testing/earth-history-19mins.mp3"
            ],
            "title": "Audio Url",
            "type": "string",
            "description": "URL of the audio file to transcribe. Supported formats: mp3, mp4, mpeg, mpga, m4a, wav or webm."
          },
          "merge_chunks": {
            "title": "Merge Chunks",
            "type": "boolean",
            "description": "Whether to merge consecutive chunks. When enabled, chunks are merged if their combined duration does not exceed max_segment_len.",
            "default": true
          }
        },
        "x-fal-order-properties": [
          "audio_url",
          "task",
          "language",
          "chunk_level",
          "max_segment_len",
          "merge_chunks",
          "version"
        ],
        "required": [
          "audio_url"
        ]
      },
      "WizperOutput": {
        "title": "WhisperOutput",
        "type": "object",
        "properties": {
          "text": {
            "title": "Text",
            "type": "string",
            "description": "Transcription of the audio file"
          },
          "languages": {
            "title": "Languages",
            "type": "array",
            "description": "List of languages that the audio file is inferred to be. Defaults to null.",
            "items": {
              "enum": [
                "af",
                "am",
                "ar",
                "as",
                "az",
                "ba",
                "be",
                "bg",
                "bn",
                "bo",
                "br",
                "bs",
                "ca",
                "cs",
                "cy",
                "da",
                "de",
                "el",
                "en",
                "es",
                "et",
                "eu",
                "fa",
                "fi",
                "fo",
                "fr",
                "gl",
                "gu",
                "ha",
                "haw",
                "he",
                "hi",
                "hr",
                "ht",
                "hu",
                "hy",
                "id",
                "is",
                "it",
                "ja",
                "jw",
                "ka",
                "kk",
                "km",
                "kn",
                "ko",
                "la",
                "lb",
                "ln",
                "lo",
                "lt",
                "lv",
                "mg",
                "mi",
                "mk",
                "ml",
                "mn",
                "mr",
                "ms",
                "mt",
                "my",
                "ne",
                "nl",
                "nn",
                "no",
                "oc",
                "pa",
                "pl",
                "ps",
                "pt",
                "ro",
                "ru",
                "sa",
                "sd",
                "si",
                "sk",
                "sl",
                "sn",
                "so",
                "sq",
                "sr",
                "su",
                "sv",
                "sw",
                "ta",
                "te",
                "tg",
                "th",
                "tk",
                "tl",
                "tr",
                "tt",
                "uk",
                "ur",
                "uz",
                "vi",
                "yi",
                "yo",
                "zh"
              ],
              "type": "string"
            }
          },
          "chunks": {
            "title": "Chunks",
            "type": "array",
            "description": "Timestamp chunks of the audio file",
            "items": {
              "$ref": "#/components/schemas/WhisperChunk"
            }
          }
        },
        "x-fal-order-properties": [
          "text",
          "chunks",
          "languages"
        ],
        "required": [
          "text",
          "chunks",
          "languages"
        ]
      },
      "WhisperChunk": {
        "title": "WhisperChunk",
        "type": "object",
        "properties": {
          "text": {
            "title": "Text",
            "type": "string",
            "description": "Transcription of the chunk"
          },
          "timestamp": {
            "maxItems": 2,
            "type": "array",
            "minItems": 2,
            "title": "Timestamp",
            "prefixItems": [
              {
                "anyOf": [
                  {
                    "type": "number"
                  },
                  {
                    "type": "null"
                  }
                ]
              },
              {
                "anyOf": [
                  {
                    "type": "number"
                  },
                  {
                    "type": "null"
                  }
                ]
              }
            ],
            "description": "Start and end timestamp of the chunk"
          }
        },
        "x-fal-order-properties": [
          "timestamp",
          "text"
        ],
        "required": [
          "timestamp",
          "text"
        ]
      },
      "WhisperInput": {
        "x-fal-order-properties": [
          "audio_url",
          "task",
          "language",
          "diarize",
          "chunk_level",
          "version",
          "batch_size",
          "prompt",
          "num_speakers"
        ],
        "type": "object",
        "properties": {
          "version": {
            "enum": [
              "3"
            ],
            "title": "Version",
            "type": "string",
            "description": "Version of the model to use. All of the models are the Whisper large variant.",
            "default": "3"
          },
          "batch_size": {
            "examples": [
              64
            ],
            "maximum": 64,
            "type": "integer",
            "minimum": 1,
            "title": "Batch Size",
            "default": 64
          },
          "language": {
            "enum": [
              "af",
              "am",
              "ar",
              "as",
              "az",
              "ba",
              "be",
              "bg",
              "bn",
              "bo",
              "br",
              "bs",
              "ca",
              "cs",
              "cy",
              "da",
              "de",
              "el",
              "en",
              "es",
              "et",
              "eu",
              "fa",
              "fi",
              "fo",
              "fr",
              "gl",
              "gu",
              "ha",
              "haw",
              "he",
              "hi",
              "hr",
              "ht",
              "hu",
              "hy",
              "id",
              "is",
              "it",
              "ja",
              "jw",
              "ka",
              "kk",
              "km",
              "kn",
              "ko",
              "la",
              "lb",
              "ln",
              "lo",
              "lt",
              "lv",
              "mg",
              "mi",
              "mk",
              "ml",
              "mn",
              "mr",
              "ms",
              "mt",
              "my",
              "ne",
              "nl",
              "nn",
              "no",
              "oc",
              "pa",
              "pl",
              "ps",
              "pt",
              "ro",
              "ru",
              "sa",
              "sd",
              "si",
              "sk",
              "sl",
              "sn",
              "so",
              "sq",
              "sr",
              "su",
              "sv",
              "sw",
              "ta",
              "te",
              "tg",
              "th",
              "tk",
              "tl",
              "tr",
              "tt",
              "uk",
              "ur",
              "uz",
              "vi",
              "yi",
              "yo",
              "zh"
            ],
            "title": "Language",
            "type": "string",
            "description": "\n        Language of the audio file. If set to null, the language will be\n        automatically detected. Defaults to null.\n\n        If translate is selected as the task, the audio will be translated to\n        English, regardless of the language selected.\n        ",
            "nullable": true
          },
          "prompt": {
            "title": "Prompt",
            "type": "string",
            "description": "Prompt to use for generation. Defaults to an empty string.",
            "default": ""
          },
          "num_speakers": {
            "examples": [
              null
            ],
            "title": "Num Speakers",
            "type": "integer",
            "minimum": 1,
            "description": "\n            Number of speakers in the audio file. Defaults to null.\n            If not provided, the number of speakers will be automatically\n            detected.\n        ",
            "nullable": true
          },
          "task": {
            "enum": [
              "transcribe",
              "translate"
            ],
            "title": "Task",
            "type": "string",
            "description": "Task to perform on the audio file. Either transcribe or translate.",
            "default": "transcribe"
          },
          "chunk_level": {
            "enum": [
              "none",
              "segment",
              "word"
            ],
            "title": "Chunk Level",
            "type": "string",
            "description": "Level of the chunks to return. Either none, segment or word. `none` would imply that all of the audio will be transcribed without the timestamp tokens, we suggest to switch to `none` if you are not satisfied with the transcription quality, since it will usually improve the quality of the results. Switching to `none` will also provide minor speed ups in the transcription due to less amount of generated tokens. Notice that setting to none will produce **a single chunk with the whole transcription**.",
            "default": "segment"
          },
          "audio_url": {
            "examples": [
              "https://storage.googleapis.com/falserverless/model_tests/whisper/dinner_conversation.mp3"
            ],
            "title": "Audio Url",
            "type": "string",
            "description": "URL of the audio file to transcribe. Supported formats: mp3, mp4, mpeg, mpga, m4a, wav or webm."
          },
          "diarize": {
            "title": "Diarize",
            "type": "boolean",
            "description": "Whether to diarize the audio file. Defaults to false. Setting to true will add costs proportional to diarization inference time.",
            "default": false
          }
        },
        "title": "WhisperInput",
        "required": [
          "audio_url"
        ]
      },
      "WhisperOutput": {
        "x-fal-order-properties": [
          "text",
          "chunks",
          "inferred_languages",
          "diarization_segments"
        ],
        "type": "object",
        "properties": {
          "text": {
            "title": "Text",
            "type": "string",
            "description": "Transcription of the audio file"
          },
          "inferred_languages": {
            "title": "Inferred Languages",
            "type": "array",
            "description": "List of languages that the audio file is inferred to be. Defaults to null.",
            "items": {
              "enum": [
                "af",
                "am",
                "ar",
                "as",
                "az",
                "ba",
                "be",
                "bg",
                "bn",
                "bo",
                "br",
                "bs",
                "ca",
                "cs",
                "cy",
                "da",
                "de",
                "el",
                "en",
                "es",
                "et",
                "eu",
                "fa",
                "fi",
                "fo",
                "fr",
                "gl",
                "gu",
                "ha",
                "haw",
                "he",
                "hi",
                "hr",
                "ht",
                "hu",
                "hy",
                "id",
                "is",
                "it",
                "ja",
                "jw",
                "ka",
                "kk",
                "km",
                "kn",
                "ko",
                "la",
                "lb",
                "ln",
                "lo",
                "lt",
                "lv",
                "mg",
                "mi",
                "mk",
                "ml",
                "mn",
                "mr",
                "ms",
                "mt",
                "my",
                "ne",
                "nl",
                "nn",
                "no",
                "oc",
                "pa",
                "pl",
                "ps",
                "pt",
                "ro",
                "ru",
                "sa",
                "sd",
                "si",
                "sk",
                "sl",
                "sn",
                "so",
                "sq",
                "sr",
                "su",
                "sv",
                "sw",
                "ta",
                "te",
                "tg",
                "th",
                "tk",
                "tl",
                "tr",
                "tt",
                "uk",
                "ur",
                "uz",
                "vi",
                "yi",
                "yo",
                "zh"
              ],
              "type": "string"
            }
          },
          "chunks": {
            "title": "Chunks",
            "type": "array",
            "description": "Timestamp chunks of the audio file",
            "items": {
              "$ref": "#/components/schemas/FalAiWhisper_WhisperChunk"
            }
          },
          "diarization_segments": {
            "title": "Diarization Segments",
            "type": "array",
            "description": "Speaker diarization segments of the audio file. Only present if diarization is enabled.",
            "items": {
              "$ref": "#/components/schemas/DiarizationSegment"
            }
          }
        },
        "title": "WhisperOutput",
        "required": [
          "text",
          "inferred_languages",
          "diarization_segments"
        ]
      },
      "FalAiWhisper_WhisperChunk": {
        "x-fal-order-properties": [
          "timestamp",
          "text",
          "speaker"
        ],
        "type": "object",
        "properties": {
          "text": {
            "title": "Text",
            "type": "string",
            "description": "Transcription of the chunk"
          },
          "timestamp": {
            "maxItems": 2,
            "type": "array",
            "minItems": 2,
            "title": "Timestamp",
            "description": "Start and end timestamp of the chunk",
            "items": {
              "0": {
                "type": "number"
              },
              "1": {
                "type": "number"
              }
            }
          },
          "speaker": {
            "title": "Speaker",
            "type": "string",
            "description": "Speaker ID of the chunk. Only present if diarization is enabled."
          }
        },
        "title": "WhisperChunk",
        "required": [
          "timestamp",
          "text"
        ]
      },
      "DiarizationSegment": {
        "x-fal-order-properties": [
          "timestamp",
          "speaker"
        ],
        "type": "object",
        "properties": {
          "timestamp": {
            "maxItems": 2,
            "type": "array",
            "minItems": 2,
            "title": "Timestamp",
            "description": "Start and end timestamp of the segment",
            "items": {
              "0": {
                "type": "number"
              },
              "1": {
                "type": "number"
              }
            }
          },
          "speaker": {
            "title": "Speaker",
            "type": "string",
            "description": "Speaker ID of the segment"
          }
        },
        "title": "DiarizationSegment",
        "required": [
          "timestamp",
          "speaker"
        ]
      }
    },
    "securitySchemes": {
      "apiKeyAuth": {
        "type": "apiKey",
        "in": "header",
        "name": "Authorization"
      }
    }
  }
}