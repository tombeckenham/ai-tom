{
  "openapi": "3.0.4",
  "info": {
    "title": "Fal.ai training Models",
    "version": "1.0.0",
    "description": "OpenAPI schemas for all fal.ai training models (33 models total)",
    "x-generated-at": "2026-01-26T05:40:51.961Z"
  },
  "components": {
    "schemas": {
      "File": {
        "type": "object",
        "properties": {
          "url": {
            "type": "string",
            "format": "uri"
          },
          "content_type": {
            "type": "string"
          },
          "file_name": {
            "type": "string"
          },
          "file_size": {
            "type": "integer"
          }
        },
        "required": [
          "url"
        ]
      },
      "QueueStatus": {
        "type": "object",
        "properties": {
          "status": {
            "type": "string",
            "enum": [
              "IN_PROGRESS",
              "COMPLETED",
              "FAILED"
            ]
          },
          "response_url": {
            "type": "string",
            "format": "uri"
          }
        },
        "required": [
          "status"
        ]
      },
      "FluxKreaTrainerInput": {
        "title": "PublicInput",
        "type": "object",
        "properties": {
          "images_data_url": {
            "title": "Images Data Url",
            "type": "string",
            "description": "\n        URL to zip archive with images. Try to use at least 4 images in general the more the better.\n\n        In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.\n    "
          },
          "is_input_format_already_preprocessed": {
            "title": "Is Input Format Already Preprocessed",
            "type": "boolean",
            "description": "Specifies whether the input data is already in a processed format. When set to False (default), the system expects raw input where image files and their corresponding caption files share the same name (e.g., 'photo.jpg' and 'photo.txt'). Set to True if your data is already in a preprocessed format.",
            "default": false
          },
          "trigger_word": {
            "title": "Trigger Word",
            "type": "string",
            "description": "Trigger word to be used in the captions. If None, a trigger word will not be used.\n        If no captions are provide the trigger_word will be used instead of captions. If captions are the trigger word will not be used.\n        ",
            "nullable": true
          },
          "steps": {
            "description": "Number of steps to train the LoRA on.",
            "type": "integer",
            "examples": [
              1000
            ],
            "maximum": 10000,
            "title": "Steps",
            "minimum": 1
          },
          "data_archive_format": {
            "title": "Data Archive Format",
            "type": "string",
            "description": "The format of the archive. If not specified, the format will be inferred from the URL.",
            "nullable": true
          },
          "is_style": {
            "title": "Is Style",
            "type": "boolean",
            "description": "If True, the training will be for a style. This will deactivate segmentation, captioning and will use trigger word instead. Use the trigger word to specify the style.",
            "default": false
          },
          "create_masks": {
            "title": "Create Masks",
            "type": "boolean",
            "description": "If True segmentation masks will be used in the weight the training loss. For people a face mask is used if possible.",
            "default": true
          }
        },
        "x-fal-order-properties": [
          "images_data_url",
          "trigger_word",
          "create_masks",
          "steps",
          "is_style",
          "is_input_format_already_preprocessed",
          "data_archive_format"
        ],
        "required": [
          "images_data_url"
        ]
      },
      "FluxKreaTrainerOutput": {
        "title": "Output",
        "type": "object",
        "properties": {
          "config_file": {
            "title": "Config File",
            "description": "URL to the training configuration file.",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiFluxKreaTrainer_File"
              }
            ]
          },
          "debug_preprocessed_output": {
            "title": "Debug Preprocessed Output",
            "description": "URL to the preprocessed images.",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiFluxKreaTrainer_File"
              }
            ]
          },
          "diffusers_lora_file": {
            "title": "Diffusers Lora File",
            "description": "URL to the trained diffusers lora weights.",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiFluxKreaTrainer_File"
              }
            ]
          }
        },
        "x-fal-order-properties": [
          "diffusers_lora_file",
          "config_file",
          "debug_preprocessed_output"
        ],
        "required": [
          "diffusers_lora_file",
          "config_file"
        ]
      },
      "FalAiFluxKreaTrainer_File": {
        "title": "File",
        "type": "object",
        "properties": {
          "file_size": {
            "examples": [
              4404019
            ],
            "title": "File Size",
            "type": "integer",
            "description": "The size of the file in bytes."
          },
          "file_name": {
            "examples": [
              "z9RV14K95DvU.png"
            ],
            "title": "File Name",
            "type": "string",
            "description": "The name of the file. It will be auto-generated if not provided."
          },
          "content_type": {
            "examples": [
              "image/png"
            ],
            "title": "Content Type",
            "type": "string",
            "description": "The mime type of the file."
          },
          "url": {
            "title": "Url",
            "type": "string",
            "description": "The URL where the file can be downloaded from."
          },
          "file_data": {
            "format": "binary",
            "title": "File Data",
            "type": "string",
            "description": "File data"
          }
        },
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size",
          "file_data"
        ],
        "required": [
          "url"
        ]
      },
      "FluxLoraFastTrainingInput": {
        "title": "PublicInput",
        "type": "object",
        "properties": {
          "images_data_url": {
            "title": "Images Data Url",
            "type": "string",
            "description": "\n        URL to zip archive with images. Try to use at least 4 images in general the more the better.\n\n        In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.\n    "
          },
          "is_input_format_already_preprocessed": {
            "title": "Is Input Format Already Preprocessed",
            "type": "boolean",
            "description": "Specifies whether the input data is already in a processed format. When set to False (default), the system expects raw input where image files and their corresponding caption files share the same name (e.g., 'photo.jpg' and 'photo.txt'). Set to True if your data is already in a preprocessed format.",
            "default": false
          },
          "trigger_word": {
            "title": "Trigger Word",
            "type": "string",
            "description": "Trigger word to be used in the captions. If None, a trigger word will not be used.\n        If no captions are provide the trigger_word will be used instead of captions. If captions are the trigger word will not be used.\n        ",
            "nullable": true
          },
          "steps": {
            "description": "Number of steps to train the LoRA on.",
            "type": "integer",
            "minimum": 1,
            "maximum": 10000,
            "examples": [
              1000
            ],
            "title": "Steps"
          },
          "data_archive_format": {
            "title": "Data Archive Format",
            "type": "string",
            "description": "The format of the archive. If not specified, the format will be inferred from the URL.",
            "nullable": true
          },
          "is_style": {
            "title": "Is Style",
            "type": "boolean",
            "description": "If True, the training will be for a style. This will deactivate segmentation, captioning and will use trigger word instead. Use the trigger word to specify the style.",
            "default": false
          },
          "create_masks": {
            "title": "Create Masks",
            "type": "boolean",
            "description": "If True segmentation masks will be used in the weight the training loss. For people a face mask is used if possible.",
            "default": true
          }
        },
        "x-fal-order-properties": [
          "images_data_url",
          "trigger_word",
          "create_masks",
          "steps",
          "is_style",
          "is_input_format_already_preprocessed",
          "data_archive_format"
        ],
        "required": [
          "images_data_url"
        ]
      },
      "FluxLoraFastTrainingOutput": {
        "title": "Output",
        "type": "object",
        "properties": {
          "config_file": {
            "title": "Config File",
            "description": "URL to the training configuration file.",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiFluxLoraFastTraining_File"
              }
            ]
          },
          "debug_preprocessed_output": {
            "title": "Debug Preprocessed Output",
            "description": "URL to the preprocessed images.",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiFluxLoraFastTraining_File"
              }
            ]
          },
          "diffusers_lora_file": {
            "title": "Diffusers Lora File",
            "description": "URL to the trained diffusers lora weights.",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiFluxLoraFastTraining_File"
              }
            ]
          }
        },
        "x-fal-order-properties": [
          "diffusers_lora_file",
          "config_file",
          "debug_preprocessed_output"
        ],
        "required": [
          "diffusers_lora_file",
          "config_file"
        ]
      },
      "FalAiFluxLoraFastTraining_File": {
        "title": "File",
        "type": "object",
        "properties": {
          "file_size": {
            "examples": [
              4404019
            ],
            "title": "File Size",
            "type": "integer",
            "description": "The size of the file in bytes."
          },
          "file_name": {
            "examples": [
              "z9RV14K95DvU.png"
            ],
            "title": "File Name",
            "type": "string",
            "description": "The name of the file. It will be auto-generated if not provided."
          },
          "content_type": {
            "examples": [
              "image/png"
            ],
            "title": "Content Type",
            "type": "string",
            "description": "The mime type of the file."
          },
          "url": {
            "title": "Url",
            "type": "string",
            "description": "The URL where the file can be downloaded from."
          },
          "file_data": {
            "format": "binary",
            "title": "File Data",
            "type": "string",
            "description": "File data"
          }
        },
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size",
          "file_data"
        ],
        "required": [
          "url"
        ]
      },
      "FluxLoraPortraitTrainerInput": {
        "title": "PublicInput",
        "type": "object",
        "properties": {
          "images_data_url": {
            "description": "\n        URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.\n\n        In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.\n\n        The captions can include a special string `[trigger]`. If a trigger_word is specified, it will replace `[trigger]` in the captions.\n    ",
            "type": "string",
            "title": "Images Data Url"
          },
          "trigger_phrase": {
            "description": "Trigger phrase to be used in the captions. If None, a trigger word will not be used.\n        If no captions are provide the trigger_work will be used instead of captions. If captions are provided, the trigger word will replace the `[trigger]` string in the captions.\n        ",
            "type": "string",
            "title": "Trigger Phrase",
            "nullable": true
          },
          "resume_from_checkpoint": {
            "description": "URL to a checkpoint to resume training from.",
            "type": "string",
            "title": "Resume From Checkpoint",
            "default": ""
          },
          "subject_crop": {
            "examples": [
              true
            ],
            "description": "If True, the subject will be cropped from the image.",
            "type": "boolean",
            "title": "Subject Crop",
            "default": true
          },
          "learning_rate": {
            "description": "Learning rate to use for training.",
            "type": "number",
            "minimum": 0.000001,
            "maximum": 0.001,
            "title": "Learning Rate",
            "examples": [
              0.0002
            ],
            "default": 0.00009
          },
          "multiresolution_training": {
            "examples": [
              true
            ],
            "description": "If True, multiresolution training will be used.",
            "type": "boolean",
            "title": "Multiresolution Training",
            "default": true
          },
          "steps": {
            "description": "Number of steps to train the LoRA on.",
            "type": "integer",
            "minimum": 1,
            "maximum": 10000,
            "title": "Steps",
            "examples": [
              1000
            ],
            "default": 2500
          },
          "data_archive_format": {
            "description": "The format of the archive. If not specified, the format will be inferred from the URL.",
            "type": "string",
            "title": "Data Archive Format",
            "nullable": true
          },
          "create_masks": {
            "examples": [
              false
            ],
            "description": "If True, masks will be created for the subject.",
            "type": "boolean",
            "title": "Create Masks",
            "default": false
          }
        },
        "x-fal-order-properties": [
          "images_data_url",
          "trigger_phrase",
          "learning_rate",
          "steps",
          "multiresolution_training",
          "subject_crop",
          "data_archive_format",
          "resume_from_checkpoint",
          "create_masks"
        ],
        "required": [
          "images_data_url"
        ]
      },
      "FluxLoraPortraitTrainerOutput": {
        "title": "Output",
        "type": "object",
        "properties": {
          "config_file": {
            "description": "URL to the training configuration file.",
            "title": "Config File",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiFluxLoraPortraitTrainer_File"
              }
            ]
          },
          "diffusers_lora_file": {
            "description": "URL to the trained diffusers lora weights.",
            "title": "Diffusers Lora File",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiFluxLoraPortraitTrainer_File"
              }
            ]
          }
        },
        "x-fal-order-properties": [
          "diffusers_lora_file",
          "config_file"
        ],
        "required": [
          "diffusers_lora_file",
          "config_file"
        ]
      },
      "FalAiFluxLoraPortraitTrainer_File": {
        "title": "File",
        "type": "object",
        "properties": {
          "file_size": {
            "examples": [
              4404019
            ],
            "description": "The size of the file in bytes.",
            "type": "integer",
            "title": "File Size"
          },
          "file_name": {
            "examples": [
              "z9RV14K95DvU.png"
            ],
            "description": "The name of the file. It will be auto-generated if not provided.",
            "type": "string",
            "title": "File Name"
          },
          "content_type": {
            "examples": [
              "image/png"
            ],
            "description": "The mime type of the file.",
            "type": "string",
            "title": "Content Type"
          },
          "url": {
            "description": "The URL where the file can be downloaded from.",
            "type": "string",
            "title": "Url"
          },
          "file_data": {
            "format": "binary",
            "description": "File data",
            "type": "string",
            "title": "File Data"
          }
        },
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size",
          "file_data"
        ],
        "required": [
          "url"
        ]
      },
      "Flux2Klein9bBaseTrainerEditInput": {
        "x-fal-order-properties": [
          "image_data_url",
          "steps",
          "learning_rate",
          "default_caption",
          "output_lora_format"
        ],
        "type": "object",
        "properties": {
          "steps": {
            "description": "Total number of training steps.",
            "type": "integer",
            "minimum": 100,
            "maximum": 10000,
            "title": "Steps",
            "multipleOf": 100,
            "default": 1000
          },
          "image_data_url": {
            "description": "\n    URL to the input data zip archive.\n\n    The zip should contain pairs of images. The images should be named:\n\n    ROOT_start.EXT and ROOT_end.EXT\n    For example:\n    photo_start.jpg and photo_end.jpg\n\n    The zip can also contain up to four reference image for each image pair. The reference images should be named:\n    ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT\n    For example:\n    photo_start.jpg, photo_start2.jpg, photo_end.jpg\n\n    The zip can also contain a text file for each image pair. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    ",
            "type": "string",
            "title": "Image Data Url"
          },
          "learning_rate": {
            "description": "Learning rate applied to trainable parameters.",
            "type": "number",
            "title": "Learning Rate",
            "default": 0.00005
          },
          "default_caption": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error.",
            "title": "Default Caption"
          },
          "output_lora_format": {
            "enum": [
              "fal",
              "comfy"
            ],
            "description": "Dictates the naming scheme for the output weights",
            "type": "string",
            "title": "Output Lora Format",
            "default": "fal"
          }
        },
        "title": "InputEditV2",
        "required": [
          "image_data_url"
        ]
      },
      "Flux2Klein9bBaseTrainerEditOutput": {
        "x-fal-order-properties": [
          "diffusers_lora_file",
          "config_file"
        ],
        "type": "object",
        "properties": {
          "config_file": {
            "description": "URL to the configuration file for the trained model.",
            "$ref": "#/components/schemas/FalAiFlux2Klein9bBaseTrainerEdit_File"
          },
          "diffusers_lora_file": {
            "description": "URL to the trained diffusers lora weights.",
            "$ref": "#/components/schemas/FalAiFlux2Klein9bBaseTrainerEdit_File"
          }
        },
        "title": "Output",
        "required": [
          "diffusers_lora_file",
          "config_file"
        ]
      },
      "FalAiFlux2Klein9bBaseTrainerEdit_File": {
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size"
        ],
        "type": "object",
        "properties": {
          "file_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "type": "null"
              }
            ],
            "description": "The size of the file in bytes.",
            "examples": [
              4404019
            ],
            "title": "File Size"
          },
          "file_name": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "The name of the file. It will be auto-generated if not provided.",
            "examples": [
              "z9RV14K95DvU.png"
            ],
            "title": "File Name"
          },
          "content_type": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "The mime type of the file.",
            "examples": [
              "image/png"
            ],
            "title": "Content Type"
          },
          "url": {
            "description": "The URL where the file can be downloaded from.",
            "type": "string",
            "title": "Url"
          }
        },
        "title": "File",
        "required": [
          "url"
        ]
      },
      "Flux2Klein9bBaseTrainerInput": {
        "x-fal-order-properties": [
          "image_data_url",
          "steps",
          "learning_rate",
          "default_caption",
          "output_lora_format"
        ],
        "type": "object",
        "properties": {
          "steps": {
            "description": "Total number of training steps.",
            "type": "integer",
            "minimum": 100,
            "maximum": 10000,
            "title": "Steps",
            "multipleOf": 100,
            "default": 1000
          },
          "image_data_url": {
            "description": "\n    URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.\n\n    The zip can also contain a text file for each image. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    ",
            "type": "string",
            "title": "Image Data Url"
          },
          "learning_rate": {
            "description": "Learning rate applied to trainable parameters.",
            "type": "number",
            "title": "Learning Rate",
            "default": 0.00005
          },
          "default_caption": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error.",
            "title": "Default Caption"
          },
          "output_lora_format": {
            "enum": [
              "fal",
              "comfy"
            ],
            "description": "Dictates the naming scheme for the output weights",
            "type": "string",
            "title": "Output Lora Format",
            "default": "fal"
          }
        },
        "description": "V2 input with multi-resolution bucketing.",
        "title": "InputT2IV2",
        "required": [
          "image_data_url"
        ]
      },
      "Flux2Klein9bBaseTrainerOutput": {
        "x-fal-order-properties": [
          "diffusers_lora_file",
          "config_file"
        ],
        "type": "object",
        "properties": {
          "config_file": {
            "description": "URL to the configuration file for the trained model.",
            "$ref": "#/components/schemas/FalAiFlux2Klein9bBaseTrainer_File"
          },
          "diffusers_lora_file": {
            "description": "URL to the trained diffusers lora weights.",
            "$ref": "#/components/schemas/FalAiFlux2Klein9bBaseTrainer_File"
          }
        },
        "title": "Output",
        "required": [
          "diffusers_lora_file",
          "config_file"
        ]
      },
      "FalAiFlux2Klein9bBaseTrainer_File": {
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size"
        ],
        "type": "object",
        "properties": {
          "file_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "type": "null"
              }
            ],
            "description": "The size of the file in bytes.",
            "examples": [
              4404019
            ],
            "title": "File Size"
          },
          "file_name": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "The name of the file. It will be auto-generated if not provided.",
            "examples": [
              "z9RV14K95DvU.png"
            ],
            "title": "File Name"
          },
          "content_type": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "The mime type of the file.",
            "examples": [
              "image/png"
            ],
            "title": "Content Type"
          },
          "url": {
            "description": "The URL where the file can be downloaded from.",
            "type": "string",
            "title": "Url"
          }
        },
        "title": "File",
        "required": [
          "url"
        ]
      },
      "Flux2Klein4bBaseTrainerInput": {
        "title": "InputT2IV2",
        "type": "object",
        "properties": {
          "steps": {
            "description": "Total number of training steps.",
            "type": "integer",
            "minimum": 100,
            "maximum": 10000,
            "title": "Steps",
            "multipleOf": 100,
            "default": 1000
          },
          "image_data_url": {
            "title": "Image Data Url",
            "type": "string",
            "description": "\n    URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.\n\n    The zip can also contain a text file for each image. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    "
          },
          "learning_rate": {
            "title": "Learning Rate",
            "type": "number",
            "description": "Learning rate applied to trainable parameters.",
            "default": 0.00005
          },
          "default_caption": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Default Caption",
            "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
          },
          "output_lora_format": {
            "enum": [
              "fal",
              "comfy"
            ],
            "title": "Output Lora Format",
            "type": "string",
            "description": "Dictates the naming scheme for the output weights",
            "default": "fal"
          }
        },
        "x-fal-order-properties": [
          "image_data_url",
          "steps",
          "learning_rate",
          "default_caption",
          "output_lora_format"
        ],
        "description": "V2 input with multi-resolution bucketing.",
        "required": [
          "image_data_url"
        ]
      },
      "Flux2Klein4bBaseTrainerOutput": {
        "title": "Output",
        "type": "object",
        "properties": {
          "config_file": {
            "description": "URL to the configuration file for the trained model.",
            "$ref": "#/components/schemas/FalAiFlux2Klein4bBaseTrainer_File"
          },
          "diffusers_lora_file": {
            "description": "URL to the trained diffusers lora weights.",
            "$ref": "#/components/schemas/FalAiFlux2Klein4bBaseTrainer_File"
          }
        },
        "x-fal-order-properties": [
          "diffusers_lora_file",
          "config_file"
        ],
        "required": [
          "diffusers_lora_file",
          "config_file"
        ]
      },
      "FalAiFlux2Klein4bBaseTrainer_File": {
        "title": "File",
        "type": "object",
        "properties": {
          "file_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "type": "null"
              }
            ],
            "title": "File Size",
            "examples": [
              4404019
            ],
            "description": "The size of the file in bytes."
          },
          "file_name": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "File Name",
            "examples": [
              "z9RV14K95DvU.png"
            ],
            "description": "The name of the file. It will be auto-generated if not provided."
          },
          "content_type": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Content Type",
            "examples": [
              "image/png"
            ],
            "description": "The mime type of the file."
          },
          "url": {
            "title": "Url",
            "type": "string",
            "description": "The URL where the file can be downloaded from."
          }
        },
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size"
        ],
        "required": [
          "url"
        ]
      },
      "Flux2Klein4bBaseTrainerEditInput": {
        "title": "InputEditV2",
        "type": "object",
        "properties": {
          "steps": {
            "description": "Total number of training steps.",
            "type": "integer",
            "minimum": 100,
            "maximum": 10000,
            "title": "Steps",
            "multipleOf": 100,
            "default": 1000
          },
          "image_data_url": {
            "title": "Image Data Url",
            "type": "string",
            "description": "\n    URL to the input data zip archive.\n\n    The zip should contain pairs of images. The images should be named:\n\n    ROOT_start.EXT and ROOT_end.EXT\n    For example:\n    photo_start.jpg and photo_end.jpg\n\n    The zip can also contain up to four reference image for each image pair. The reference images should be named:\n    ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT\n    For example:\n    photo_start.jpg, photo_start2.jpg, photo_end.jpg\n\n    The zip can also contain a text file for each image pair. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    "
          },
          "learning_rate": {
            "title": "Learning Rate",
            "type": "number",
            "description": "Learning rate applied to trainable parameters.",
            "default": 0.00005
          },
          "default_caption": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Default Caption",
            "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
          },
          "output_lora_format": {
            "enum": [
              "fal",
              "comfy"
            ],
            "title": "Output Lora Format",
            "type": "string",
            "description": "Dictates the naming scheme for the output weights",
            "default": "fal"
          }
        },
        "x-fal-order-properties": [
          "image_data_url",
          "steps",
          "learning_rate",
          "default_caption",
          "output_lora_format"
        ],
        "required": [
          "image_data_url"
        ]
      },
      "Flux2Klein4bBaseTrainerEditOutput": {
        "title": "Output",
        "type": "object",
        "properties": {
          "config_file": {
            "description": "URL to the configuration file for the trained model.",
            "$ref": "#/components/schemas/FalAiFlux2Klein4bBaseTrainerEdit_File"
          },
          "diffusers_lora_file": {
            "description": "URL to the trained diffusers lora weights.",
            "$ref": "#/components/schemas/FalAiFlux2Klein4bBaseTrainerEdit_File"
          }
        },
        "x-fal-order-properties": [
          "diffusers_lora_file",
          "config_file"
        ],
        "required": [
          "diffusers_lora_file",
          "config_file"
        ]
      },
      "FalAiFlux2Klein4bBaseTrainerEdit_File": {
        "title": "File",
        "type": "object",
        "properties": {
          "file_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "type": "null"
              }
            ],
            "title": "File Size",
            "examples": [
              4404019
            ],
            "description": "The size of the file in bytes."
          },
          "file_name": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "File Name",
            "examples": [
              "z9RV14K95DvU.png"
            ],
            "description": "The name of the file. It will be auto-generated if not provided."
          },
          "content_type": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Content Type",
            "examples": [
              "image/png"
            ],
            "description": "The mime type of the file."
          },
          "url": {
            "title": "Url",
            "type": "string",
            "description": "The URL where the file can be downloaded from."
          }
        },
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size"
        ],
        "required": [
          "url"
        ]
      },
      "QwenImage2512TrainerV2Input": {
        "title": "Input",
        "type": "object",
        "properties": {
          "steps": {
            "description": "Number of steps to train for",
            "type": "integer",
            "minimum": 10,
            "maximum": 40000,
            "title": "Steps",
            "default": 2000
          },
          "image_data_url": {
            "title": "Image Data Url",
            "type": "string",
            "description": "\n        URL to the input data zip archive.\n\n        The zip should contain pairs of images and corresponding captions.\n\n        The images should be named: ROOT.EXT. For example: 001.jpg\n\n        The corresponding captions should be named: ROOT.txt. For example: 001.txt\n\n        If no text file is provided for an image, the default_caption will be used.\n        "
          },
          "learning_rate": {
            "title": "Learning Rate",
            "type": "number",
            "description": "Learning rate.",
            "default": 0.0005
          },
          "default_caption": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Default Caption",
            "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
          }
        },
        "x-fal-order-properties": [
          "image_data_url",
          "steps",
          "default_caption",
          "learning_rate"
        ],
        "required": [
          "image_data_url"
        ]
      },
      "QwenImage2512TrainerV2Output": {
        "title": "Output",
        "type": "object",
        "properties": {
          "config_file": {
            "description": "URL to the configuration file for the trained model.",
            "$ref": "#/components/schemas/FalAiQwenImage2512TrainerV2_File"
          },
          "diffusers_lora_file": {
            "description": "URL to the trained diffusers lora weights.",
            "$ref": "#/components/schemas/FalAiQwenImage2512TrainerV2_File"
          }
        },
        "x-fal-order-properties": [
          "diffusers_lora_file",
          "config_file"
        ],
        "required": [
          "diffusers_lora_file",
          "config_file"
        ]
      },
      "FalAiQwenImage2512TrainerV2_File": {
        "title": "File",
        "type": "object",
        "properties": {
          "file_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "type": "null"
              }
            ],
            "title": "File Size",
            "examples": [
              4404019
            ],
            "description": "The size of the file in bytes."
          },
          "file_name": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "File Name",
            "examples": [
              "z9RV14K95DvU.png"
            ],
            "description": "The name of the file. It will be auto-generated if not provided."
          },
          "content_type": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Content Type",
            "examples": [
              "image/png"
            ],
            "description": "The mime type of the file."
          },
          "url": {
            "title": "Url",
            "type": "string",
            "description": "The URL where the file can be downloaded from."
          }
        },
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size"
        ],
        "required": [
          "url"
        ]
      },
      "Flux2TrainerV2EditInput": {
        "x-fal-order-properties": [
          "image_data_url",
          "steps",
          "learning_rate",
          "default_caption",
          "output_lora_format"
        ],
        "type": "object",
        "properties": {
          "steps": {
            "description": "Total number of training steps.",
            "type": "integer",
            "minimum": 100,
            "title": "Steps",
            "maximum": 10000,
            "multipleOf": 100,
            "default": 1000
          },
          "image_data_url": {
            "description": "\n    URL to the input data zip archive.\n\n    The zip should contain pairs of images. The images should be named:\n\n    ROOT_start.EXT and ROOT_end.EXT\n    For example:\n    photo_start.jpg and photo_end.jpg\n\n    The zip can also contain up to four reference image for each image pair. The reference images should be named:\n    ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT\n    For example:\n    photo_start.jpg, photo_start2.jpg, photo_end.jpg\n\n    The zip can also contain a text file for each image pair. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    ",
            "type": "string",
            "title": "Image Data Url"
          },
          "learning_rate": {
            "description": "Learning rate applied to trainable parameters.",
            "type": "number",
            "title": "Learning Rate",
            "default": 0.00005
          },
          "default_caption": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error.",
            "title": "Default Caption"
          },
          "output_lora_format": {
            "enum": [
              "fal",
              "comfy"
            ],
            "title": "Output Lora Format",
            "type": "string",
            "description": "Dictates the naming scheme for the output weights",
            "default": "fal"
          }
        },
        "title": "InputEditV2",
        "required": [
          "image_data_url"
        ]
      },
      "Flux2TrainerV2EditOutput": {
        "x-fal-order-properties": [
          "diffusers_lora_file",
          "config_file"
        ],
        "type": "object",
        "properties": {
          "config_file": {
            "description": "URL to the configuration file for the trained model.",
            "$ref": "#/components/schemas/FalAiFlux2TrainerV2Edit_File"
          },
          "diffusers_lora_file": {
            "description": "URL to the trained diffusers lora weights.",
            "$ref": "#/components/schemas/FalAiFlux2TrainerV2Edit_File"
          }
        },
        "title": "Output",
        "required": [
          "diffusers_lora_file",
          "config_file"
        ]
      },
      "FalAiFlux2TrainerV2Edit_File": {
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size"
        ],
        "type": "object",
        "properties": {
          "file_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "type": "null"
              }
            ],
            "description": "The size of the file in bytes.",
            "title": "File Size",
            "examples": [
              4404019
            ]
          },
          "file_name": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "The name of the file. It will be auto-generated if not provided.",
            "title": "File Name",
            "examples": [
              "z9RV14K95DvU.png"
            ]
          },
          "content_type": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "The mime type of the file.",
            "title": "Content Type",
            "examples": [
              "image/png"
            ]
          },
          "url": {
            "description": "The URL where the file can be downloaded from.",
            "type": "string",
            "title": "Url"
          }
        },
        "title": "File",
        "required": [
          "url"
        ]
      },
      "Flux2TrainerV2Input": {
        "description": "V2 input with multi-resolution bucketing.",
        "type": "object",
        "properties": {
          "steps": {
            "description": "Total number of training steps.",
            "type": "integer",
            "minimum": 100,
            "title": "Steps",
            "maximum": 10000,
            "multipleOf": 100,
            "default": 1000
          },
          "image_data_url": {
            "description": "\n    URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.\n\n    The zip can also contain a text file for each image. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    ",
            "type": "string",
            "title": "Image Data Url"
          },
          "learning_rate": {
            "description": "Learning rate applied to trainable parameters.",
            "type": "number",
            "title": "Learning Rate",
            "default": 0.00005
          },
          "default_caption": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error.",
            "title": "Default Caption"
          },
          "output_lora_format": {
            "enum": [
              "fal",
              "comfy"
            ],
            "title": "Output Lora Format",
            "type": "string",
            "description": "Dictates the naming scheme for the output weights",
            "default": "fal"
          }
        },
        "title": "InputT2IV2",
        "x-fal-order-properties": [
          "image_data_url",
          "steps",
          "learning_rate",
          "default_caption",
          "output_lora_format"
        ],
        "required": [
          "image_data_url"
        ]
      },
      "Flux2TrainerV2Output": {
        "x-fal-order-properties": [
          "diffusers_lora_file",
          "config_file"
        ],
        "type": "object",
        "properties": {
          "config_file": {
            "description": "URL to the configuration file for the trained model.",
            "$ref": "#/components/schemas/FalAiFlux2TrainerV2_File"
          },
          "diffusers_lora_file": {
            "description": "URL to the trained diffusers lora weights.",
            "$ref": "#/components/schemas/FalAiFlux2TrainerV2_File"
          }
        },
        "title": "Output",
        "required": [
          "diffusers_lora_file",
          "config_file"
        ]
      },
      "FalAiFlux2TrainerV2_File": {
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size"
        ],
        "type": "object",
        "properties": {
          "file_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "type": "null"
              }
            ],
            "description": "The size of the file in bytes.",
            "title": "File Size",
            "examples": [
              4404019
            ]
          },
          "file_name": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "The name of the file. It will be auto-generated if not provided.",
            "title": "File Name",
            "examples": [
              "z9RV14K95DvU.png"
            ]
          },
          "content_type": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "The mime type of the file.",
            "title": "Content Type",
            "examples": [
              "image/png"
            ]
          },
          "url": {
            "description": "The URL where the file can be downloaded from.",
            "type": "string",
            "title": "Url"
          }
        },
        "title": "File",
        "required": [
          "url"
        ]
      },
      "Ltx2V2vTrainerInput": {
        "description": "Input configuration for LTX-2 video-to-video (IC-LoRA) training.",
        "type": "object",
        "properties": {
          "number_of_steps": {
            "description": "The number of training steps.",
            "type": "integer",
            "minimum": 100,
            "maximum": 20000,
            "examples": [
              2000
            ],
            "title": "Number Of Steps",
            "step": 100,
            "default": 2000
          },
          "frame_rate": {
            "description": "Target frames per second for the video.",
            "type": "integer",
            "examples": [
              25
            ],
            "maximum": 60,
            "minimum": 8,
            "title": "Frame Rate",
            "default": 25
          },
          "learning_rate": {
            "description": "Learning rate for optimization. Higher values can lead to faster training but may cause overfitting.",
            "type": "number",
            "minimum": 0.000001,
            "maximum": 1,
            "examples": [
              0.0002
            ],
            "title": "Learning Rate",
            "step": 0.0001,
            "default": 0.0002
          },
          "validation": {
            "description": "A list of validation inputs with prompts and reference videos.",
            "type": "array",
            "title": "Validation",
            "maxItems": 2,
            "items": {
              "$ref": "#/components/schemas/V2VValidation"
            },
            "default": []
          },
          "number_of_frames": {
            "description": "Number of frames per training sample. Must satisfy frames % 8 == 1 (e.g., 1, 9, 17, 25, 33, 41, 49, 57, 65, 73, 81, 89, 97).",
            "type": "integer",
            "examples": [
              89
            ],
            "maximum": 121,
            "minimum": 9,
            "title": "Number Of Frames",
            "default": 89
          },
          "training_data_url": {
            "description": "URL to zip archive with videos or images. Try to use at least 10 files, although more is better.\n\n        **Supported video formats:** .mp4, .mov, .avi, .mkv\n        **Supported image formats:** .png, .jpg, .jpeg\n\n        Note: The dataset must contain ONLY videos OR ONLY images - mixed datasets are not supported.\n\n        The archive can also contain text files with captions. Each text file should have the same name as the media file it corresponds to.",
            "type": "string",
            "title": "Training Data Url"
          },
          "split_input_duration_threshold": {
            "description": "The duration threshold in seconds. If a video is longer than this, it will be split into scenes.",
            "type": "number",
            "examples": [
              30
            ],
            "maximum": 60,
            "minimum": 1,
            "title": "Split Input Duration Threshold",
            "default": 30
          },
          "rank": {
            "enum": [
              8,
              16,
              32,
              64,
              128
            ],
            "title": "Rank",
            "type": "integer",
            "examples": [
              32
            ],
            "description": "The rank of the LoRA adaptation. Higher values increase capacity but use more memory.",
            "default": 32
          },
          "first_frame_conditioning_p": {
            "minimum": 0,
            "maximum": 1,
            "type": "number",
            "title": "First Frame Conditioning P",
            "description": "Probability of conditioning on the first frame during training. Lower values work better for video-to-video transformation.",
            "default": 0.1
          },
          "stg_scale": {
            "minimum": 0,
            "maximum": 3,
            "type": "number",
            "title": "Stg Scale",
            "description": "STG (Spatio-Temporal Guidance) scale. 0.0 disables STG. Recommended value is 1.0.",
            "default": 1
          },
          "resolution": {
            "enum": [
              "low",
              "medium",
              "high"
            ],
            "title": "Resolution",
            "type": "string",
            "examples": [
              "medium"
            ],
            "description": "Resolution to use for training. Higher resolutions require more memory.",
            "default": "medium"
          },
          "split_input_into_scenes": {
            "examples": [
              true
            ],
            "title": "Split Input Into Scenes",
            "type": "boolean",
            "description": "If true, videos above a certain duration threshold will be split into scenes.",
            "default": true
          },
          "trigger_phrase": {
            "examples": [
              ""
            ],
            "description": "A phrase that will trigger the LoRA style. Will be prepended to captions during training.",
            "type": "string",
            "title": "Trigger Phrase",
            "default": ""
          },
          "validation_frame_rate": {
            "description": "Target frames per second for validation videos.",
            "type": "integer",
            "examples": [
              25
            ],
            "maximum": 60,
            "minimum": 8,
            "title": "Validation Frame Rate",
            "default": 25
          },
          "aspect_ratio": {
            "enum": [
              "16:9",
              "1:1",
              "9:16"
            ],
            "title": "Aspect Ratio",
            "type": "string",
            "examples": [
              "1:1"
            ],
            "description": "Aspect ratio to use for training.",
            "default": "1:1"
          },
          "validation_resolution": {
            "enum": [
              "low",
              "medium",
              "high"
            ],
            "title": "Validation Resolution",
            "type": "string",
            "examples": [
              "high"
            ],
            "description": "The resolution to use for validation.",
            "default": "high"
          },
          "validation_number_of_frames": {
            "description": "The number of frames in validation videos.",
            "type": "integer",
            "examples": [
              89
            ],
            "maximum": 121,
            "minimum": 9,
            "title": "Validation Number Of Frames",
            "default": 89
          },
          "validation_aspect_ratio": {
            "enum": [
              "16:9",
              "1:1",
              "9:16"
            ],
            "title": "Validation Aspect Ratio",
            "type": "string",
            "examples": [
              "1:1"
            ],
            "description": "The aspect ratio to use for validation.",
            "default": "1:1"
          },
          "validation_negative_prompt": {
            "title": "Validation Negative Prompt",
            "type": "string",
            "description": "A negative prompt to use for validation.",
            "default": "worst quality, inconsistent motion, blurry, jittery, distorted"
          },
          "auto_scale_input": {
            "examples": [
              false
            ],
            "title": "Auto Scale Input",
            "type": "boolean",
            "description": "If true, videos will be automatically scaled to the target frame count and fps. This option has no effect on image datasets.",
            "default": false
          }
        },
        "title": "LTX2V2VInput",
        "x-fal-order-properties": [
          "training_data_url",
          "rank",
          "number_of_steps",
          "learning_rate",
          "number_of_frames",
          "frame_rate",
          "resolution",
          "aspect_ratio",
          "trigger_phrase",
          "auto_scale_input",
          "split_input_into_scenes",
          "split_input_duration_threshold",
          "first_frame_conditioning_p",
          "validation",
          "validation_negative_prompt",
          "validation_number_of_frames",
          "validation_frame_rate",
          "validation_resolution",
          "validation_aspect_ratio",
          "stg_scale"
        ],
        "required": [
          "training_data_url"
        ]
      },
      "Ltx2V2vTrainerOutput": {
        "description": "Output from LTX-2 video-to-video training.",
        "type": "object",
        "properties": {
          "lora_file": {
            "description": "URL to the trained IC-LoRA weights (.safetensors).",
            "$ref": "#/components/schemas/FalAiLtx2V2vTrainer_File"
          },
          "config_file": {
            "description": "Configuration used for setting up inference endpoints.",
            "$ref": "#/components/schemas/FalAiLtx2V2vTrainer_File"
          },
          "debug_dataset": {
            "anyOf": [
              {
                "$ref": "#/components/schemas/FalAiLtx2V2vTrainer_File"
              },
              {
                "type": "null"
              }
            ],
            "description": "URL to the debug dataset archive containing decoded videos."
          },
          "video": {
            "anyOf": [
              {
                "$ref": "#/components/schemas/FalAiLtx2V2vTrainer_File"
              },
              {
                "type": "null"
              }
            ],
            "description": "The URL to the validation videos (with reference videos side-by-side), if any."
          }
        },
        "title": "LTX2V2VOutput",
        "x-fal-order-properties": [
          "video",
          "lora_file",
          "config_file",
          "debug_dataset"
        ],
        "required": [
          "video",
          "lora_file",
          "config_file"
        ]
      },
      "V2VValidation": {
        "description": "Validation input for video-to-video training.",
        "type": "object",
        "properties": {
          "prompt": {
            "title": "Prompt",
            "type": "string",
            "description": "The prompt to use for validation."
          },
          "reference_video_url": {
            "title": "Reference Video Url",
            "type": "string",
            "minLength": 1,
            "description": "URL to reference video for IC-LoRA validation. This is the input video that will be transformed."
          }
        },
        "title": "V2VValidation",
        "x-fal-order-properties": [
          "prompt",
          "reference_video_url"
        ],
        "required": [
          "prompt",
          "reference_video_url"
        ]
      },
      "FalAiLtx2V2vTrainer_File": {
        "title": "File",
        "type": "object",
        "properties": {
          "file_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "type": "null"
              }
            ],
            "description": "The size of the file in bytes.",
            "examples": [
              4404019
            ],
            "title": "File Size"
          },
          "file_name": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "The name of the file. It will be auto-generated if not provided.",
            "examples": [
              "z9RV14K95DvU.png"
            ],
            "title": "File Name"
          },
          "content_type": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "The mime type of the file.",
            "examples": [
              "image/png"
            ],
            "title": "Content Type"
          },
          "url": {
            "title": "Url",
            "type": "string",
            "description": "The URL where the file can be downloaded from."
          }
        },
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size"
        ],
        "required": [
          "url"
        ]
      },
      "Ltx2VideoTrainerInput": {
        "title": "LTX2Input",
        "type": "object",
        "properties": {
          "audio_normalize": {
            "title": "Audio Normalize",
            "type": "boolean",
            "description": "Normalize audio peak amplitude to a consistent level. Recommended for consistent audio levels across the dataset.",
            "default": true
          },
          "audio_preserve_pitch": {
            "title": "Audio Preserve Pitch",
            "type": "boolean",
            "description": "When audio duration doesn't match video duration, stretch/compress audio without changing pitch. If disabled, audio is trimmed or padded with silence.",
            "default": true
          },
          "frame_rate": {
            "description": "Target frames per second for the video.",
            "type": "integer",
            "minimum": 8,
            "maximum": 60,
            "title": "Frame Rate",
            "examples": [
              25
            ],
            "default": 25
          },
          "number_of_steps": {
            "description": "The number of training steps.",
            "type": "integer",
            "minimum": 100,
            "maximum": 20000,
            "title": "Number Of Steps",
            "examples": [
              2000
            ],
            "step": 100,
            "default": 2000
          },
          "learning_rate": {
            "description": "Learning rate for optimization. Higher values can lead to faster training but may cause overfitting.",
            "type": "number",
            "minimum": 0.000001,
            "maximum": 1,
            "title": "Learning Rate",
            "examples": [
              0.0002
            ],
            "step": 0.0001,
            "default": 0.0002
          },
          "validation": {
            "title": "Validation",
            "type": "array",
            "maxItems": 2,
            "description": "A list of validation prompts to use during training. When providing an image, _all_ validation inputs must have an image.",
            "items": {
              "$ref": "#/components/schemas/Validation"
            },
            "default": []
          },
          "number_of_frames": {
            "description": "Number of frames per training sample. Must satisfy frames % 8 == 1 (e.g., 1, 9, 17, 25, 33, 41, 49, 57, 65, 73, 81, 89, 97).",
            "type": "integer",
            "minimum": 9,
            "maximum": 121,
            "title": "Number Of Frames",
            "examples": [
              89
            ],
            "default": 89
          },
          "training_data_url": {
            "title": "Training Data Url",
            "type": "string",
            "description": "URL to zip archive with videos or images. Try to use at least 10 files, although more is better.\n\n        **Supported video formats:** .mp4, .mov, .avi, .mkv\n        **Supported image formats:** .png, .jpg, .jpeg\n\n        Note: The dataset must contain ONLY videos OR ONLY images - mixed datasets are not supported.\n\n        The archive can also contain text files with captions. Each text file should have the same name as the media file it corresponds to."
          },
          "split_input_duration_threshold": {
            "description": "The duration threshold in seconds. If a video is longer than this, it will be split into scenes.",
            "type": "number",
            "minimum": 1,
            "maximum": 60,
            "title": "Split Input Duration Threshold",
            "examples": [
              30
            ],
            "default": 30
          },
          "rank": {
            "enum": [
              8,
              16,
              32,
              64,
              128
            ],
            "description": "The rank of the LoRA adaptation. Higher values increase capacity but use more memory.",
            "type": "integer",
            "title": "Rank",
            "examples": [
              32
            ],
            "default": 32
          },
          "first_frame_conditioning_p": {
            "minimum": 0,
            "maximum": 1,
            "type": "number",
            "title": "First Frame Conditioning P",
            "description": "Probability of conditioning on the first frame during training. Higher values improve image-to-video performance.",
            "default": 0.5
          },
          "stg_scale": {
            "minimum": 0,
            "maximum": 3,
            "type": "number",
            "title": "Stg Scale",
            "description": "STG (Spatio-Temporal Guidance) scale. 0.0 disables STG. Recommended value is 1.0.",
            "default": 1
          },
          "aspect_ratio": {
            "enum": [
              "16:9",
              "1:1",
              "9:16"
            ],
            "description": "Aspect ratio to use for training.",
            "type": "string",
            "title": "Aspect Ratio",
            "examples": [
              "1:1"
            ],
            "default": "1:1"
          },
          "trigger_phrase": {
            "examples": [
              ""
            ],
            "description": "A phrase that will trigger the LoRA style. Will be prepended to captions during training.",
            "type": "string",
            "title": "Trigger Phrase",
            "default": ""
          },
          "resolution": {
            "enum": [
              "low",
              "medium",
              "high"
            ],
            "description": "Resolution to use for training. Higher resolutions require more memory.",
            "type": "string",
            "title": "Resolution",
            "examples": [
              "medium"
            ],
            "default": "medium"
          },
          "validation_frame_rate": {
            "description": "Target frames per second for validation videos.",
            "type": "integer",
            "minimum": 8,
            "maximum": 60,
            "title": "Validation Frame Rate",
            "examples": [
              25
            ],
            "default": 25
          },
          "split_input_into_scenes": {
            "examples": [
              true
            ],
            "description": "If true, videos above a certain duration threshold will be split into scenes.",
            "type": "boolean",
            "title": "Split Input Into Scenes",
            "default": true
          },
          "with_audio": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "type": "null"
              }
            ],
            "title": "With Audio",
            "description": "Enable joint audio-video training. If None (default), automatically detects whether input videos have audio. Set to True to force audio training, or False to disable."
          },
          "generate_audio_in_validation": {
            "title": "Generate Audio In Validation",
            "type": "boolean",
            "description": "Whether to generate audio in validation samples.",
            "default": true
          },
          "validation_resolution": {
            "enum": [
              "low",
              "medium",
              "high"
            ],
            "description": "The resolution to use for validation.",
            "type": "string",
            "title": "Validation Resolution",
            "examples": [
              "high"
            ],
            "default": "high"
          },
          "validation_number_of_frames": {
            "description": "The number of frames in validation videos.",
            "type": "integer",
            "minimum": 9,
            "maximum": 121,
            "title": "Validation Number Of Frames",
            "examples": [
              89
            ],
            "default": 89
          },
          "validation_aspect_ratio": {
            "enum": [
              "16:9",
              "1:1",
              "9:16"
            ],
            "description": "The aspect ratio to use for validation.",
            "type": "string",
            "title": "Validation Aspect Ratio",
            "examples": [
              "1:1"
            ],
            "default": "1:1"
          },
          "validation_negative_prompt": {
            "title": "Validation Negative Prompt",
            "type": "string",
            "description": "A negative prompt to use for validation.",
            "default": "worst quality, inconsistent motion, blurry, jittery, distorted"
          },
          "auto_scale_input": {
            "examples": [
              false
            ],
            "description": "If true, videos will be automatically scaled to the target frame count and fps. This option has no effect on image datasets.",
            "type": "boolean",
            "title": "Auto Scale Input",
            "default": false
          }
        },
        "description": "Input configuration for LTX-2 text-to-video training.",
        "x-fal-order-properties": [
          "training_data_url",
          "rank",
          "number_of_steps",
          "learning_rate",
          "number_of_frames",
          "frame_rate",
          "resolution",
          "aspect_ratio",
          "trigger_phrase",
          "auto_scale_input",
          "split_input_into_scenes",
          "split_input_duration_threshold",
          "with_audio",
          "audio_normalize",
          "audio_preserve_pitch",
          "first_frame_conditioning_p",
          "validation",
          "validation_negative_prompt",
          "validation_number_of_frames",
          "validation_frame_rate",
          "validation_resolution",
          "validation_aspect_ratio",
          "stg_scale",
          "generate_audio_in_validation"
        ],
        "required": [
          "training_data_url"
        ]
      },
      "Ltx2VideoTrainerOutput": {
        "title": "LTX2Output",
        "type": "object",
        "properties": {
          "lora_file": {
            "description": "URL to the trained LoRA weights (.safetensors).",
            "$ref": "#/components/schemas/FalAiLtx2VideoTrainer_File"
          },
          "config_file": {
            "description": "Configuration used for setting up inference endpoints.",
            "$ref": "#/components/schemas/FalAiLtx2VideoTrainer_File"
          },
          "debug_dataset": {
            "anyOf": [
              {
                "$ref": "#/components/schemas/FalAiLtx2VideoTrainer_File"
              },
              {
                "type": "null"
              }
            ],
            "description": "URL to the debug dataset archive containing decoded videos and audio."
          },
          "video": {
            "anyOf": [
              {
                "$ref": "#/components/schemas/FalAiLtx2VideoTrainer_File"
              },
              {
                "type": "null"
              }
            ],
            "description": "The URL to the validation videos, if any."
          }
        },
        "description": "Output from LTX-2 training.",
        "x-fal-order-properties": [
          "video",
          "lora_file",
          "config_file",
          "debug_dataset"
        ],
        "required": [
          "video",
          "lora_file",
          "config_file"
        ]
      },
      "Validation": {
        "title": "Validation",
        "type": "object",
        "properties": {
          "prompt": {
            "title": "Prompt",
            "type": "string",
            "description": "The prompt to use for validation."
          },
          "image_url": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Image Url",
            "description": "An image to use for image-to-video validation. If provided for one validation, _all_ validation inputs must have an image."
          }
        },
        "x-fal-order-properties": [
          "prompt",
          "image_url"
        ],
        "required": [
          "prompt"
        ]
      },
      "FalAiLtx2VideoTrainer_File": {
        "title": "File",
        "type": "object",
        "properties": {
          "file_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "type": "null"
              }
            ],
            "title": "File Size",
            "description": "The size of the file in bytes.",
            "examples": [
              4404019
            ]
          },
          "file_name": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "File Name",
            "description": "The name of the file. It will be auto-generated if not provided.",
            "examples": [
              "z9RV14K95DvU.png"
            ]
          },
          "content_type": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Content Type",
            "description": "The mime type of the file.",
            "examples": [
              "image/png"
            ]
          },
          "url": {
            "title": "Url",
            "type": "string",
            "description": "The URL where the file can be downloaded from."
          }
        },
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size"
        ],
        "required": [
          "url"
        ]
      },
      "QwenImage2512TrainerInput": {
        "x-fal-order-properties": [
          "image_data_url",
          "learning_rate",
          "steps",
          "default_caption"
        ],
        "type": "object",
        "properties": {
          "steps": {
            "description": "Number of steps to train for",
            "type": "integer",
            "minimum": 100,
            "maximum": 30000,
            "title": "Steps",
            "multipleOf": 100,
            "default": 1000
          },
          "image_data_url": {
            "description": "\n    URL to the input data zip archive for text-to-image training.\n\n    The zip should contain images with their corresponding text captions:\n\n    image.EXT and image.txt\n    For example:\n    photo.jpg and photo.txt\n\n    The text file contains the caption/prompt describing the target image.\n\n    If no text file is provided for an image, the default_caption will be used.\n\n    If no default_caption is provided and a text file is missing, the training will fail.\n    ",
            "type": "string",
            "title": "Image Data Url"
          },
          "learning_rate": {
            "description": "Learning rate for LoRA parameters.",
            "type": "number",
            "title": "Learning Rate",
            "default": 0.0005
          },
          "default_caption": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error.",
            "title": "Default Caption"
          }
        },
        "title": "InputImage",
        "required": [
          "image_data_url"
        ]
      },
      "QwenImage2512TrainerOutput": {
        "x-fal-order-properties": [
          "diffusers_lora_file",
          "config_file"
        ],
        "type": "object",
        "properties": {
          "config_file": {
            "description": "URL to the configuration file for the trained model.",
            "$ref": "#/components/schemas/FalAiQwenImage2512Trainer_File"
          },
          "diffusers_lora_file": {
            "description": "URL to the trained diffusers lora weights.",
            "$ref": "#/components/schemas/FalAiQwenImage2512Trainer_File"
          }
        },
        "title": "Output",
        "required": [
          "diffusers_lora_file",
          "config_file"
        ]
      },
      "FalAiQwenImage2512Trainer_File": {
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size"
        ],
        "type": "object",
        "properties": {
          "file_size": {
            "examples": [
              4404019
            ],
            "description": "The size of the file in bytes.",
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "type": "null"
              }
            ],
            "title": "File Size"
          },
          "file_name": {
            "examples": [
              "z9RV14K95DvU.png"
            ],
            "description": "The name of the file. It will be auto-generated if not provided.",
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "File Name"
          },
          "content_type": {
            "examples": [
              "image/png"
            ],
            "description": "The mime type of the file.",
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Content Type"
          },
          "url": {
            "description": "The URL where the file can be downloaded from.",
            "type": "string",
            "title": "Url"
          }
        },
        "title": "File",
        "required": [
          "url"
        ]
      },
      "QwenImageEdit2511TrainerInput": {
        "x-fal-order-properties": [
          "image_data_url",
          "learning_rate",
          "steps",
          "default_caption"
        ],
        "type": "object",
        "properties": {
          "steps": {
            "description": "Number of steps to train for",
            "type": "integer",
            "minimum": 100,
            "maximum": 30000,
            "title": "Steps",
            "multipleOf": 100,
            "default": 1000
          },
          "image_data_url": {
            "description": "\n    URL to the input data zip archive.\n\n    The zip should contain pairs of images. The images should be named:\n\n    ROOT_start.EXT and ROOT_end.EXT\n    For example:\n    photo_start.jpg and photo_end.jpg\n\n    The zip can also contain more than one reference image for each image pair. The reference images should be named:\n    ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT\n    For example:\n    photo_start.jpg, photo_start2.jpg, photo_end.jpg\n\n    The Reference Image Count field should be set to the number of reference images.\n\n    The zip can also contain a text file for each image pair. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    ",
            "type": "string",
            "title": "Image Data Url"
          },
          "learning_rate": {
            "description": "Learning rate for LoRA parameters.",
            "type": "number",
            "title": "Learning Rate",
            "default": 0.0001
          },
          "default_caption": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error.",
            "title": "Default Caption"
          }
        },
        "title": "Input2511",
        "required": [
          "image_data_url"
        ]
      },
      "QwenImageEdit2511TrainerOutput": {
        "x-fal-order-properties": [
          "diffusers_lora_file",
          "config_file"
        ],
        "type": "object",
        "properties": {
          "config_file": {
            "description": "URL to the configuration file for the trained model.",
            "$ref": "#/components/schemas/FalAiQwenImageEdit2511Trainer_File"
          },
          "diffusers_lora_file": {
            "description": "URL to the trained diffusers lora weights.",
            "$ref": "#/components/schemas/FalAiQwenImageEdit2511Trainer_File"
          }
        },
        "title": "Output",
        "required": [
          "diffusers_lora_file",
          "config_file"
        ]
      },
      "FalAiQwenImageEdit2511Trainer_File": {
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size"
        ],
        "type": "object",
        "properties": {
          "file_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "type": "null"
              }
            ],
            "description": "The size of the file in bytes.",
            "examples": [
              4404019
            ],
            "title": "File Size"
          },
          "file_name": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "The name of the file. It will be auto-generated if not provided.",
            "examples": [
              "z9RV14K95DvU.png"
            ],
            "title": "File Name"
          },
          "content_type": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "The mime type of the file.",
            "examples": [
              "image/png"
            ],
            "title": "Content Type"
          },
          "url": {
            "description": "The URL where the file can be downloaded from.",
            "type": "string",
            "title": "Url"
          }
        },
        "title": "File",
        "required": [
          "url"
        ]
      },
      "QwenImageLayeredTrainerInput": {
        "x-fal-order-properties": [
          "image_data_url",
          "learning_rate",
          "steps",
          "default_caption"
        ],
        "type": "object",
        "properties": {
          "steps": {
            "description": "Number of steps to train for",
            "type": "integer",
            "minimum": 100,
            "maximum": 10000,
            "title": "Steps",
            "multipleOf": 100,
            "default": 1000
          },
          "image_data_url": {
            "title": "Image Data Url",
            "type": "string",
            "description": "\n    URL to the input data zip archive.\n\n    The zip should contain groups of images. The images should be named:\n\n    ROOT_start.EXT, ROOT_end.EXT, ROOT_end2.EXT, ..., ROOT_endN.EXT\n    For example:\n    photo_start.png, photo_end.png, photo_end2.png, ..., photo_endN.png\n\n    The start image is the base image that will be decomposed into layers.\n    The end images are the layers that will be added to the base image.  ROOT_end.EXT is the first layer, ROOT_end2.EXT is the second layer, and so on.\n    You can have up to 8 layers.\n    All image groups must have the same number of output layers.\n\n    The end images can contain transparent regions. Only PNG and WebP images are supported since these are the only formats that support transparency.\n\n    The zip can also contain a text file for each image group. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify a description of the base image.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    "
          },
          "learning_rate": {
            "title": "Learning Rate",
            "type": "number",
            "description": "Learning rate for LoRA parameters.",
            "default": 0.0001
          },
          "default_caption": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error.",
            "title": "Default Caption"
          }
        },
        "title": "Input",
        "required": [
          "image_data_url"
        ]
      },
      "QwenImageLayeredTrainerOutput": {
        "x-fal-order-properties": [
          "diffusers_lora_file",
          "config_file"
        ],
        "type": "object",
        "properties": {
          "config_file": {
            "description": "URL to the configuration file for the trained model.",
            "$ref": "#/components/schemas/FalAiQwenImageLayeredTrainer_File"
          },
          "diffusers_lora_file": {
            "description": "URL to the trained diffusers lora weights.",
            "$ref": "#/components/schemas/FalAiQwenImageLayeredTrainer_File"
          }
        },
        "title": "Output",
        "required": [
          "diffusers_lora_file",
          "config_file"
        ]
      },
      "FalAiQwenImageLayeredTrainer_File": {
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size"
        ],
        "type": "object",
        "properties": {
          "file_size": {
            "examples": [
              4404019
            ],
            "title": "File Size",
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "type": "null"
              }
            ],
            "description": "The size of the file in bytes."
          },
          "file_name": {
            "examples": [
              "z9RV14K95DvU.png"
            ],
            "title": "File Name",
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "The name of the file. It will be auto-generated if not provided."
          },
          "content_type": {
            "examples": [
              "image/png"
            ],
            "title": "Content Type",
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "The mime type of the file."
          },
          "url": {
            "title": "Url",
            "type": "string",
            "description": "The URL where the file can be downloaded from."
          }
        },
        "title": "File",
        "required": [
          "url"
        ]
      },
      "QwenImageEdit2509TrainerInput": {
        "title": "InputPlus",
        "type": "object",
        "properties": {
          "steps": {
            "description": "Number of steps to train for",
            "type": "integer",
            "minimum": 100,
            "title": "Steps",
            "maximum": 30000,
            "multipleOf": 100,
            "default": 1000
          },
          "image_data_url": {
            "title": "Image Data Url",
            "type": "string",
            "description": "\n    URL to the input data zip archive.\n\n    The zip should contain pairs of images. The images should be named:\n\n    ROOT_start.EXT and ROOT_end.EXT\n    For example:\n    photo_start.jpg and photo_end.jpg\n\n    The zip can also contain more than one reference image for each image pair. The reference images should be named:\n    ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT\n    For example:\n    photo_start.jpg, photo_start2.jpg, photo_end.jpg\n\n    The Reference Image Count field should be set to the number of reference images.\n\n    The zip can also contain a text file for each image pair. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    "
          },
          "learning_rate": {
            "title": "Learning Rate",
            "type": "number",
            "description": "Learning rate for LoRA parameters.",
            "default": 0.0001
          },
          "default_caption": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error.",
            "title": "Default Caption"
          }
        },
        "x-fal-order-properties": [
          "image_data_url",
          "learning_rate",
          "steps",
          "default_caption"
        ],
        "required": [
          "image_data_url"
        ]
      },
      "QwenImageEdit2509TrainerOutput": {
        "title": "Output",
        "type": "object",
        "properties": {
          "config_file": {
            "description": "URL to the configuration file for the trained model.",
            "$ref": "#/components/schemas/FalAiQwenImageEdit2509Trainer_File"
          },
          "diffusers_lora_file": {
            "description": "URL to the trained diffusers lora weights.",
            "$ref": "#/components/schemas/FalAiQwenImageEdit2509Trainer_File"
          }
        },
        "x-fal-order-properties": [
          "diffusers_lora_file",
          "config_file"
        ],
        "required": [
          "diffusers_lora_file",
          "config_file"
        ]
      },
      "FalAiQwenImageEdit2509Trainer_File": {
        "title": "File",
        "type": "object",
        "properties": {
          "file_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "type": "null"
              }
            ],
            "description": "The size of the file in bytes.",
            "title": "File Size",
            "examples": [
              4404019
            ]
          },
          "file_name": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "The name of the file. It will be auto-generated if not provided.",
            "title": "File Name",
            "examples": [
              "z9RV14K95DvU.png"
            ]
          },
          "content_type": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "The mime type of the file.",
            "title": "Content Type",
            "examples": [
              "image/png"
            ]
          },
          "url": {
            "title": "Url",
            "type": "string",
            "description": "The URL where the file can be downloaded from."
          }
        },
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size"
        ],
        "required": [
          "url"
        ]
      },
      "ZImageTrainerInput": {
        "title": "Input",
        "type": "object",
        "properties": {
          "steps": {
            "description": "Total number of training steps.",
            "type": "integer",
            "minimum": 100,
            "title": "Steps",
            "maximum": 10000,
            "multipleOf": 100,
            "default": 1000
          },
          "image_data_url": {
            "title": "Image Data Url",
            "type": "string",
            "description": "\n    URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.\n\n    The zip can also contain a text file for each image. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    "
          },
          "training_type": {
            "enum": [
              "content",
              "style",
              "balanced"
            ],
            "title": "Training Type",
            "type": "string",
            "description": "Type of training to perform. Use 'content' to focus on the content of the images, 'style' to focus on the style of the images, and 'balanced' to focus on a combination of both.",
            "default": "balanced"
          },
          "learning_rate": {
            "title": "Learning Rate",
            "type": "number",
            "description": "Learning rate applied to trainable parameters.",
            "default": 0.0001
          },
          "default_caption": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Default Caption",
            "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
          }
        },
        "x-fal-order-properties": [
          "image_data_url",
          "steps",
          "learning_rate",
          "default_caption",
          "training_type"
        ],
        "required": [
          "image_data_url"
        ]
      },
      "ZImageTrainerOutput": {
        "title": "Output",
        "type": "object",
        "properties": {
          "config_file": {
            "description": "URL to the configuration file for the trained model.",
            "$ref": "#/components/schemas/FalAiZImageTrainer_File"
          },
          "diffusers_lora_file": {
            "description": "URL to the trained diffusers lora weights.",
            "$ref": "#/components/schemas/FalAiZImageTrainer_File"
          }
        },
        "x-fal-order-properties": [
          "diffusers_lora_file",
          "config_file"
        ],
        "required": [
          "diffusers_lora_file",
          "config_file"
        ]
      },
      "FalAiZImageTrainer_File": {
        "title": "File",
        "type": "object",
        "properties": {
          "file_size": {
            "examples": [
              4404019
            ],
            "title": "File Size",
            "description": "The size of the file in bytes.",
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "type": "null"
              }
            ]
          },
          "file_name": {
            "examples": [
              "z9RV14K95DvU.png"
            ],
            "title": "File Name",
            "description": "The name of the file. It will be auto-generated if not provided.",
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ]
          },
          "content_type": {
            "examples": [
              "image/png"
            ],
            "title": "Content Type",
            "description": "The mime type of the file.",
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ]
          },
          "url": {
            "title": "Url",
            "type": "string",
            "description": "The URL where the file can be downloaded from."
          }
        },
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size"
        ],
        "required": [
          "url"
        ]
      },
      "Flux2TrainerEditInput": {
        "title": "InputEdit",
        "type": "object",
        "properties": {
          "steps": {
            "description": "Total number of training steps.",
            "type": "integer",
            "minimum": 100,
            "title": "Steps",
            "maximum": 10000,
            "multipleOf": 100,
            "default": 1000
          },
          "image_data_url": {
            "title": "Image Data Url",
            "type": "string",
            "description": "\n    URL to the input data zip archive.\n\n    The zip should contain pairs of images. The images should be named:\n\n    ROOT_start.EXT and ROOT_end.EXT\n    For example:\n    photo_start.jpg and photo_end.jpg\n\n    The zip can also contain up to four reference image for each image pair. The reference images should be named:\n    ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT\n    For example:\n    photo_start.jpg, photo_start2.jpg, photo_end.jpg\n\n    The zip can also contain a text file for each image pair. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    "
          },
          "learning_rate": {
            "title": "Learning Rate",
            "type": "number",
            "description": "Learning rate applied to trainable parameters.",
            "default": 0.00005
          },
          "default_caption": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Default Caption",
            "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
          },
          "output_lora_format": {
            "enum": [
              "fal",
              "comfy"
            ],
            "title": "Output Lora Format",
            "type": "string",
            "description": "Dictates the naming scheme for the output weights",
            "default": "fal"
          }
        },
        "x-fal-order-properties": [
          "image_data_url",
          "steps",
          "learning_rate",
          "default_caption",
          "output_lora_format"
        ],
        "required": [
          "image_data_url"
        ]
      },
      "Flux2TrainerEditOutput": {
        "title": "Output",
        "type": "object",
        "properties": {
          "config_file": {
            "description": "URL to the configuration file for the trained model.",
            "$ref": "#/components/schemas/FalAiFlux2TrainerEdit_File"
          },
          "diffusers_lora_file": {
            "description": "URL to the trained diffusers lora weights.",
            "$ref": "#/components/schemas/FalAiFlux2TrainerEdit_File"
          }
        },
        "x-fal-order-properties": [
          "diffusers_lora_file",
          "config_file"
        ],
        "required": [
          "diffusers_lora_file",
          "config_file"
        ]
      },
      "FalAiFlux2TrainerEdit_File": {
        "title": "File",
        "type": "object",
        "properties": {
          "file_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "type": "null"
              }
            ],
            "title": "File Size",
            "description": "The size of the file in bytes.",
            "examples": [
              4404019
            ]
          },
          "file_name": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "File Name",
            "description": "The name of the file. It will be auto-generated if not provided.",
            "examples": [
              "z9RV14K95DvU.png"
            ]
          },
          "content_type": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Content Type",
            "description": "The mime type of the file.",
            "examples": [
              "image/png"
            ]
          },
          "url": {
            "title": "Url",
            "type": "string",
            "description": "The URL where the file can be downloaded from."
          }
        },
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size"
        ],
        "required": [
          "url"
        ]
      },
      "Flux2TrainerInput": {
        "title": "InputT2I",
        "type": "object",
        "properties": {
          "steps": {
            "description": "Total number of training steps.",
            "type": "integer",
            "minimum": 100,
            "title": "Steps",
            "maximum": 10000,
            "multipleOf": 100,
            "default": 1000
          },
          "image_data_url": {
            "title": "Image Data Url",
            "type": "string",
            "description": "\n    URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.\n\n    The zip can also contain a text file for each image. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    "
          },
          "learning_rate": {
            "title": "Learning Rate",
            "type": "number",
            "description": "Learning rate applied to trainable parameters.",
            "default": 0.00005
          },
          "default_caption": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Default Caption",
            "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
          },
          "output_lora_format": {
            "enum": [
              "fal",
              "comfy"
            ],
            "title": "Output Lora Format",
            "type": "string",
            "description": "Dictates the naming scheme for the output weights",
            "default": "fal"
          }
        },
        "x-fal-order-properties": [
          "image_data_url",
          "steps",
          "learning_rate",
          "default_caption",
          "output_lora_format"
        ],
        "required": [
          "image_data_url"
        ]
      },
      "Flux2TrainerOutput": {
        "title": "Output",
        "type": "object",
        "properties": {
          "config_file": {
            "description": "URL to the configuration file for the trained model.",
            "$ref": "#/components/schemas/FalAiFlux2Trainer_File"
          },
          "diffusers_lora_file": {
            "description": "URL to the trained diffusers lora weights.",
            "$ref": "#/components/schemas/FalAiFlux2Trainer_File"
          }
        },
        "x-fal-order-properties": [
          "diffusers_lora_file",
          "config_file"
        ],
        "required": [
          "diffusers_lora_file",
          "config_file"
        ]
      },
      "FalAiFlux2Trainer_File": {
        "title": "File",
        "type": "object",
        "properties": {
          "file_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "type": "null"
              }
            ],
            "title": "File Size",
            "description": "The size of the file in bytes.",
            "examples": [
              4404019
            ]
          },
          "file_name": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "File Name",
            "description": "The name of the file. It will be auto-generated if not provided.",
            "examples": [
              "z9RV14K95DvU.png"
            ]
          },
          "content_type": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Content Type",
            "description": "The mime type of the file.",
            "examples": [
              "image/png"
            ]
          },
          "url": {
            "title": "Url",
            "type": "string",
            "description": "The URL where the file can be downloaded from."
          }
        },
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size"
        ],
        "required": [
          "url"
        ]
      },
      "QwenImageEditPlusTrainerInput": {
        "title": "InputPlus",
        "type": "object",
        "properties": {
          "steps": {
            "description": "Number of steps to train for",
            "type": "integer",
            "minimum": 100,
            "title": "Steps",
            "maximum": 30000,
            "multipleOf": 100,
            "default": 1000
          },
          "image_data_url": {
            "title": "Image Data Url",
            "type": "string",
            "description": "\n    URL to the input data zip archive.\n\n    The zip should contain pairs of images. The images should be named:\n\n    ROOT_start.EXT and ROOT_end.EXT\n    For example:\n    photo_start.jpg and photo_end.jpg\n\n    The zip can also contain more than one reference image for each image pair. The reference images should be named:\n    ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT\n    For example:\n    photo_start.jpg, photo_start2.jpg, photo_end.jpg\n\n    The Reference Image Count field should be set to the number of reference images.\n\n    The zip can also contain a text file for each image pair. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    "
          },
          "learning_rate": {
            "title": "Learning Rate",
            "type": "number",
            "description": "Learning rate for LoRA parameters.",
            "default": 0.0001
          },
          "default_caption": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error.",
            "title": "Default Caption"
          }
        },
        "x-fal-order-properties": [
          "image_data_url",
          "learning_rate",
          "steps",
          "default_caption"
        ],
        "required": [
          "image_data_url"
        ]
      },
      "QwenImageEditPlusTrainerOutput": {
        "title": "Output",
        "type": "object",
        "properties": {
          "config_file": {
            "description": "URL to the configuration file for the trained model.",
            "$ref": "#/components/schemas/FalAiQwenImageEditPlusTrainer_File"
          },
          "diffusers_lora_file": {
            "description": "URL to the trained diffusers lora weights.",
            "$ref": "#/components/schemas/FalAiQwenImageEditPlusTrainer_File"
          }
        },
        "x-fal-order-properties": [
          "diffusers_lora_file",
          "config_file"
        ],
        "required": [
          "diffusers_lora_file",
          "config_file"
        ]
      },
      "FalAiQwenImageEditPlusTrainer_File": {
        "title": "File",
        "type": "object",
        "properties": {
          "file_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "type": "null"
              }
            ],
            "description": "The size of the file in bytes.",
            "title": "File Size",
            "examples": [
              4404019
            ]
          },
          "file_name": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "The name of the file. It will be auto-generated if not provided.",
            "title": "File Name",
            "examples": [
              "z9RV14K95DvU.png"
            ]
          },
          "content_type": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "The mime type of the file.",
            "title": "Content Type",
            "examples": [
              "image/png"
            ]
          },
          "url": {
            "title": "Url",
            "type": "string",
            "description": "The URL where the file can be downloaded from."
          }
        },
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size"
        ],
        "required": [
          "url"
        ]
      },
      "QwenImageEditTrainerInput": {
        "x-fal-order-properties": [
          "image_data_url",
          "learning_rate",
          "steps",
          "default_caption"
        ],
        "type": "object",
        "properties": {
          "steps": {
            "description": "Number of steps to train for",
            "type": "integer",
            "minimum": 100,
            "maximum": 30000,
            "title": "Steps",
            "multipleOf": 100,
            "default": 1000
          },
          "image_data_url": {
            "title": "Image Data Url",
            "type": "string",
            "description": "\n    URL to the input data zip archive.\n\n    The zip should contain pairs of images. The images should be named:\n\n    ROOT_start.EXT and ROOT_end.EXT\n    For example:\n    photo_start.jpg and photo_end.jpg\n\n    The zip can also contain a text file for each image pair. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    "
          },
          "learning_rate": {
            "title": "Learning Rate",
            "type": "number",
            "description": "Learning rate for LoRA parameters.",
            "default": 0.0001
          },
          "default_caption": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Default Caption",
            "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
          }
        },
        "title": "InputEdit",
        "required": [
          "image_data_url"
        ]
      },
      "QwenImageEditTrainerOutput": {
        "x-fal-order-properties": [
          "diffusers_lora_file",
          "config_file"
        ],
        "type": "object",
        "properties": {
          "config_file": {
            "description": "URL to the configuration file for the trained model.",
            "$ref": "#/components/schemas/FalAiQwenImageEditTrainer_File"
          },
          "diffusers_lora_file": {
            "description": "URL to the trained diffusers lora weights.",
            "$ref": "#/components/schemas/FalAiQwenImageEditTrainer_File"
          }
        },
        "title": "Output",
        "required": [
          "diffusers_lora_file",
          "config_file"
        ]
      },
      "FalAiQwenImageEditTrainer_File": {
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size"
        ],
        "type": "object",
        "properties": {
          "file_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "type": "null"
              }
            ],
            "title": "File Size",
            "examples": [
              4404019
            ],
            "description": "The size of the file in bytes."
          },
          "file_name": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "File Name",
            "examples": [
              "z9RV14K95DvU.png"
            ],
            "description": "The name of the file. It will be auto-generated if not provided."
          },
          "content_type": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Content Type",
            "examples": [
              "image/png"
            ],
            "description": "The mime type of the file."
          },
          "url": {
            "title": "Url",
            "type": "string",
            "description": "The URL where the file can be downloaded from."
          }
        },
        "title": "File",
        "required": [
          "url"
        ]
      },
      "QwenImageTrainerInput": {
        "title": "PublicInput",
        "type": "object",
        "properties": {
          "steps": {
            "description": "Total number of training steps to perform. Default is 4000.",
            "type": "integer",
            "minimum": 1,
            "maximum": 8000,
            "title": "Steps",
            "default": 1000
          },
          "image_data_url": {
            "title": "Image Data Url",
            "type": "string",
            "description": "\n        URL to zip archive with images for training. The archive should contain images and corresponding text files with captions.\n        Each text file should have the same name as the image file it corresponds to (e.g., image1.jpg and image1.txt).\n        If text files are missing for some images, you can provide a trigger_phrase to automatically create them.\n        Supported image formats: PNG, JPG, JPEG, WEBP.\n        Try to use at least 10 images, although more is better.\n    "
          },
          "learning_rate": {
            "minimum": 0.000001,
            "maximum": 0.01,
            "type": "number",
            "title": "Learning Rate",
            "description": "Learning rate for training. Default is 5e-4",
            "default": 0.0005
          },
          "trigger_phrase": {
            "title": "Trigger Phrase",
            "type": "string",
            "description": "Default caption to use for images that don't have corresponding text files. If provided, missing .txt files will be created automatically.",
            "default": ""
          }
        },
        "x-fal-order-properties": [
          "image_data_url",
          "steps",
          "learning_rate",
          "trigger_phrase"
        ],
        "required": [
          "image_data_url"
        ]
      },
      "QwenImageTrainerOutput": {
        "title": "Output",
        "type": "object",
        "properties": {
          "lora_file": {
            "title": "Lora File",
            "description": "URL to the trained LoRA weights file.",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiQwenImageTrainer_File"
              }
            ]
          },
          "config_file": {
            "title": "Config File",
            "description": "URL to the training configuration file.",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiQwenImageTrainer_File"
              }
            ]
          }
        },
        "x-fal-order-properties": [
          "lora_file",
          "config_file"
        ],
        "required": [
          "lora_file",
          "config_file"
        ]
      },
      "FalAiQwenImageTrainer_File": {
        "title": "File",
        "type": "object",
        "properties": {
          "file_size": {
            "examples": [
              4404019
            ],
            "title": "File Size",
            "type": "integer",
            "description": "The size of the file in bytes."
          },
          "file_name": {
            "examples": [
              "z9RV14K95DvU.png"
            ],
            "title": "File Name",
            "type": "string",
            "description": "The name of the file. It will be auto-generated if not provided."
          },
          "content_type": {
            "examples": [
              "image/png"
            ],
            "title": "Content Type",
            "type": "string",
            "description": "The mime type of the file."
          },
          "url": {
            "title": "Url",
            "type": "string",
            "description": "The URL where the file can be downloaded from."
          },
          "file_data": {
            "format": "binary",
            "title": "File Data",
            "type": "string",
            "description": "File data"
          }
        },
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size",
          "file_data"
        ],
        "required": [
          "url"
        ]
      },
      "Wan22ImageTrainerInput": {
        "x-fal-order-properties": [
          "training_data_url",
          "trigger_phrase",
          "include_synthetic_captions",
          "use_face_detection",
          "use_face_cropping",
          "use_masks",
          "steps",
          "learning_rate",
          "is_style"
        ],
        "type": "object",
        "properties": {
          "trigger_phrase": {
            "description": "Trigger phrase for the model.",
            "type": "string",
            "title": "Trigger Phrase"
          },
          "use_masks": {
            "examples": [
              true
            ],
            "description": "Whether to use masks for the training data.",
            "type": "boolean",
            "title": "Use Masks",
            "default": true
          },
          "learning_rate": {
            "description": "Learning rate for training.",
            "type": "number",
            "examples": [
              0.0007
            ],
            "title": "Learning Rate",
            "minimum": 0.000001,
            "maximum": 0.1,
            "multipleOf": 0.000001,
            "default": 0.0007
          },
          "use_face_cropping": {
            "examples": [
              false
            ],
            "description": "Whether to use face cropping for the training data. When enabled, images will be cropped to the face before resizing.",
            "type": "boolean",
            "title": "Use Face Cropping",
            "default": false
          },
          "training_data_url": {
            "description": "URL to the training data.",
            "type": "string",
            "title": "Training Data URL"
          },
          "steps": {
            "description": "Number of training steps.",
            "type": "integer",
            "minimum": 10,
            "title": "Number of Steps",
            "examples": [
              1000
            ],
            "maximum": 6000,
            "default": 1000
          },
          "include_synthetic_captions": {
            "description": "Whether to include synthetic captions.",
            "type": "boolean",
            "title": "Include Synthetic Captions",
            "default": false
          },
          "is_style": {
            "examples": [
              false
            ],
            "description": "Whether the training data is style data. If true, face specific options like masking and face detection will be disabled.",
            "type": "boolean",
            "title": "Is Style",
            "default": false
          },
          "use_face_detection": {
            "examples": [
              true
            ],
            "description": "Whether to use face detection for the training data. When enabled, images will use the center of the face as the center of the image when resizing.",
            "type": "boolean",
            "title": "Use Face Detection",
            "default": true
          }
        },
        "title": "BasicInput",
        "required": [
          "training_data_url",
          "trigger_phrase"
        ]
      },
      "Wan22ImageTrainerOutput": {
        "x-fal-order-properties": [
          "diffusers_lora_file",
          "high_noise_lora",
          "config_file"
        ],
        "type": "object",
        "properties": {
          "config_file": {
            "description": "Config file helping inference endpoints after training.",
            "title": "Config File",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiWan22ImageTrainer_File"
              }
            ]
          },
          "high_noise_lora": {
            "description": "High noise LoRA file.",
            "title": "High Noise LoRA",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiWan22ImageTrainer_File"
              }
            ]
          },
          "diffusers_lora_file": {
            "description": "Low noise LoRA file.",
            "title": "Low Noise LoRA",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiWan22ImageTrainer_File"
              }
            ]
          }
        },
        "title": "WanTrainerResponse",
        "required": [
          "diffusers_lora_file",
          "high_noise_lora",
          "config_file"
        ]
      },
      "FalAiWan22ImageTrainer_File": {
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size",
          "file_data"
        ],
        "type": "object",
        "properties": {
          "file_size": {
            "examples": [
              4404019
            ],
            "description": "The size of the file in bytes.",
            "type": "integer",
            "title": "File Size"
          },
          "file_name": {
            "examples": [
              "z9RV14K95DvU.png"
            ],
            "description": "The name of the file. It will be auto-generated if not provided.",
            "type": "string",
            "title": "File Name"
          },
          "content_type": {
            "examples": [
              "image/png"
            ],
            "description": "The mime type of the file.",
            "type": "string",
            "title": "Content Type"
          },
          "url": {
            "description": "The URL where the file can be downloaded from.",
            "type": "string",
            "title": "Url"
          },
          "file_data": {
            "format": "binary",
            "description": "File data",
            "type": "string",
            "title": "File Data"
          }
        },
        "title": "File",
        "required": [
          "url"
        ]
      },
      "FluxKontextTrainerInput": {
        "title": "Input",
        "type": "object",
        "properties": {
          "steps": {
            "description": "Number of steps to train for",
            "type": "integer",
            "minimum": 2,
            "maximum": 10000,
            "title": "Steps",
            "default": 1000
          },
          "image_data_url": {
            "title": "Image Data Url",
            "type": "string",
            "description": "\n        URL to the input data zip archive.\n\n        The zip should contain pairs of images. The images should be named:\n\n        ROOT_start.EXT and ROOT_end.EXT\n        For example:\n        photo_start.jpg and photo_end.jpg\n\n        The zip can also contain a text file for each image pair. The text file should be named:\n        ROOT.txt\n        For example:\n        photo.txt\n\n        This text file can be used to specify the edit instructions for the image pair.\n\n        If no text file is provided, the default_caption will be used.\n\n        If no default_caption is provided, the training will fail.\n        "
          },
          "learning_rate": {
            "title": "Learning Rate",
            "type": "number",
            "default": 0.0001
          },
          "default_caption": {
            "title": "Default Caption",
            "type": "string",
            "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
          },
          "output_lora_format": {
            "enum": [
              "fal",
              "comfy"
            ],
            "title": "Output Lora Format",
            "type": "string",
            "description": "Dictates the naming scheme for the output weights",
            "default": "fal"
          }
        },
        "x-fal-order-properties": [
          "image_data_url",
          "steps",
          "learning_rate",
          "default_caption",
          "output_lora_format"
        ],
        "required": [
          "image_data_url"
        ]
      },
      "FluxKontextTrainerOutput": {
        "title": "Output",
        "type": "object",
        "properties": {
          "config_file": {
            "title": "Config File",
            "description": "URL to the configuration file for the trained model.",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiFluxKontextTrainer_File"
              }
            ]
          },
          "diffusers_lora_file": {
            "title": "Diffusers Lora File",
            "description": "URL to the trained diffusers lora weights.",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiFluxKontextTrainer_File"
              }
            ]
          }
        },
        "x-fal-order-properties": [
          "diffusers_lora_file",
          "config_file"
        ],
        "required": [
          "diffusers_lora_file",
          "config_file"
        ]
      },
      "FalAiFluxKontextTrainer_File": {
        "title": "File",
        "type": "object",
        "properties": {
          "file_size": {
            "examples": [
              4404019
            ],
            "title": "File Size",
            "type": "integer",
            "description": "The size of the file in bytes."
          },
          "file_name": {
            "examples": [
              "z9RV14K95DvU.png"
            ],
            "title": "File Name",
            "type": "string",
            "description": "The name of the file. It will be auto-generated if not provided."
          },
          "content_type": {
            "examples": [
              "image/png"
            ],
            "title": "Content Type",
            "type": "string",
            "description": "The mime type of the file."
          },
          "url": {
            "title": "Url",
            "type": "string",
            "description": "The URL where the file can be downloaded from."
          },
          "file_data": {
            "format": "binary",
            "title": "File Data",
            "type": "string",
            "description": "File data"
          }
        },
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size",
          "file_data"
        ],
        "required": [
          "url"
        ]
      },
      "WanTrainerT2vInput": {
        "title": "Input",
        "type": "object",
        "properties": {
          "number_of_steps": {
            "minimum": 1,
            "maximum": 20000,
            "type": "integer",
            "title": "Number Of Steps",
            "description": "The number of steps to train for.",
            "default": 400
          },
          "training_data_url": {
            "title": "Training Data URL",
            "type": "string",
            "description": "URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.\n\n        In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to."
          },
          "trigger_phrase": {
            "title": "Trigger Phrase",
            "type": "string",
            "description": "The phrase that will trigger the model to generate an image.",
            "default": ""
          },
          "learning_rate": {
            "minimum": 0.000001,
            "maximum": 1,
            "type": "number",
            "title": "Learning Rate",
            "description": "The rate at which the model learns. Higher values can lead to faster training, but over-fitting.",
            "default": 0.0002
          },
          "auto_scale_input": {
            "examples": [
              true
            ],
            "title": "Auto-Scale Input",
            "type": "boolean",
            "description": "If true, the input will be automatically scale the video to 81 frames at 16fps.",
            "default": false
          }
        },
        "x-fal-order-properties": [
          "training_data_url",
          "number_of_steps",
          "learning_rate",
          "trigger_phrase",
          "auto_scale_input"
        ],
        "required": [
          "training_data_url"
        ]
      },
      "WanTrainerT2vOutput": {
        "title": "Output",
        "type": "object",
        "properties": {
          "lora_file": {
            "title": "Lora File",
            "description": "URL to the trained LoRA weights.",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiWanTrainerT2v_File"
              }
            ]
          },
          "config_file": {
            "title": "Config File",
            "description": "Configuration used for setting up the inference endpoints.",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiWanTrainerT2v_File"
              }
            ]
          }
        },
        "x-fal-order-properties": [
          "lora_file",
          "config_file"
        ],
        "required": [
          "lora_file",
          "config_file"
        ]
      },
      "FalAiWanTrainerT2v_File": {
        "title": "File",
        "type": "object",
        "properties": {
          "file_size": {
            "examples": [
              4404019
            ],
            "title": "File Size",
            "type": "integer",
            "description": "The size of the file in bytes."
          },
          "file_name": {
            "examples": [
              "z9RV14K95DvU.png"
            ],
            "title": "File Name",
            "type": "string",
            "description": "The name of the file. It will be auto-generated if not provided."
          },
          "content_type": {
            "examples": [
              "image/png"
            ],
            "title": "Content Type",
            "type": "string",
            "description": "The mime type of the file."
          },
          "url": {
            "title": "Url",
            "type": "string",
            "description": "The URL where the file can be downloaded from."
          },
          "file_data": {
            "format": "binary",
            "title": "File Data",
            "type": "string",
            "description": "File data"
          }
        },
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size",
          "file_data"
        ],
        "required": [
          "url"
        ]
      },
      "WanTrainerT2v14bInput": {
        "title": "Input",
        "type": "object",
        "properties": {
          "number_of_steps": {
            "minimum": 1,
            "maximum": 20000,
            "type": "integer",
            "title": "Number Of Steps",
            "description": "The number of steps to train for.",
            "default": 400
          },
          "training_data_url": {
            "title": "Training Data URL",
            "type": "string",
            "description": "URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.\n\n        In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to."
          },
          "trigger_phrase": {
            "title": "Trigger Phrase",
            "type": "string",
            "description": "The phrase that will trigger the model to generate an image.",
            "default": ""
          },
          "learning_rate": {
            "minimum": 0.000001,
            "maximum": 1,
            "type": "number",
            "title": "Learning Rate",
            "description": "The rate at which the model learns. Higher values can lead to faster training, but over-fitting.",
            "default": 0.0002
          },
          "auto_scale_input": {
            "examples": [
              true
            ],
            "title": "Auto-Scale Input",
            "type": "boolean",
            "description": "If true, the input will be automatically scale the video to 81 frames at 16fps.",
            "default": false
          }
        },
        "x-fal-order-properties": [
          "training_data_url",
          "number_of_steps",
          "learning_rate",
          "trigger_phrase",
          "auto_scale_input"
        ],
        "required": [
          "training_data_url"
        ]
      },
      "WanTrainerT2v14bOutput": {
        "title": "Output",
        "type": "object",
        "properties": {
          "lora_file": {
            "title": "Lora File",
            "description": "URL to the trained LoRA weights.",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiWanTrainerT2v14b_File"
              }
            ]
          },
          "config_file": {
            "title": "Config File",
            "description": "Configuration used for setting up the inference endpoints.",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiWanTrainerT2v14b_File"
              }
            ]
          }
        },
        "x-fal-order-properties": [
          "lora_file",
          "config_file"
        ],
        "required": [
          "lora_file",
          "config_file"
        ]
      },
      "FalAiWanTrainerT2v14b_File": {
        "title": "File",
        "type": "object",
        "properties": {
          "file_size": {
            "examples": [
              4404019
            ],
            "title": "File Size",
            "type": "integer",
            "description": "The size of the file in bytes."
          },
          "file_name": {
            "examples": [
              "z9RV14K95DvU.png"
            ],
            "title": "File Name",
            "type": "string",
            "description": "The name of the file. It will be auto-generated if not provided."
          },
          "content_type": {
            "examples": [
              "image/png"
            ],
            "title": "Content Type",
            "type": "string",
            "description": "The mime type of the file."
          },
          "url": {
            "title": "Url",
            "type": "string",
            "description": "The URL where the file can be downloaded from."
          },
          "file_data": {
            "format": "binary",
            "title": "File Data",
            "type": "string",
            "description": "File data"
          }
        },
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size",
          "file_data"
        ],
        "required": [
          "url"
        ]
      },
      "WanTrainerI2v720pInput": {
        "title": "Input",
        "type": "object",
        "properties": {
          "number_of_steps": {
            "minimum": 1,
            "maximum": 20000,
            "type": "integer",
            "title": "Number Of Steps",
            "description": "The number of steps to train for.",
            "default": 400
          },
          "training_data_url": {
            "title": "Training Data URL",
            "type": "string",
            "description": "URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.\n\n        In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to."
          },
          "trigger_phrase": {
            "title": "Trigger Phrase",
            "type": "string",
            "description": "The phrase that will trigger the model to generate an image.",
            "default": ""
          },
          "learning_rate": {
            "minimum": 0.000001,
            "maximum": 1,
            "type": "number",
            "title": "Learning Rate",
            "description": "The rate at which the model learns. Higher values can lead to faster training, but over-fitting.",
            "default": 0.0002
          },
          "auto_scale_input": {
            "examples": [
              true
            ],
            "title": "Auto-Scale Input",
            "type": "boolean",
            "description": "If true, the input will be automatically scale the video to 81 frames at 16fps.",
            "default": false
          }
        },
        "x-fal-order-properties": [
          "training_data_url",
          "number_of_steps",
          "learning_rate",
          "trigger_phrase",
          "auto_scale_input"
        ],
        "required": [
          "training_data_url"
        ]
      },
      "WanTrainerI2v720pOutput": {
        "title": "Output",
        "type": "object",
        "properties": {
          "lora_file": {
            "title": "Lora File",
            "description": "URL to the trained LoRA weights.",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiWanTrainerI2v720p_File"
              }
            ]
          },
          "config_file": {
            "title": "Config File",
            "description": "Configuration used for setting up the inference endpoints.",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiWanTrainerI2v720p_File"
              }
            ]
          }
        },
        "x-fal-order-properties": [
          "lora_file",
          "config_file"
        ],
        "required": [
          "lora_file",
          "config_file"
        ]
      },
      "FalAiWanTrainerI2v720p_File": {
        "title": "File",
        "type": "object",
        "properties": {
          "file_size": {
            "examples": [
              4404019
            ],
            "title": "File Size",
            "type": "integer",
            "description": "The size of the file in bytes."
          },
          "file_name": {
            "examples": [
              "z9RV14K95DvU.png"
            ],
            "title": "File Name",
            "type": "string",
            "description": "The name of the file. It will be auto-generated if not provided."
          },
          "content_type": {
            "examples": [
              "image/png"
            ],
            "title": "Content Type",
            "type": "string",
            "description": "The mime type of the file."
          },
          "url": {
            "title": "Url",
            "type": "string",
            "description": "The URL where the file can be downloaded from."
          },
          "file_data": {
            "format": "binary",
            "title": "File Data",
            "type": "string",
            "description": "File data"
          }
        },
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size",
          "file_data"
        ],
        "required": [
          "url"
        ]
      },
      "WanTrainerFlf2v720pInput": {
        "title": "Input",
        "type": "object",
        "properties": {
          "number_of_steps": {
            "minimum": 1,
            "maximum": 20000,
            "type": "integer",
            "title": "Number Of Steps",
            "description": "The number of steps to train for.",
            "default": 400
          },
          "training_data_url": {
            "title": "Training Data URL",
            "type": "string",
            "description": "URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.\n\n        In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to."
          },
          "trigger_phrase": {
            "title": "Trigger Phrase",
            "type": "string",
            "description": "The phrase that will trigger the model to generate an image.",
            "default": ""
          },
          "learning_rate": {
            "minimum": 0.000001,
            "maximum": 1,
            "type": "number",
            "title": "Learning Rate",
            "description": "The rate at which the model learns. Higher values can lead to faster training, but over-fitting.",
            "default": 0.0002
          },
          "auto_scale_input": {
            "examples": [
              true
            ],
            "title": "Auto-Scale Input",
            "type": "boolean",
            "description": "If true, the input will be automatically scale the video to 81 frames at 16fps.",
            "default": false
          }
        },
        "x-fal-order-properties": [
          "training_data_url",
          "number_of_steps",
          "learning_rate",
          "trigger_phrase",
          "auto_scale_input"
        ],
        "required": [
          "training_data_url"
        ]
      },
      "WanTrainerFlf2v720pOutput": {
        "title": "Output",
        "type": "object",
        "properties": {
          "lora_file": {
            "title": "Lora File",
            "description": "URL to the trained LoRA weights.",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiWanTrainerFlf2v720p_File"
              }
            ]
          },
          "config_file": {
            "title": "Config File",
            "description": "Configuration used for setting up the inference endpoints.",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiWanTrainerFlf2v720p_File"
              }
            ]
          }
        },
        "x-fal-order-properties": [
          "lora_file",
          "config_file"
        ],
        "required": [
          "lora_file",
          "config_file"
        ]
      },
      "FalAiWanTrainerFlf2v720p_File": {
        "title": "File",
        "type": "object",
        "properties": {
          "file_size": {
            "examples": [
              4404019
            ],
            "title": "File Size",
            "type": "integer",
            "description": "The size of the file in bytes."
          },
          "file_name": {
            "examples": [
              "z9RV14K95DvU.png"
            ],
            "title": "File Name",
            "type": "string",
            "description": "The name of the file. It will be auto-generated if not provided."
          },
          "content_type": {
            "examples": [
              "image/png"
            ],
            "title": "Content Type",
            "type": "string",
            "description": "The mime type of the file."
          },
          "url": {
            "title": "Url",
            "type": "string",
            "description": "The URL where the file can be downloaded from."
          },
          "file_data": {
            "format": "binary",
            "title": "File Data",
            "type": "string",
            "description": "File data"
          }
        },
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size",
          "file_data"
        ],
        "required": [
          "url"
        ]
      },
      "LtxVideoTrainerInput": {
        "x-fal-order-properties": [
          "training_data_url",
          "rank",
          "number_of_steps",
          "number_of_frames",
          "frame_rate",
          "resolution",
          "aspect_ratio",
          "learning_rate",
          "trigger_phrase",
          "auto_scale_input",
          "split_input_into_scenes",
          "split_input_duration_threshold",
          "validation",
          "validation_negative_prompt",
          "validation_number_of_frames",
          "validation_resolution",
          "validation_aspect_ratio",
          "validation_reverse"
        ],
        "type": "object",
        "properties": {
          "number_of_steps": {
            "description": "The number of steps to train for.",
            "type": "integer",
            "minimum": 100,
            "maximum": 20000,
            "title": "Number Of Steps",
            "examples": [
              1000
            ],
            "step": 100,
            "default": 1000
          },
          "frame_rate": {
            "description": "The target frames per second for the video.",
            "type": "integer",
            "minimum": 8,
            "maximum": 60,
            "title": "Frame Rate",
            "examples": [
              25
            ],
            "default": 25
          },
          "learning_rate": {
            "description": "The rate at which the model learns. Higher values can lead to faster training, but over-fitting.",
            "type": "number",
            "minimum": 0.000001,
            "maximum": 1,
            "title": "Learning Rate",
            "examples": [
              0.0002
            ],
            "step": 0.0001,
            "default": 0.0002
          },
          "validation": {
            "description": "A list of validation prompts to use during training. When providing an image, _all_ validation inputs must have an image.",
            "type": "array",
            "maxItems": 2,
            "title": "Validation",
            "items": {
              "$ref": "#/components/schemas/FalAiLtxVideoTrainer_Validation"
            },
            "default": []
          },
          "number_of_frames": {
            "description": "The number of frames to use for training. This is the number of frames per second multiplied by the number of seconds.",
            "type": "integer",
            "minimum": 25,
            "maximum": 121,
            "title": "Number Of Frames",
            "examples": [
              81
            ],
            "default": 81
          },
          "validation_reverse": {
            "description": "If true, the validation videos will be reversed. This is useful for effects that are learned in reverse and then applied in reverse.",
            "type": "boolean",
            "title": "Validation Reverse",
            "default": false
          },
          "training_data_url": {
            "description": "URL to zip archive with videos or images. Try to use at least 10 files, although more is better.\n\n        **Supported video formats:** .mp4, .mov, .avi, .mkv\n        **Supported image formats:** .png, .jpg, .jpeg\n\n        Note: The dataset must contain ONLY videos OR ONLY images - mixed datasets are not supported.\n\n        The archive can also contain text files with captions. Each text file should have the same name as the media file it corresponds to.",
            "type": "string",
            "title": "Training Data Url"
          },
          "split_input_duration_threshold": {
            "description": "The duration threshold in seconds. If a video is longer than this, it will be split into scenes. If you provide captions for a split video, the caption will be applied to each scene. If you do not provide captions, scenes will be auto-captioned.",
            "type": "number",
            "minimum": 1,
            "maximum": 60,
            "title": "Split Input Duration Threshold",
            "examples": [
              30
            ],
            "default": 30
          },
          "rank": {
            "enum": [
              8,
              16,
              32,
              64,
              128
            ],
            "description": "The rank of the LoRA.",
            "type": "integer",
            "title": "Rank",
            "examples": [
              128
            ],
            "default": 128
          },
          "aspect_ratio": {
            "enum": [
              "16:9",
              "1:1",
              "9:16"
            ],
            "description": "The aspect ratio to use for training. This is the aspect ratio of the video.",
            "type": "string",
            "title": "Aspect Ratio",
            "examples": [
              "1:1"
            ],
            "default": "1:1"
          },
          "trigger_phrase": {
            "examples": [
              ""
            ],
            "description": "The phrase that will trigger the model to generate an image.",
            "type": "string",
            "title": "Trigger Phrase",
            "default": ""
          },
          "resolution": {
            "enum": [
              "low",
              "medium",
              "high"
            ],
            "description": "The resolution to use for training. This is the resolution of the video.",
            "type": "string",
            "title": "Resolution",
            "examples": [
              "medium"
            ],
            "default": "medium"
          },
          "split_input_into_scenes": {
            "examples": [
              true
            ],
            "description": "If true, videos above a certain duration threshold will be split into scenes. If you provide captions for a split video, the caption will be applied to each scene. If you do not provide captions, scenes will be auto-captioned. This option has no effect on image datasets.",
            "type": "boolean",
            "title": "Split Input Into Scenes",
            "default": true
          },
          "validation_resolution": {
            "enum": [
              "low",
              "medium",
              "high"
            ],
            "description": "The resolution to use for validation.",
            "type": "string",
            "title": "Validation Resolution",
            "examples": [
              "high"
            ],
            "default": "high"
          },
          "validation_number_of_frames": {
            "description": "The number of frames to use for validation.",
            "type": "integer",
            "minimum": 8,
            "maximum": 121,
            "title": "Validation Number Of Frames",
            "examples": [
              81
            ],
            "default": 81
          },
          "validation_aspect_ratio": {
            "enum": [
              "16:9",
              "1:1",
              "9:16"
            ],
            "description": "The aspect ratio to use for validation.",
            "type": "string",
            "title": "Validation Aspect Ratio",
            "examples": [
              "1:1"
            ],
            "default": "1:1"
          },
          "validation_negative_prompt": {
            "description": "A negative prompt to use for validation.",
            "type": "string",
            "title": "Validation Negative Prompt",
            "default": "blurry, low quality, bad quality, out of focus"
          },
          "auto_scale_input": {
            "examples": [
              false
            ],
            "description": "If true, videos will be automatically scaled to the target frame count and fps. This option has no effect on image datasets.",
            "type": "boolean",
            "title": "Auto Scale Input",
            "default": false
          }
        },
        "title": "Input",
        "required": [
          "training_data_url"
        ]
      },
      "LtxVideoTrainerOutput": {
        "x-fal-order-properties": [
          "video",
          "lora_file",
          "config_file"
        ],
        "type": "object",
        "properties": {
          "lora_file": {
            "description": "URL to the trained LoRA weights.",
            "$ref": "#/components/schemas/FalAiLtxVideoTrainer_File"
          },
          "config_file": {
            "description": "Configuration used for setting up the inference endpoints.",
            "$ref": "#/components/schemas/FalAiLtxVideoTrainer_File"
          },
          "video": {
            "anyOf": [
              {
                "$ref": "#/components/schemas/FalAiLtxVideoTrainer_File"
              },
              {
                "type": "null"
              }
            ],
            "description": "The URL to the validations video."
          }
        },
        "title": "TrainingOutput",
        "required": [
          "video",
          "lora_file",
          "config_file"
        ]
      },
      "FalAiLtxVideoTrainer_Validation": {
        "x-fal-order-properties": [
          "prompt",
          "image_url"
        ],
        "type": "object",
        "properties": {
          "prompt": {
            "description": "The prompt to use for validation.",
            "type": "string",
            "title": "Prompt"
          },
          "image_url": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "An image to use for image-to-video validation. If provided for one validation, _all_ validation inputs must have an image.",
            "title": "Image Url"
          }
        },
        "title": "Validation",
        "required": [
          "prompt"
        ]
      },
      "FalAiLtxVideoTrainer_File": {
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size"
        ],
        "type": "object",
        "properties": {
          "file_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "type": "null"
              }
            ],
            "description": "The size of the file in bytes.",
            "title": "File Size",
            "examples": [
              4404019
            ]
          },
          "file_name": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "The name of the file. It will be auto-generated if not provided.",
            "title": "File Name",
            "examples": [
              "z9RV14K95DvU.png"
            ]
          },
          "content_type": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "The mime type of the file.",
            "title": "Content Type",
            "examples": [
              "image/png"
            ]
          },
          "url": {
            "description": "The URL where the file can be downloaded from.",
            "type": "string",
            "title": "Url"
          }
        },
        "title": "File",
        "required": [
          "url"
        ]
      },
      "RecraftV3CreateStyleInput": {
        "title": "StyleReferenceInput",
        "type": "object",
        "properties": {
          "images_data_url": {
            "title": "Images Data Url",
            "type": "string",
            "description": "URL to zip archive with images, use PNG format. Maximum 5 images are allowed."
          },
          "base_style": {
            "enum": [
              "any",
              "realistic_image",
              "digital_illustration",
              "vector_illustration",
              "realistic_image/b_and_w",
              "realistic_image/hard_flash",
              "realistic_image/hdr",
              "realistic_image/natural_light",
              "realistic_image/studio_portrait",
              "realistic_image/enterprise",
              "realistic_image/motion_blur",
              "realistic_image/evening_light",
              "realistic_image/faded_nostalgia",
              "realistic_image/forest_life",
              "realistic_image/mystic_naturalism",
              "realistic_image/natural_tones",
              "realistic_image/organic_calm",
              "realistic_image/real_life_glow",
              "realistic_image/retro_realism",
              "realistic_image/retro_snapshot",
              "realistic_image/urban_drama",
              "realistic_image/village_realism",
              "realistic_image/warm_folk",
              "digital_illustration/pixel_art",
              "digital_illustration/hand_drawn",
              "digital_illustration/grain",
              "digital_illustration/infantile_sketch",
              "digital_illustration/2d_art_poster",
              "digital_illustration/handmade_3d",
              "digital_illustration/hand_drawn_outline",
              "digital_illustration/engraving_color",
              "digital_illustration/2d_art_poster_2",
              "digital_illustration/antiquarian",
              "digital_illustration/bold_fantasy",
              "digital_illustration/child_book",
              "digital_illustration/child_books",
              "digital_illustration/cover",
              "digital_illustration/crosshatch",
              "digital_illustration/digital_engraving",
              "digital_illustration/expressionism",
              "digital_illustration/freehand_details",
              "digital_illustration/grain_20",
              "digital_illustration/graphic_intensity",
              "digital_illustration/hard_comics",
              "digital_illustration/long_shadow",
              "digital_illustration/modern_folk",
              "digital_illustration/multicolor",
              "digital_illustration/neon_calm",
              "digital_illustration/noir",
              "digital_illustration/nostalgic_pastel",
              "digital_illustration/outline_details",
              "digital_illustration/pastel_gradient",
              "digital_illustration/pastel_sketch",
              "digital_illustration/pop_art",
              "digital_illustration/pop_renaissance",
              "digital_illustration/street_art",
              "digital_illustration/tablet_sketch",
              "digital_illustration/urban_glow",
              "digital_illustration/urban_sketching",
              "digital_illustration/vanilla_dreams",
              "digital_illustration/young_adult_book",
              "digital_illustration/young_adult_book_2",
              "vector_illustration/bold_stroke",
              "vector_illustration/chemistry",
              "vector_illustration/colored_stencil",
              "vector_illustration/contour_pop_art",
              "vector_illustration/cosmics",
              "vector_illustration/cutout",
              "vector_illustration/depressive",
              "vector_illustration/editorial",
              "vector_illustration/emotional_flat",
              "vector_illustration/infographical",
              "vector_illustration/marker_outline",
              "vector_illustration/mosaic",
              "vector_illustration/naivector",
              "vector_illustration/roundish_flat",
              "vector_illustration/segmented_colors",
              "vector_illustration/sharp_contrast",
              "vector_illustration/thin",
              "vector_illustration/vector_photo",
              "vector_illustration/vivid_shapes",
              "vector_illustration/engraving",
              "vector_illustration/line_art",
              "vector_illustration/line_circuit",
              "vector_illustration/linocut"
            ],
            "title": "Base Style",
            "type": "string",
            "description": "The base style of the generated images, this topic is covered above.",
            "default": "digital_illustration"
          }
        },
        "x-fal-order-properties": [
          "images_data_url",
          "base_style"
        ],
        "required": [
          "images_data_url"
        ]
      },
      "RecraftV3CreateStyleOutput": {
        "title": "StyleReferenceOutput",
        "type": "object",
        "properties": {
          "style_id": {
            "format": "uuid4",
            "title": "Style Id",
            "type": "string",
            "description": "The ID of the created style, this ID can be used to reference the style in the future."
          }
        },
        "x-fal-order-properties": [
          "style_id"
        ],
        "required": [
          "style_id"
        ]
      },
      "TurboFluxTrainerInput": {
        "title": "Input",
        "type": "object",
        "properties": {
          "images_data_url": {
            "title": "Images Data Url",
            "type": "string",
            "description": "\n        URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.\n        "
          },
          "trigger_phrase": {
            "title": "Trigger Phrase",
            "type": "string",
            "description": "Trigger phrase to be used in the captions. If None, a trigger word will not be used.\n        If no captions are provide the trigger_work will be used instead of captions. If captions are provided, the trigger word will replace the `[trigger]` string in the captions.\n        ",
            "default": "ohwx"
          },
          "steps": {
            "description": "Number of steps to train the LoRA on.",
            "type": "integer",
            "minimum": 1,
            "maximum": 10000,
            "title": "Steps",
            "examples": [
              1000
            ],
            "default": 1000
          },
          "learning_rate": {
            "description": "Learning rate for the training.",
            "type": "number",
            "minimum": 1e-7,
            "maximum": 0.01,
            "title": "Learning Rate",
            "default": 0.00115
          },
          "training_style": {
            "enum": [
              "subject",
              "style"
            ],
            "title": "Training Style",
            "type": "string",
            "description": "Training style to use.",
            "default": "subject"
          },
          "face_crop": {
            "title": "Face Crop",
            "type": "boolean",
            "description": "Whether to try to detect the face and crop the images to the face.",
            "default": true
          }
        },
        "x-fal-order-properties": [
          "images_data_url",
          "trigger_phrase",
          "steps",
          "learning_rate",
          "training_style",
          "face_crop"
        ],
        "required": [
          "images_data_url"
        ]
      },
      "TurboFluxTrainerOutput": {
        "title": "Output",
        "type": "object",
        "properties": {
          "config_file": {
            "title": "Config File",
            "description": "URL to the trained diffusers config file.",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiTurboFluxTrainer_File"
              }
            ]
          },
          "diffusers_lora_file": {
            "title": "Diffusers Lora File",
            "description": "URL to the trained diffusers lora weights.",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiTurboFluxTrainer_File"
              }
            ]
          }
        },
        "x-fal-order-properties": [
          "diffusers_lora_file",
          "config_file"
        ],
        "required": [
          "diffusers_lora_file",
          "config_file"
        ]
      },
      "FalAiTurboFluxTrainer_File": {
        "title": "File",
        "type": "object",
        "properties": {
          "file_size": {
            "examples": [
              4404019
            ],
            "title": "File Size",
            "type": "integer",
            "description": "The size of the file in bytes."
          },
          "file_name": {
            "examples": [
              "z9RV14K95DvU.png"
            ],
            "title": "File Name",
            "type": "string",
            "description": "The name of the file. It will be auto-generated if not provided."
          },
          "content_type": {
            "examples": [
              "image/png"
            ],
            "title": "Content Type",
            "type": "string",
            "description": "The mime type of the file."
          },
          "url": {
            "title": "Url",
            "type": "string",
            "description": "The URL where the file can be downloaded from."
          },
          "file_data": {
            "format": "binary",
            "title": "File Data",
            "type": "string",
            "description": "File data"
          }
        },
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size",
          "file_data"
        ],
        "required": [
          "url"
        ]
      },
      "WanTrainerInput": {
        "title": "Input",
        "type": "object",
        "properties": {
          "number_of_steps": {
            "minimum": 1,
            "maximum": 20000,
            "type": "integer",
            "title": "Number Of Steps",
            "description": "The number of steps to train for.",
            "default": 400
          },
          "training_data_url": {
            "title": "Training Data URL",
            "type": "string",
            "description": "URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.\n\n        In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to."
          },
          "trigger_phrase": {
            "title": "Trigger Phrase",
            "type": "string",
            "description": "The phrase that will trigger the model to generate an image.",
            "default": ""
          },
          "learning_rate": {
            "minimum": 0.000001,
            "maximum": 1,
            "type": "number",
            "title": "Learning Rate",
            "description": "The rate at which the model learns. Higher values can lead to faster training, but over-fitting.",
            "default": 0.0002
          },
          "auto_scale_input": {
            "examples": [
              true
            ],
            "title": "Auto-Scale Input",
            "type": "boolean",
            "description": "If true, the input will be automatically scale the video to 81 frames at 16fps.",
            "default": false
          }
        },
        "x-fal-order-properties": [
          "training_data_url",
          "number_of_steps",
          "learning_rate",
          "trigger_phrase",
          "auto_scale_input"
        ],
        "required": [
          "training_data_url"
        ]
      },
      "WanTrainerOutput": {
        "title": "Output",
        "type": "object",
        "properties": {
          "lora_file": {
            "title": "Lora File",
            "description": "URL to the trained LoRA weights.",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiWanTrainer_File"
              }
            ]
          },
          "config_file": {
            "title": "Config File",
            "description": "Configuration used for setting up the inference endpoints.",
            "allOf": [
              {
                "$ref": "#/components/schemas/FalAiWanTrainer_File"
              }
            ]
          }
        },
        "x-fal-order-properties": [
          "lora_file",
          "config_file"
        ],
        "required": [
          "lora_file",
          "config_file"
        ]
      },
      "FalAiWanTrainer_File": {
        "title": "File",
        "type": "object",
        "properties": {
          "file_size": {
            "examples": [
              4404019
            ],
            "title": "File Size",
            "type": "integer",
            "description": "The size of the file in bytes."
          },
          "file_name": {
            "examples": [
              "z9RV14K95DvU.png"
            ],
            "title": "File Name",
            "type": "string",
            "description": "The name of the file. It will be auto-generated if not provided."
          },
          "content_type": {
            "examples": [
              "image/png"
            ],
            "title": "Content Type",
            "type": "string",
            "description": "The mime type of the file."
          },
          "url": {
            "title": "Url",
            "type": "string",
            "description": "The URL where the file can be downloaded from."
          },
          "file_data": {
            "format": "binary",
            "title": "File Data",
            "type": "string",
            "description": "File data"
          }
        },
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size",
          "file_data"
        ],
        "required": [
          "url"
        ]
      },
      "HunyuanVideoLoraTrainingInput": {
        "title": "PublicInput",
        "type": "object",
        "properties": {
          "trigger_word": {
            "title": "Trigger Word",
            "type": "string",
            "description": "The trigger word to use.",
            "default": ""
          },
          "images_data_url": {
            "title": "Images Data Url",
            "type": "string",
            "description": "\n        URL to zip archive with images. Try to use at least 4 images in general the more the better.\n\n        In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.\n    "
          },
          "steps": {
            "description": "Number of steps to train the LoRA on.",
            "type": "integer",
            "minimum": 1,
            "maximum": 5000,
            "examples": [
              1000
            ],
            "title": "Steps"
          },
          "data_archive_format": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Data Archive Format",
            "description": "The format of the archive. If not specified, the format will be inferred from the URL.",
            "nullable": true
          },
          "learning_rate": {
            "title": "Learning Rate",
            "type": "number",
            "description": "Learning rate to use for training.",
            "default": 0.0001
          },
          "do_caption": {
            "title": "Do Caption",
            "type": "boolean",
            "description": "Whether to generate captions for the images.",
            "default": true
          }
        },
        "x-fal-order-properties": [
          "images_data_url",
          "steps",
          "trigger_word",
          "learning_rate",
          "do_caption",
          "data_archive_format"
        ],
        "required": [
          "images_data_url",
          "steps"
        ]
      },
      "HunyuanVideoLoraTrainingOutput": {
        "title": "Output",
        "type": "object",
        "properties": {
          "config_file": {
            "description": "URL to the lora configuration file.",
            "$ref": "#/components/schemas/FalAiHunyuanVideoLoraTraining_File"
          },
          "diffusers_lora_file": {
            "description": "URL to the trained diffusers lora weights.",
            "$ref": "#/components/schemas/FalAiHunyuanVideoLoraTraining_File"
          }
        },
        "x-fal-order-properties": [
          "diffusers_lora_file",
          "config_file"
        ],
        "required": [
          "diffusers_lora_file",
          "config_file"
        ]
      },
      "FalAiHunyuanVideoLoraTraining_File": {
        "title": "File",
        "type": "object",
        "properties": {
          "file_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "type": "null"
              }
            ],
            "title": "File Size",
            "description": "The size of the file in bytes.",
            "examples": [
              4404019
            ]
          },
          "file_name": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "File Name",
            "description": "The name of the file. It will be auto-generated if not provided.",
            "examples": [
              "z9RV14K95DvU.png"
            ]
          },
          "content_type": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Content Type",
            "description": "The mime type of the file.",
            "examples": [
              "image/png"
            ]
          },
          "url": {
            "title": "Url",
            "type": "string",
            "description": "The URL where the file can be downloaded from."
          }
        },
        "x-fal-order-properties": [
          "url",
          "content_type",
          "file_name",
          "file_size"
        ],
        "required": [
          "url"
        ]
      }
    },
    "securitySchemes": {
      "apiKeyAuth": {
        "type": "apiKey",
        "in": "header",
        "name": "Authorization"
      }
    }
  }
}