// This file is auto-generated by @hey-api/openapi-ts

export type ClientOptions = {
  baseUrl: `${string}://${string}` | (string & {})
}

export type File = {
  url: string
  content_type?: string
  file_name?: string
  file_size?: number
}

export type QueueStatus = {
  status: 'IN_PROGRESS' | 'COMPLETED' | 'FAILED'
  response_url?: string
}

/**
 * InputRemoveBackgroundModel
 */
export type VideoBackgroundRemovalInput = {
  /**
   * Video Url
   *
   * Input video to remove background from. Size should be less than 14142x14142 and duration less than 30s.
   */
  video_url: string
  /**
   * Output Container And Codec
   *
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, mov_h265, mov_proresks, mkv_h265, mkv_h264, mkv_vp9, gif.
   */
  output_container_and_codec?:
    | 'mp4_h265'
    | 'mp4_h264'
    | 'webm_vp9'
    | 'mov_h265'
    | 'mov_proresks'
    | 'mkv_h265'
    | 'mkv_h264'
    | 'mkv_vp9'
    | 'gif'
  /**
   * Background Color
   *
   * Background color. Options: Transparent, Black, White, Gray, Red, Green, Blue, Yellow, Cyan, Magenta, Orange.
   */
  background_color?:
    | 'Transparent'
    | 'Black'
    | 'White'
    | 'Gray'
    | 'Red'
    | 'Green'
    | 'Blue'
    | 'Yellow'
    | 'Cyan'
    | 'Magenta'
    | 'Orange'
}

/**
 * OutputRemoveBackgroundModel
 */
export type VideoBackgroundRemovalOutput = {
  /**
   * Video
   *
   * Video with removed background and audio.
   */
  video: Video | BriaVideoBackgroundRemovalFile
}

/**
 * Video
 */
export type Video = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * File
 */
export type BriaVideoBackgroundRemovalFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * BaseInput
 */
export type MmaudioV2Input = {
  /**
   * Prompt
   *
   * The prompt to generate the audio for.
   */
  prompt: string
  /**
   * Video Url
   *
   * The URL of the video to generate the audio for.
   */
  video_url: string
  /**
   * Num Steps
   *
   * The number of steps to generate the audio for.
   */
  num_steps?: number
  /**
   * Duration
   *
   * The duration of the audio to generate.
   */
  duration?: number
  /**
   * Cfg Strength
   *
   * The strength of Classifier Free Guidance.
   */
  cfg_strength?: number
  /**
   * Seed
   *
   * The seed for the random number generator
   */
  seed?: number
  /**
   * Mask Away Clip
   *
   * Whether to mask away the clip.
   */
  mask_away_clip?: boolean
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the audio for.
   */
  negative_prompt?: string
}

/**
 * Output
 */
export type MmaudioV2Output = {
  /**
   * Video
   *
   * The generated video with the lip sync.
   */
  video: FalAiMmaudioV2File
}

/**
 * File
 */
export type FalAiMmaudioV2File = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * FaceSwapInputVideo
 *
 * Input schema for image â†” video face swap
 */
export type AiFaceSwapFaceswapvideoInput = {
  /**
   * Source Face Url
   *
   * Source face image
   */
  source_face_url: string
  /**
   * Target Video Url
   *
   * Target video URL
   */
  target_video_url: string
}

/**
 * FaceFusionVideoOutput
 *
 * FaceFusion output payload when video content is generated
 */
export type AiFaceSwapFaceswapvideoOutput = {
  /**
   * Processing Time Ms
   *
   * Optional processing duration in milliseconds
   */
  processing_time_ms?: number | unknown
  video: HalfMoonAiAiFaceSwapFaceswapvideoVideo
}

/**
 * Video
 */
export type HalfMoonAiAiFaceSwapFaceswapvideoVideo = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * LoRAInput
 *
 * LoRA weight configuration.
 */
export type LoRaInput = {
  /**
   * Path
   *
   * URL, HuggingFace repo ID (owner/repo) to lora weights.
   */
  path: string
  /**
   * Scale
   *
   * Scale factor for LoRA application (0.0 to 4.0).
   */
  scale?: number
  /**
   * Weight Name
   *
   * Name of the LoRA weight. Only used if `path` is a HuggingFace repository, and is only required when the repository contains multiple LoRA weights.
   */
  weight_name?: string | unknown
}

/**
 * LTX2VideoToVideoOutput
 */
export type Ltx219bDistilledVideoToVideoLoraOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number
  video: VideoFile
}

/**
 * VideoFile
 */
export type VideoFile = {
  /**
   * Height
   *
   * The height of the video
   */
  height?: number | unknown
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number | unknown
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number | unknown
  /**
   * Width
   *
   * The width of the video
   */
  width?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
}

/**
 * LTX2DistilledVideoToVideoInput
 */
export type Ltx219bDistilledVideoToVideoInput = {
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean
  /**
   * Video URL
   *
   * The URL of the video to generate the video from.
   */
  video_url: string
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: 'none' | 'regular' | 'high' | 'full'
  /**
   * IC-LoRA Scale
   *
   * The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA.
   */
  ic_lora_scale?: number
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | 'dolly_in'
    | 'dolly_out'
    | 'dolly_left'
    | 'dolly_right'
    | 'jib_up'
    | 'jib_down'
    | 'static'
    | 'none'
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | 'auto'
    | 'square_hd'
    | 'square'
    | 'portrait_4_3'
    | 'portrait_16_9'
    | 'landscape_4_3'
    | 'landscape_16_9'
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number
  /**
   * Image Strength
   *
   * The strength of the image to use for the video generation.
   */
  image_strength?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string
  /**
   * Preprocessor
   *
   * The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type.
   */
  preprocessor?: 'depth' | 'canny' | 'pose' | 'none'
  /**
   * IC-LoRA
   *
   * The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)
   */
  ic_lora?:
    | 'match_preprocessor'
    | 'canny'
    | 'depth'
    | 'pose'
    | 'detailer'
    | 'none'
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | 'X264 (.mp4)'
    | 'VP9 (.webm)'
    | 'PRORES4444 (.mov)'
    | 'GIF (.gif)'
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Video Strength
   *
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
   */
  video_strength?: number
  /**
   * Image URL
   *
   * An optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | unknown
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Match Input FPS
   *
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS.
   */
  match_input_fps?: boolean
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown
  /**
   * Match Video Length
   *
   * When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames.
   */
  match_video_length?: boolean
}

/**
 * ImageSize
 */
export type ImageSize = {
  /**
   * Height
   *
   * The height of the generated image.
   */
  height?: number
  /**
   * Width
   *
   * The width of the generated image.
   */
  width?: number
}

/**
 * LTX2VideoToVideoOutput
 */
export type Ltx219bDistilledVideoToVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number
  video: FalAiLtx219bDistilledVideoToVideoVideoFile
}

/**
 * VideoFile
 */
export type FalAiLtx219bDistilledVideoToVideoVideoFile = {
  /**
   * Height
   *
   * The height of the video
   */
  height?: number | unknown
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number | unknown
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number | unknown
  /**
   * Width
   *
   * The width of the video
   */
  width?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
}

/**
 * LoRAInput
 *
 * LoRA weight configuration.
 */
export type FalAiLtx219bVideoToVideoLoraLoRaInput = {
  /**
   * Path
   *
   * URL, HuggingFace repo ID (owner/repo) to lora weights.
   */
  path: string
  /**
   * Scale
   *
   * Scale factor for LoRA application (0.0 to 4.0).
   */
  scale?: number
  /**
   * Weight Name
   *
   * Name of the LoRA weight. Only used if `path` is a HuggingFace repository, and is only required when the repository contains multiple LoRA weights.
   */
  weight_name?: string | unknown
}

/**
 * LTX2VideoToVideoOutput
 */
export type Ltx219bVideoToVideoLoraOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number
  video: FalAiLtx219bVideoToVideoLoraVideoFile
}

/**
 * VideoFile
 */
export type FalAiLtx219bVideoToVideoLoraVideoFile = {
  /**
   * Height
   *
   * The height of the video
   */
  height?: number | unknown
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number | unknown
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number | unknown
  /**
   * Width
   *
   * The width of the video
   */
  width?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
}

/**
 * LTX2VideoToVideoInput
 */
export type Ltx219bVideoToVideoInput = {
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean
  /**
   * Video URL
   *
   * The URL of the video to generate the video from.
   */
  video_url: string
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * IC-LoRA Scale
   *
   * The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA.
   */
  ic_lora_scale?: number
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | FalAiLtx219bVideoToVideoImageSize
    | 'auto'
    | 'square_hd'
    | 'square'
    | 'portrait_4_3'
    | 'portrait_16_9'
    | 'landscape_4_3'
    | 'landscape_16_9'
  /**
   * Guidance Scale
   *
   * The guidance scale to use.
   */
  guidance_scale?: number
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number
  /**
   * Video Strength
   *
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
   */
  video_strength?: number
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | 'X264 (.mp4)'
    | 'VP9 (.webm)'
    | 'PRORES4444 (.mov)'
    | 'GIF (.gif)'
  /**
   * Image URL
   *
   * An optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | unknown
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown
  /**
   * Match Video Length
   *
   * When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames.
   */
  match_video_length?: boolean
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: 'none' | 'regular' | 'high' | 'full'
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | 'dolly_in'
    | 'dolly_out'
    | 'dolly_left'
    | 'dolly_right'
    | 'jib_up'
    | 'jib_down'
    | 'static'
    | 'none'
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Image Strength
   *
   * The strength of the image to use for the video generation.
   */
  image_strength?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string
  /**
   * Preprocessor
   *
   * The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type.
   */
  preprocessor?: 'depth' | 'canny' | 'pose' | 'none'
  /**
   * IC-LoRA
   *
   * The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)
   */
  ic_lora?:
    | 'match_preprocessor'
    | 'canny'
    | 'depth'
    | 'pose'
    | 'detailer'
    | 'none'
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number
  /**
   * Match Input FPS
   *
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS.
   */
  match_input_fps?: boolean
}

/**
 * ImageSize
 */
export type FalAiLtx219bVideoToVideoImageSize = {
  /**
   * Height
   *
   * The height of the generated image.
   */
  height?: number
  /**
   * Width
   *
   * The width of the generated image.
   */
  width?: number
}

/**
 * LTX2VideoToVideoOutput
 */
export type Ltx219bVideoToVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number
  video: FalAiLtx219bVideoToVideoVideoFile
}

/**
 * VideoFile
 */
export type FalAiLtx219bVideoToVideoVideoFile = {
  /**
   * Height
   *
   * The height of the video
   */
  height?: number | unknown
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number | unknown
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number | unknown
  /**
   * Width
   *
   * The width of the video
   */
  width?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
}

/**
 * LoRAInput
 *
 * LoRA weight configuration.
 */
export type FalAiLtx219bDistilledExtendVideoLoraLoRaInput = {
  /**
   * Path
   *
   * URL, HuggingFace repo ID (owner/repo) to lora weights.
   */
  path: string
  /**
   * Scale
   *
   * Scale factor for LoRA application (0.0 to 4.0).
   */
  scale?: number
  /**
   * Weight Name
   *
   * Name of the LoRA weight. Only used if `path` is a HuggingFace repository, and is only required when the repository contains multiple LoRA weights.
   */
  weight_name?: string | unknown
}

/**
 * LTX2ExtendVideoOutput
 */
export type Ltx219bDistilledExtendVideoLoraOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number
  video: FalAiLtx219bDistilledExtendVideoLoraVideoFile
}

/**
 * VideoFile
 */
export type FalAiLtx219bDistilledExtendVideoLoraVideoFile = {
  /**
   * Height
   *
   * The height of the video
   */
  height?: number | unknown
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number | unknown
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number | unknown
  /**
   * Width
   *
   * The width of the video
   */
  width?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
}

/**
 * LTX2DistilledExtendVideoInput
 */
export type Ltx219bDistilledExtendVideoInput = {
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean
  /**
   * Video URL
   *
   * The URL of the video to extend.
   */
  video_url: string
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: 'none' | 'regular' | 'high' | 'full'
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | 'dolly_in'
    | 'dolly_out'
    | 'dolly_left'
    | 'dolly_right'
    | 'jib_up'
    | 'jib_down'
    | 'static'
    | 'none'
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | FalAiLtx219bDistilledExtendVideoImageSize
    | 'auto'
    | 'square_hd'
    | 'square'
    | 'portrait_4_3'
    | 'portrait_16_9'
    | 'landscape_4_3'
    | 'landscape_16_9'
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string
  /**
   * Video Strength
   *
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
   */
  video_strength?: number
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | 'X264 (.mp4)'
    | 'VP9 (.webm)'
    | 'PRORES4444 (.mov)'
    | 'GIF (.gif)'
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Number of Context Frames
   *
   * The number of frames to use as context for the extension.
   */
  num_context_frames?: number
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Match Input FPS
   *
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS.
   */
  match_input_fps?: boolean
  /**
   * Audio Strength
   *
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.
   */
  audio_strength?: number
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown
}

/**
 * ImageSize
 */
export type FalAiLtx219bDistilledExtendVideoImageSize = {
  /**
   * Height
   *
   * The height of the generated image.
   */
  height?: number
  /**
   * Width
   *
   * The width of the generated image.
   */
  width?: number
}

/**
 * LTX2ExtendVideoOutput
 */
export type Ltx219bDistilledExtendVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number
  video: FalAiLtx219bDistilledExtendVideoVideoFile
}

/**
 * VideoFile
 */
export type FalAiLtx219bDistilledExtendVideoVideoFile = {
  /**
   * Height
   *
   * The height of the video
   */
  height?: number | unknown
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number | unknown
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number | unknown
  /**
   * Width
   *
   * The width of the video
   */
  width?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
}

/**
 * LoRAInput
 *
 * LoRA weight configuration.
 */
export type FalAiLtx219bExtendVideoLoraLoRaInput = {
  /**
   * Path
   *
   * URL, HuggingFace repo ID (owner/repo) to lora weights.
   */
  path: string
  /**
   * Scale
   *
   * Scale factor for LoRA application (0.0 to 4.0).
   */
  scale?: number
  /**
   * Weight Name
   *
   * Name of the LoRA weight. Only used if `path` is a HuggingFace repository, and is only required when the repository contains multiple LoRA weights.
   */
  weight_name?: string | unknown
}

/**
 * LTX2ExtendVideoOutput
 */
export type Ltx219bExtendVideoLoraOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number
  video: FalAiLtx219bExtendVideoLoraVideoFile
}

/**
 * VideoFile
 */
export type FalAiLtx219bExtendVideoLoraVideoFile = {
  /**
   * Height
   *
   * The height of the video
   */
  height?: number | unknown
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number | unknown
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number | unknown
  /**
   * Width
   *
   * The width of the video
   */
  width?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
}

/**
 * LTX2ExtendVideoInput
 *
 * extend_direction: ExtendDirection = Field(
 * description="Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning.",
 * default="forward",
 * ui={"important": True},
 * title="Extend Direction",
 * )
 */
export type Ltx219bExtendVideoInput = {
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean
  /**
   * Video URL
   *
   * The URL of the video to extend.
   */
  video_url: string
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: 'none' | 'regular' | 'high' | 'full'
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | 'dolly_in'
    | 'dolly_out'
    | 'dolly_left'
    | 'dolly_right'
    | 'jib_up'
    | 'jib_down'
    | 'static'
    | 'none'
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | FalAiLtx219bExtendVideoImageSize
    | 'auto'
    | 'square_hd'
    | 'square'
    | 'portrait_4_3'
    | 'portrait_16_9'
    | 'landscape_4_3'
    | 'landscape_16_9'
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string
  /**
   * Guidance Scale
   *
   * The guidance scale to use.
   */
  guidance_scale?: number
  /**
   * Video Strength
   *
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
   */
  video_strength?: number
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | 'X264 (.mp4)'
    | 'VP9 (.webm)'
    | 'PRORES4444 (.mov)'
    | 'GIF (.gif)'
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Number of Context Frames
   *
   * The number of frames to use as context for the extension.
   */
  num_context_frames?: number
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Audio Strength
   *
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.
   */
  audio_strength?: number
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number
  /**
   * Match Input FPS
   *
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS.
   */
  match_input_fps?: boolean
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown
}

/**
 * ImageSize
 */
export type FalAiLtx219bExtendVideoImageSize = {
  /**
   * Height
   *
   * The height of the generated image.
   */
  height?: number
  /**
   * Width
   *
   * The width of the generated image.
   */
  width?: number
}

/**
 * LTX2ExtendVideoOutput
 */
export type Ltx219bExtendVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number
  video: FalAiLtx219bExtendVideoVideoFile
}

/**
 * VideoFile
 */
export type FalAiLtx219bExtendVideoVideoFile = {
  /**
   * Height
   *
   * The height of the video
   */
  height?: number | unknown
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number | unknown
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number | unknown
  /**
   * Width
   *
   * The width of the video
   */
  width?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
}

/**
 * EraseByKeyPointsInputModel
 */
export type VideoEraseKeypointsInput = {
  /**
   * Preserve Audio
   *
   * If true, audio will be preserved in the output video.
   */
  preserve_audio?: boolean
  /**
   * Video Url
   *
   * Input video to erase object from. duration must be less than 5s.
   */
  video_url: string
  /**
   * Output Container And Codec
   *
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.
   */
  output_container_and_codec?:
    | 'mp4_h265'
    | 'mp4_h264'
    | 'webm_vp9'
    | 'gif'
    | 'mov_h264'
    | 'mov_h265'
    | 'mov_proresks'
    | 'mkv_h264'
    | 'mkv_h265'
    | 'mkv_vp9'
    | 'mkv_mpeg4'
  /**
   * Keypoints
   *
   * Input keypoints [x,y] to erase or keep from the video. Format like so: {'x':100, 'y':100, 'type':'positive/negative'}
   */
  keypoints: Array<string>
  /**
   * Auto Trim
   *
   * auto trim the video, to working duration ( 5s )
   */
  auto_trim?: boolean
}

/**
 * VideoOutput
 */
export type VideoEraseKeypointsOutput = {
  /**
   * Video
   *
   * Final video.
   */
  video: BriaVideoEraseKeypointsVideo | BriaVideoEraseKeypointsFile
}

/**
 * Video
 */
export type BriaVideoEraseKeypointsVideo = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * File
 */
export type BriaVideoEraseKeypointsFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * EraseByPromptInputModel
 */
export type VideoErasePromptInput = {
  /**
   * Preserve Audio
   *
   * If true, audio will be preserved in the output video.
   */
  preserve_audio?: boolean
  /**
   * Video Url
   *
   * Input video to erase object from. duration must be less than 5s.
   */
  video_url: string
  /**
   * Prompt
   *
   * Input prompt to detect object to erase
   */
  prompt: string
  /**
   * Output Container And Codec
   *
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.
   */
  output_container_and_codec?:
    | 'mp4_h265'
    | 'mp4_h264'
    | 'webm_vp9'
    | 'gif'
    | 'mov_h264'
    | 'mov_h265'
    | 'mov_proresks'
    | 'mkv_h264'
    | 'mkv_h265'
    | 'mkv_vp9'
    | 'mkv_mpeg4'
  /**
   * Auto Trim
   *
   * auto trim the video, to working duration ( 5s )
   */
  auto_trim?: boolean
}

/**
 * VideoOutput
 */
export type VideoErasePromptOutput = {
  /**
   * Video
   *
   * Final video.
   */
  video: BriaVideoErasePromptVideo | BriaVideoErasePromptFile
}

/**
 * Video
 */
export type BriaVideoErasePromptVideo = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * File
 */
export type BriaVideoErasePromptFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * EraseInputModel
 */
export type VideoEraseMaskInput = {
  /**
   * Preserve Audio
   *
   * If true, audio will be preserved in the output video.
   */
  preserve_audio?: boolean
  /**
   * Video Url
   *
   * Input video to erase object from. duration must be less than 5s.
   */
  video_url: string
  /**
   * Output Container And Codec
   *
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.
   */
  output_container_and_codec?:
    | 'mp4_h265'
    | 'mp4_h264'
    | 'webm_vp9'
    | 'gif'
    | 'mov_h264'
    | 'mov_h265'
    | 'mov_proresks'
    | 'mkv_h264'
    | 'mkv_h265'
    | 'mkv_vp9'
    | 'mkv_mpeg4'
  /**
   * Mask Video Url
   *
   * Input video to mask erase object from. duration must be less than 5s.
   */
  mask_video_url: string
  /**
   * Auto Trim
   *
   * auto trim the video, to working duration ( 5s )
   */
  auto_trim?: boolean
}

/**
 * VideoOutput
 */
export type VideoEraseMaskOutput = {
  /**
   * Video
   *
   * Final video.
   */
  video: BriaVideoEraseMaskVideo | BriaVideoEraseMaskFile
}

/**
 * Video
 */
export type BriaVideoEraseMaskVideo = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * File
 */
export type BriaVideoEraseMaskFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * LightXRelightRequest
 *
 * Relighting-only request (minimal schema).
 */
export type LightxRelightInput = {
  /**
   * Prompt
   *
   * Optional text prompt. If omitted, Light-X will auto-caption the video.
   */
  prompt?: string
  /**
   * Video Url
   *
   * URL of the input video.
   */
  video_url: string
  /**
   * Relight Parameters
   *
   * Relighting parameters (required for relight_condition_type='ic'). Not used for 'bg' (which expects a background image URL instead).
   */
  relight_parameters?: RelightParameters
  /**
   * Ref Id
   *
   * Frame index to use as referencen to relight the video with reference.
   */
  ref_id?: number
  /**
   * Relit Cond Img Url
   *
   * URL of conditioning image. Required for relight_condition_type='ref'/'hdr'. Also required for relight_condition_type='bg' (background image).
   */
  relit_cond_img_url?: string
  /**
   * Relit Cond Type
   *
   * Relight condition type.
   */
  relit_cond_type?: 'ic' | 'ref' | 'hdr' | 'bg'
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
}

/**
 * RelightParameters
 *
 * Relighting parameters for video relighting operations.
 *
 * Used with relight_condition_type 'ic' (intrinsic conditioning).
 */
export type RelightParameters = {
  /**
   * Relight Prompt
   *
   * Text prompt describing the desired lighting condition.
   */
  relight_prompt: string
  /**
   * Bg Source
   *
   * Direction of the light source (used for IC-light).
   */
  bg_source?: 'Left' | 'Right' | 'Top' | 'Bottom'
  /**
   * Use Sky Mask
   *
   * Whether to use sky masking for outdoor scenes.
   */
  use_sky_mask?: boolean
  /**
   * Cfg
   *
   * Classifier-free guidance scale for relighting.
   */
  cfg?: number
}

/**
 * LightXOutput
 */
export type LightxRelightOutput = {
  /**
   * Viz Video
   *
   * Optional: visualization/debug video (if produced by the pipeline).
   */
  viz_video?: FalAiLightxRelightFile
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Input Video
   *
   * Optional: normalized/processed input video (if produced by the pipeline).
   */
  input_video?: FalAiLightxRelightFile
  /**
   * Video
   *
   * The generated video file.
   */
  video: FalAiLightxRelightFile
}

/**
 * File
 */
export type FalAiLightxRelightFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * LightXRecameraRequest
 *
 * Re-camera-only request (minimal schema).
 */
export type LightxRecameraInput = {
  /**
   * Prompt
   *
   * Optional text prompt. If omitted, Light-X will auto-caption the video.
   */
  prompt?: string
  /**
   * Trajectory
   *
   * Camera trajectory parameters (required for recamera mode).
   */
  trajectory?: TrajectoryParameters
  /**
   * Video Url
   *
   * URL of the input video.
   */
  video_url: string
  /**
   * Camera
   *
   * Camera control mode.
   */
  camera?: 'traj' | 'target'
  /**
   * Target Pose
   *
   * Target camera pose [theta, phi, radius, x, y] (required when camera='target').
   */
  target_pose?: Array<number>
  /**
   * Mode
   *
   * Camera motion mode.
   */
  mode?: 'gradual' | 'bullet' | 'direct' | 'dolly-zoom'
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
}

/**
 * TrajectoryParameters
 *
 * Camera trajectory parameters for re-camera operations.
 *
 * Each list represents interpolation values across frames:
 * - theta: Horizontal rotation angles (degrees)
 * - phi: Vertical rotation angles (degrees)
 * - radius: Camera distance scaling factors
 */
export type TrajectoryParameters = {
  /**
   * Theta
   *
   * Horizontal rotation angles (degrees) for each keyframe.
   */
  theta: Array<number>
  /**
   * Radius
   *
   * Camera distance scaling factors for each keyframe.
   */
  radius: Array<number>
  /**
   * Phi
   *
   * Vertical rotation angles (degrees) for each keyframe.
   */
  phi: Array<number>
}

/**
 * LightXOutput
 */
export type LightxRecameraOutput = {
  /**
   * Viz Video
   *
   * Optional: visualization/debug video (if produced by the pipeline).
   */
  viz_video?: FalAiLightxRecameraFile
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Input Video
   *
   * Optional: normalized/processed input video (if produced by the pipeline).
   */
  input_video?: FalAiLightxRecameraFile
  /**
   * Video
   *
   * The generated video file.
   */
  video: FalAiLightxRecameraFile
}

/**
 * File
 */
export type FalAiLightxRecameraFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * MotionControlRequest
 *
 * Request model for motion control video generation.
 */
export type KlingVideoV26StandardMotionControlInput = {
  /**
   * Prompt
   */
  prompt?: string
  /**
   * Video Url
   *
   * Reference video URL. The character actions in the generated video will be consistent with this reference video. Should contain a realistic style character with entire body or upper body visible, including head, without obstruction. Duration limit depends on character_orientation: 10s max for 'image', 30s max for 'video'.
   */
  video_url: string
  /**
   * Character Orientation
   *
   * Controls whether the output character's orientation matches the reference image or video. 'video': orientation matches reference video - better for complex motions (max 30s). 'image': orientation matches reference image - better for following camera movements (max 10s).
   */
  character_orientation: 'image' | 'video'
  /**
   * Keep Original Sound
   *
   * Whether to keep the original sound from the reference video.
   */
  keep_original_sound?: boolean
  /**
   * Image Url
   *
   * Reference image URL. The characters, backgrounds, and other elements in the generated video are based on this reference image. Characters should have clear body proportions, avoid occlusion, and occupy more than 5% of the image area.
   */
  image_url: string
}

/**
 * MotionControlOutput
 *
 * Output model for motion control video generation.
 */
export type KlingVideoV26StandardMotionControlOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: FalAiKlingVideoV26StandardMotionControlFile
}

/**
 * File
 */
export type FalAiKlingVideoV26StandardMotionControlFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * MotionControlRequest
 *
 * Request model for motion control video generation.
 */
export type KlingVideoV26ProMotionControlInput = {
  /**
   * Prompt
   */
  prompt?: string
  /**
   * Video Url
   *
   * Reference video URL. The character actions in the generated video will be consistent with this reference video. Should contain a realistic style character with entire body or upper body visible, including head, without obstruction. Duration limit depends on character_orientation: 10s max for 'image', 30s max for 'video'.
   */
  video_url: string
  /**
   * Character Orientation
   *
   * Controls whether the output character's orientation matches the reference image or video. 'video': orientation matches reference video - better for complex motions (max 30s). 'image': orientation matches reference image - better for following camera movements (max 10s).
   */
  character_orientation: 'image' | 'video'
  /**
   * Keep Original Sound
   *
   * Whether to keep the original sound from the reference video.
   */
  keep_original_sound?: boolean
  /**
   * Image Url
   *
   * Reference image URL. The characters, backgrounds, and other elements in the generated video are based on this reference image. Characters should have clear body proportions, avoid occlusion, and occupy more than 5% of the image area.
   */
  image_url: string
}

/**
 * MotionControlOutput
 *
 * Output model for motion control video generation.
 */
export type KlingVideoV26ProMotionControlOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: FalAiKlingVideoV26ProMotionControlFile
}

/**
 * File
 */
export type FalAiKlingVideoV26ProMotionControlFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * LucyRestyleInput
 */
export type LucyRestyleInput = {
  /**
   * Sync Mode
   *
   *
   * If set to true, the function will wait for the video to be generated
   * and uploaded before returning the response. This will increase the
   * latency of the function but it allows you to get the video directly
   * in the response without going through the CDN.
   *
   */
  sync_mode?: boolean
  /**
   * Video Url
   *
   * URL of the video to edit
   */
  video_url: string
  /**
   * Resolution
   *
   * Resolution of the generated video
   */
  resolution?: '720p'
  /**
   * Prompt
   *
   * Text description of the desired video content
   */
  prompt: string
  /**
   * Seed
   *
   * Seed for video generation
   */
  seed?: number
  /**
   * Enhance Prompt
   *
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean
}

/**
 * LucyRestyleOutput
 */
export type LucyRestyleOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: DecartLucyRestyleFile
}

/**
 * File
 */
export type DecartLucyRestyleFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * ScailRequest
 */
export type ScailInput = {
  /**
   * Prompt
   *
   * The prompt to guide video generation.
   */
  prompt: string
  /**
   * Video Url
   *
   * The URL of the video to use as a reference for the video generation.
   */
  video_url: string
  /**
   * Resolution
   *
   * Output resolution. Outputs 896x512 (landscape) or 512x896 (portrait) based on the input image aspect ratio.
   */
  resolution?: '512p'
  /**
   * Num Inference Steps
   *
   * The number of inference steps to use for the video generation.
   */
  num_inference_steps?: number
  /**
   * Multi Character
   *
   * Enable multi-character mode. Use when driving video has multiple people.
   */
  multi_character?: boolean
  /**
   * Image Url
   *
   * The URL of the image to use as a reference for the video generation.
   */
  image_url: string
}

/**
 * ScailResponse
 */
export type ScailOutput = {
  /**
   * Video
   *
   * The generated video file.
   */
  video: FalAiScailFile
}

/**
 * File
 */
export type FalAiScailFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * CrystalVideoUpscaleInput
 */
export type CrystalVideoUpscalerInput = {
  /**
   * Video Url
   *
   * URL to the input video.
   */
  video_url: string
  /**
   * Scale Factor
   *
   * Scale factor. The scale factor must be chosen such that the upscaled video does not exceed 5K resolution.
   */
  scale_factor?: number
}

/**
 * CrystalVideoUpscaleOutput
 */
export type CrystalVideoUpscalerOutput = {
  /**
   * Video
   *
   * URL to the upscaled video
   */
  video: ClarityaiCrystalVideoUpscalerVideoFile
}

/**
 * VideoFile
 */
export type ClarityaiCrystalVideoUpscalerVideoFile = {
  /**
   * Height
   *
   * The height of the video
   */
  height?: number
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number
  /**
   * Width
   *
   * The width of the video
   */
  width?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * ReferenceToVideoInput
 *
 * Input for Wan 2.6 reference-to-video generation (R2V)
 */
export type V26ReferenceToVideoInput = {
  /**
   * Prompt
   *
   * Use @Video1, @Video2, @Video3 to reference subjects from your videos. Works for people, animals, or objects. For multi-shot prompts: '[0-3s] Shot 1. [3-6s] Shot 2.' Max 800 characters.
   */
  prompt: string
  /**
   * Resolution
   *
   * Video resolution tier. R2V only supports 720p and 1080p (no 480p).
   */
  resolution?: '720p' | '1080p'
  /**
   * Video Urls
   *
   * Reference videos for subject consistency (1-3 videos). Videos' FPS must be at least 16 FPS.Reference in prompt as @Video1, @Video2, @Video3. Works for people, animals, or objects.
   */
  video_urls: Array<string>
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video.
   */
  aspect_ratio?: '16:9' | '9:16' | '1:1' | '4:3' | '3:4'
  /**
   * Duration
   *
   * Duration of the generated video in seconds. R2V supports only 5 or 10 seconds (no 15s).
   */
  duration?: '5' | '10'
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt rewriting using LLM.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
  /**
   * Multi Shots
   *
   * When true (default), enables intelligent multi-shot segmentation for coherent narrative videos with multiple shots. When false, generates single continuous shot. Only active when enable_prompt_expansion is True.
   */
  multi_shots?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt to describe content to avoid. Max 500 characters.
   */
  negative_prompt?: string
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
}

/**
 * ReferenceToVideoOutput
 *
 * Output for reference-to-video generation
 */
export type V26ReferenceToVideoOutput = {
  /**
   * Actual Prompt
   *
   * The actual prompt used if prompt rewriting was enabled
   */
  actual_prompt?: string
  /**
   * Seed
   *
   * The seed used for generation
   */
  seed: number
  /**
   * Video
   *
   * The generated video file
   */
  video: WanV26ReferenceToVideoVideoFile
}

/**
 * VideoFile
 */
export type WanV26ReferenceToVideoVideoFile = {
  /**
   * Height
   *
   * The height of the video
   */
  height?: number
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number
  /**
   * Width
   *
   * The width of the video
   */
  width?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * Veo31VideoToVideoInput
 *
 * Input for video extension/video-to-video generation.
 */
export type Veo31FastExtendVideoInput = {
  /**
   * Prompt
   *
   * The text prompt describing how the video should be extended
   */
  prompt: string
  /**
   * Duration
   *
   * The duration of the generated video.
   */
  duration?: '7s'
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '9:16'
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean
  /**
   * Auto Fix
   *
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean
  /**
   * Video URL
   *
   * URL of the video to extend. The video should be 720p or 1080p resolution in 16:9 or 9:16 aspect ratio.
   */
  video_url: string
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: '720p'
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number
  /**
   * Negative Prompt
   *
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string
}

/**
 * Veo31VideoToVideoOutput
 */
export type Veo31FastExtendVideoOutput = {
  /**
   * Video
   *
   * The extended video.
   */
  video: FalAiVeo31FastExtendVideoFile
}

/**
 * File
 */
export type FalAiVeo31FastExtendVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * Veo31VideoToVideoInput
 *
 * Input for video extension/video-to-video generation.
 */
export type Veo31ExtendVideoInput = {
  /**
   * Prompt
   *
   * The text prompt describing how the video should be extended
   */
  prompt: string
  /**
   * Duration
   *
   * The duration of the generated video.
   */
  duration?: '7s'
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '9:16'
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean
  /**
   * Auto Fix
   *
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean
  /**
   * Video URL
   *
   * URL of the video to extend. The video should be 720p or 1080p resolution in 16:9 or 9:16 aspect ratio.
   */
  video_url: string
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: '720p'
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number
  /**
   * Negative Prompt
   *
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string
}

/**
 * Veo31VideoToVideoOutput
 */
export type Veo31ExtendVideoOutput = {
  /**
   * Video
   *
   * The extended video.
   */
  video: FalAiVeo31ExtendVideoFile
}

/**
 * File
 */
export type FalAiVeo31ExtendVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * OmniVideoElementInput
 */
export type OmniVideoElementInput = {
  /**
   * Reference Image Urls
   *
   * Additional reference images from different angles. 1-3 images supported. At least one image is required.
   */
  reference_image_urls?: Array<string>
  /**
   * Frontal Image Url
   *
   * The frontal image of the element (main view).
   *
   * Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s
   */
  frontal_image_url: string
}

/**
 * OmniV2VReferenceOutput
 */
export type KlingVideoO1StandardVideoToVideoReferenceOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: FalAiKlingVideoO1StandardVideoToVideoReferenceFile
}

/**
 * File
 */
export type FalAiKlingVideoO1StandardVideoToVideoReferenceFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * OmniVideoElementInput
 */
export type FalAiKlingVideoO1StandardVideoToVideoEditOmniVideoElementInput = {
  /**
   * Reference Image Urls
   *
   * Additional reference images from different angles. 1-3 images supported. At least one image is required.
   */
  reference_image_urls?: Array<string>
  /**
   * Frontal Image Url
   *
   * The frontal image of the element (main view).
   *
   * Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s
   */
  frontal_image_url: string
}

/**
 * OmniV2VEditOutput
 */
export type KlingVideoO1StandardVideoToVideoEditOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: FalAiKlingVideoO1StandardVideoToVideoEditFile
}

/**
 * File
 */
export type FalAiKlingVideoO1StandardVideoToVideoEditFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * SteadyDancerRequest
 *
 * Request model for SteadyDancer human animation.
 */
export type SteadyDancerInput = {
  /**
   * Prompt
   *
   * Text prompt describing the desired animation.
   */
  prompt?: string
  /**
   * Video Url
   *
   * URL of the driving pose video. The motion from this video will be transferred to the reference image.
   */
  video_url?: string
  /**
   * Acceleration
   *
   * Acceleration levels.
   */
  acceleration?: 'light' | 'moderate' | 'aggressive'
  /**
   * Pose Guidance Scale
   *
   * Pose guidance scale for pose control strength.
   */
  pose_guidance_scale?: number
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Pose Guidance End
   *
   * End ratio for pose guidance. Controls when pose guidance ends.
   */
  pose_guidance_end?: number
  /**
   * Frames Per Second
   *
   * Frames per second of the generated video. Must be between 5 to 24. If not specified, uses the FPS from the input video.
   */
  frames_per_second?: number
  /**
   * Guidance Scale
   *
   * Classifier-free guidance scale for prompt adherence.
   */
  guidance_scale?: number
  /**
   * Num Frames
   *
   * Number of frames to generate. If not specified, uses the frame count from the input video (capped at 241). Will be adjusted to nearest valid value (must satisfy 4k+1 pattern).
   */
  num_frames?: number
  /**
   * Use Turbo
   *
   * If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized (num_inference_steps=6, guidance_scale=1.0) and uses the LightX2V distillation LoRA.
   */
  use_turbo?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. If 'auto', will be determined from the reference image.
   */
  aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1'
  /**
   * Pose Guidance Start
   *
   * Start ratio for pose guidance. Controls when pose guidance begins.
   */
  pose_guidance_start?: number
  /**
   * Resolution
   *
   * Resolution of the generated video. 576p is default, 720p for higher quality. 480p is lower quality.
   */
  resolution?: '480p' | '576p' | '720p'
  /**
   * Image Url
   *
   * URL of the reference image to animate. This is the person/character whose appearance will be preserved.
   */
  image_url?: string
  /**
   * Preserve Audio
   *
   * If enabled, copies audio from the input driving video to the output video.
   */
  preserve_audio?: boolean
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
}

/**
 * SteadyDancerResponse
 *
 * Response model for SteadyDancer.
 */
export type SteadyDancerOutput = {
  /**
   * Num Frames
   *
   * The actual number of frames generated (aligned to 4k+1 pattern).
   */
  num_frames: number
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated dance animation video.
   */
  video: FalAiSteadyDancerFile
}

/**
 * File
 */
export type FalAiSteadyDancerFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * OneToALLAnimationRequest
 */
export type OneToAllAnimation13bInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * Resolution
   *
   * The resolution of the video to generate.
   */
  resolution?: '480p' | '580p' | '720p'
  /**
   * Image Guidance Scale
   *
   * The image guidance scale to use for the video generation.
   */
  image_guidance_scale?: number
  /**
   * Pose Guidance Scale
   *
   * The pose guidance scale to use for the video generation.
   */
  pose_guidance_scale?: number
  /**
   * Video Url
   *
   * The URL of the video to use as a reference for the video generation.
   */
  video_url: string
  /**
   * Image Url
   *
   * The URL of the image to use as a reference for the video generation.
   */
  image_url: string
  /**
   * Num Inference Steps
   *
   * The number of inference steps to use for the video generation.
   */
  num_inference_steps?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt: string
}

/**
 * OneToALLAnimationResponse
 */
export type OneToAllAnimation13bOutput = {
  /**
   * Video
   *
   * The generated video file.
   */
  video: FalAiOneToAllAnimation13bFile
}

/**
 * File
 */
export type FalAiOneToAllAnimation13bFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * OneToALLAnimationRequest
 */
export type OneToAllAnimation14bInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * Resolution
   *
   * The resolution of the video to generate.
   */
  resolution?: '480p' | '580p' | '720p'
  /**
   * Image Guidance Scale
   *
   * The image guidance scale to use for the video generation.
   */
  image_guidance_scale?: number
  /**
   * Pose Guidance Scale
   *
   * The pose guidance scale to use for the video generation.
   */
  pose_guidance_scale?: number
  /**
   * Video Url
   *
   * The URL of the video to use as a reference for the video generation.
   */
  video_url: string
  /**
   * Image Url
   *
   * The URL of the image to use as a reference for the video generation.
   */
  image_url: string
  /**
   * Num Inference Steps
   *
   * The number of inference steps to use for the video generation.
   */
  num_inference_steps?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt: string
}

/**
 * OneToALLAnimationResponse
 */
export type OneToAllAnimation14bOutput = {
  /**
   * Video
   *
   * The generated video file.
   */
  video: FalAiOneToAllAnimation14bFile
}

/**
 * File
 */
export type FalAiOneToAllAnimation14bFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * Input
 *
 * Input parameters for Wan Vision Enhancer (Video-to-Video)
 */
export type WanVisionEnhancerInput = {
  /**
   * Prompt
   *
   * Optional prompt to prepend to the VLM-generated description. Leave empty to use only the auto-generated description from the video.
   */
  prompt?: string | unknown
  /**
   * Video Url
   *
   * The URL of the video to enhance with Wan Video. Maximum 200MB file size. Videos longer than 500 frames will have only the first 500 frames processed (~8-21 seconds depending on fps).
   */
  video_url: string
  /**
   * Seed
   *
   * Random seed for reproducibility. If not provided, a random seed will be used.
   */
  seed?: number | unknown
  /**
   * Output Resolution
   *
   * Target output resolution for the enhanced video. 720p (native, fast) or 1080p (upscaled, slower). Processing is always done at 720p, then upscaled if 1080p selected.
   */
  target_resolution?: '720p' | '1080p'
  /**
   * Negative Prompt
   *
   * Negative prompt to avoid unwanted features.
   */
  negative_prompt?: string | unknown
  /**
   * Creativity
   *
   * Controls how much the model enhances/changes the video. 0 = Minimal change (preserves original), 1 = Subtle enhancement (default), 2 = Medium enhancement, 3 = Strong enhancement, 4 = Maximum enhancement.
   */
  creativity?: number
}

/**
 * Output
 *
 * Output from Wan Vision Enhancer
 */
export type WanVisionEnhancerOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Timings
   *
   * The timings of the different steps in the workflow.
   */
  timings: {
    [key: string]: number
  }
  /**
   * The enhanced video file.
   */
  video: FalAiWanVisionEnhancerFile
}

/**
 * File
 */
export type FalAiWanVisionEnhancerFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * React1Input
 */
export type SyncLipsyncReact1Input = {
  /**
   * Emotion
   *
   * Emotion prompt for the generation. Currently supports single-word emotions only.
   */
  emotion: 'happy' | 'angry' | 'sad' | 'neutral' | 'disgusted' | 'surprised'
  /**
   * Video Url
   *
   * URL to the input video. Must be **15 seconds or shorter**.
   */
  video_url: string
  /**
   * Lipsync Mode
   *
   * Lipsync mode when audio and video durations are out of sync.
   */
  lipsync_mode?: 'cut_off' | 'loop' | 'bounce' | 'silence' | 'remap'
  /**
   * Audio Url
   *
   * URL to the input audio. Must be **15 seconds or shorter**.
   */
  audio_url: string
  /**
   * Temperature
   *
   * Controls the expresiveness of the lipsync.
   */
  temperature?: number
  /**
   * Model Mode
   *
   * Controls the edit region and movement scope for the model. Available options:
   * - `lips`: Only lipsync using react-1 (minimal facial changes).
   * - `face`: Lipsync + facial expressions without head movements.
   * - `head`: Lipsync + facial expressions + natural talking head movements.
   */
  model_mode?: 'lips' | 'face' | 'head'
}

/**
 * React1Output
 */
export type SyncLipsyncReact1Output = {
  /**
   * Video
   *
   * The generated video with synchronized lip and facial movements.
   */
  video: FalAiSyncLipsyncReact1VideoFile
}

/**
 * VideoFile
 */
export type FalAiSyncLipsyncReact1VideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number
  /**
   * Height
   *
   * The height of the video
   */
  height?: number
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Width
   *
   * The width of the video
   */
  width?: number
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * FastGeneralRembgInput
 */
export type VideoBackgroundRemovalFastInput = {
  /**
   * Video Url
   */
  video_url: string
  /**
   * Subject Is Person
   *
   * Set to False if the subject is not a person.
   */
  subject_is_person?: boolean
  /**
   * Output Codec
   *
   * Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality.
   */
  output_codec?: 'vp9' | 'h264'
  /**
   * Refine Foreground Edges
   *
   * Improves the quality of the extracted object's edges.
   */
  refine_foreground_edges?: boolean
}

/**
 * FastGeneralRembgOutput
 */
export type VideoBackgroundRemovalFastOutput = {
  /**
   * Video
   */
  video: Array<VeedVideoBackgroundRemovalFastFile>
}

/**
 * File
 */
export type VeedVideoBackgroundRemovalFastFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * OmniVideoElementInput
 */
export type FalAiKlingVideoO1VideoToVideoEditOmniVideoElementInput = {
  /**
   * Reference Image Urls
   *
   * Additional reference images from different angles. 1-3 images supported. At least one image is required.
   */
  reference_image_urls?: Array<string>
  /**
   * Frontal Image Url
   *
   * The frontal image of the element (main view).
   *
   * Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s
   */
  frontal_image_url: string
}

/**
 * OmniV2VEditOutput
 */
export type KlingVideoO1VideoToVideoEditOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: FalAiKlingVideoO1VideoToVideoEditFile
}

/**
 * File
 */
export type FalAiKlingVideoO1VideoToVideoEditFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * OmniVideoElementInput
 */
export type FalAiKlingVideoO1VideoToVideoReferenceOmniVideoElementInput = {
  /**
   * Reference Image Urls
   *
   * Additional reference images from different angles. 1-3 images supported. At least one image is required.
   */
  reference_image_urls?: Array<string>
  /**
   * Frontal Image Url
   *
   * The frontal image of the element (main view).
   *
   * Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s
   */
  frontal_image_url: string
}

/**
 * OmniV2VReferenceOutput
 */
export type KlingVideoO1VideoToVideoReferenceOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: FalAiKlingVideoO1VideoToVideoReferenceFile
}

/**
 * File
 */
export type FalAiKlingVideoO1VideoToVideoReferenceFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * GeneralRembgInput
 */
export type VeedVideoBackgroundRemovalVideoBackgroundRemovalInput = {
  /**
   * Video Url
   */
  video_url: string
  /**
   * Subject Is Person
   *
   * Set to False if the subject is not a person.
   */
  subject_is_person?: boolean
  /**
   * Output Codec
   *
   * Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality.
   */
  output_codec?: 'vp9' | 'h264'
  /**
   * Refine Foreground Edges
   *
   * Improves the quality of the extracted object's edges.
   */
  refine_foreground_edges?: boolean
}

/**
 * GeneralRembgOutput
 */
export type VeedVideoBackgroundRemovalVideoBackgroundRemovalOutput = {
  /**
   * Video
   */
  video: Array<VeedVideoBackgroundRemovalFile>
}

/**
 * File
 */
export type VeedVideoBackgroundRemovalFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * GreenScreenRembgInput
 */
export type VideoBackgroundRemovalGreenScreenInput = {
  /**
   * Video Url
   */
  video_url: string
  /**
   * Output Codec
   *
   * Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality.
   */
  output_codec?: 'vp9' | 'h264'
  /**
   * Spill Suppression Strength
   *
   * Increase the value if green spots remain in the video, decrease if color changes are noticed on the extracted subject.
   */
  spill_suppression_strength?: number | unknown
}

/**
 * GreenScreenRembgOutput
 */
export type VideoBackgroundRemovalGreenScreenOutput = {
  /**
   * Video
   */
  video: Array<VeedVideoBackgroundRemovalGreenScreenFile>
}

/**
 * File
 */
export type VeedVideoBackgroundRemovalGreenScreenFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * LTXRetakeVideoRequest
 */
export type Ltx2RetakeVideoInput = {
  /**
   * Prompt
   *
   * The prompt to retake the video with
   */
  prompt: string
  /**
   * Duration
   *
   * The duration of the video to retake in seconds
   */
  duration?: number
  /**
   * Video URL
   *
   * The URL of the video to retake
   */
  video_url: string
  /**
   * Start Time
   *
   * The start time of the video to retake in seconds
   */
  start_time?: number
  /**
   * Retake Mode
   *
   * The retake mode to use for the retake
   */
  retake_mode?: 'replace_audio' | 'replace_video' | 'replace_audio_and_video'
}

/**
 * LTXRetakeVideoResponse
 */
export type Ltx2RetakeVideoOutput = {
  /**
   * Video
   *
   * The generated video file
   */
  video: FalAiLtx2RetakeVideoVideoFile
}

/**
 * VideoFile
 */
export type FalAiLtx2RetakeVideoVideoFile = {
  /**
   * Height
   *
   * The height of the video
   */
  height?: number
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number
  /**
   * Width
   *
   * The width of the video
   */
  width?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * LucyEditFastInput
 */
export type LucyEditFastInput = {
  /**
   * Sync Mode
   *
   *
   * If set to true, the function will wait for the video to be generated
   * and uploaded before returning the response. This will increase the
   * latency of the function but it allows you to get the video directly
   * in the response without going through the CDN.
   *
   */
  sync_mode?: boolean
  /**
   * Video Url
   *
   * URL of the video to edit
   */
  video_url: string
  /**
   * Prompt
   *
   * Text description of the desired video content
   */
  prompt: string
  /**
   * Enhance Prompt
   *
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean
}

/**
 * LucyEditFastOutput
 */
export type LucyEditFastOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: DecartLucyEditFastFile
}

/**
 * File
 */
export type DecartLucyEditFastFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * SAM3VideoRLEInput
 */
export type Sam3VideoRleInput = {
  /**
   * Prompt
   *
   * Text prompt for segmentation. Use commas to track multiple objects (e.g., 'person, cloth').
   */
  prompt?: string
  /**
   * Video Url
   *
   * The URL of the video to be segmented.
   */
  video_url: string
  /**
   * Detection Threshold
   *
   * Detection confidence threshold (0.0-1.0). Lower = more detections but less precise. Defaults: 0.5 for existing, 0.7 for new objects. Try 0.2-0.3 if text prompts fail.
   */
  detection_threshold?: number
  /**
   * Box Prompts
   *
   * List of box prompts with optional frame_index.
   */
  box_prompts?: Array<BoxPrompt>
  /**
   * Boundingbox Zip
   *
   * Return per-frame bounding box overlays as a zip archive.
   */
  boundingbox_zip?: boolean
  /**
   * Point Prompts
   *
   * List of point prompts with frame indices.
   */
  point_prompts?: Array<PointPrompt>
  /**
   * Frame Index
   *
   * Frame index used for initial interaction when mask_url is provided.
   */
  frame_index?: number
  /**
   * Mask Url
   *
   * The URL of the mask to be applied initially.
   */
  mask_url?: string
  /**
   * Apply Mask
   *
   * Apply the mask on the video.
   */
  apply_mask?: boolean
}

/**
 * BoxPrompt
 */
export type BoxPrompt = {
  /**
   * Y Min
   *
   * Y Min Coordinate of the box
   */
  y_min?: number
  /**
   * Object Id
   *
   * Optional object identifier. Boxes sharing an object id refine the same object.
   */
  object_id?: number
  /**
   * Frame Index
   *
   * The frame index to interact with.
   */
  frame_index?: number
  /**
   * X Max
   *
   * X Max Coordinate of the box
   */
  x_max?: number
  /**
   * X Min
   *
   * X Min Coordinate of the box
   */
  x_min?: number
  /**
   * Y Max
   *
   * Y Max Coordinate of the box
   */
  y_max?: number
}

/**
 * PointPrompt
 */
export type PointPrompt = {
  /**
   * Y
   *
   * Y Coordinate of the prompt
   */
  y?: number
  /**
   * X
   *
   * X Coordinate of the prompt
   */
  x?: number
  /**
   * Object Id
   *
   * Optional object identifier. Prompts sharing an object id refine the same object.
   */
  object_id?: number
  /**
   * Frame Index
   *
   * The frame index to interact with.
   */
  frame_index?: number
  /**
   * Label
   *
   * 1 for foreground, 0 for background
   */
  label?: 0 | 1
}

/**
 * SAM3VideoOutput
 */
export type Sam3VideoRleOutput = {
  /**
   * Boundingbox Frames Zip
   *
   * Zip file containing per-frame bounding box overlays.
   */
  boundingbox_frames_zip?: FalAiSam3VideoRleFile
  /**
   * Video
   *
   * The segmented video.
   */
  video: FalAiSam3VideoRleFile
}

/**
 * File
 */
export type FalAiSam3VideoRleFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * SAM3VideoInput
 */
export type Sam3VideoInput = {
  /**
   * Prompt
   *
   * Text prompt for segmentation. Use commas to track multiple objects (e.g., 'person, cloth').
   */
  prompt?: string
  /**
   * Video Url
   *
   * The URL of the video to be segmented.
   */
  video_url: string
  /**
   * Detection Threshold
   *
   * Detection confidence threshold (0.0-1.0). Lower = more detections but less precise.
   */
  detection_threshold?: number
  /**
   * Box Prompts
   *
   * List of box prompt coordinates (x_min, y_min, x_max, y_max).
   */
  box_prompts?: Array<BoxPromptBase>
  /**
   * Point Prompts
   *
   * List of point prompts
   */
  point_prompts?: Array<PointPromptBase>
  /**
   * Apply Mask
   *
   * Apply the mask on the video.
   */
  apply_mask?: boolean
  /**
   * Text Prompt
   *
   * [DEPRECATED] Use 'prompt' instead. Kept for backward compatibility.
   *
   * @deprecated
   */
  text_prompt?: string
}

/**
 * BoxPromptBase
 */
export type BoxPromptBase = {
  /**
   * Y Min
   *
   * Y Min Coordinate of the box
   */
  y_min?: number
  /**
   * Object Id
   *
   * Optional object identifier. Boxes sharing an object id refine the same object.
   */
  object_id?: number
  /**
   * X Max
   *
   * X Max Coordinate of the box
   */
  x_max?: number
  /**
   * X Min
   *
   * X Min Coordinate of the box
   */
  x_min?: number
  /**
   * Y Max
   *
   * Y Max Coordinate of the box
   */
  y_max?: number
}

/**
 * PointPromptBase
 */
export type PointPromptBase = {
  /**
   * Y
   *
   * Y Coordinate of the prompt
   */
  y?: number
  /**
   * X
   *
   * X Coordinate of the prompt
   */
  x?: number
  /**
   * Object Id
   *
   * Optional object identifier. Prompts sharing an object id refine the same object.
   */
  object_id?: number
  /**
   * Label
   *
   * 1 for foreground, 0 for background
   */
  label?: 0 | 1
}

/**
 * SAM3VideoOutput
 */
export type Sam3VideoOutput = {
  /**
   * Boundingbox Frames Zip
   *
   * Zip file containing per-frame bounding box overlays.
   */
  boundingbox_frames_zip?: FalAiSam3VideoFile
  /**
   * Video
   *
   * The segmented video.
   */
  video: FalAiSam3VideoFile
}

/**
 * File
 */
export type FalAiSam3VideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * EdittoInput
 */
export type EdittoInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL to the source video file. Required for inpainting.
   */
  video_url: string
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
}

/**
 * EdittoOutput
 */
export type EdittoOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FalAiEdittoFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: FalAiEdittoVideoFile
}

/**
 * File
 */
export type FalAiEdittoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * VideoFile
 */
export type FalAiEdittoVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number | unknown
  /**
   * Height
   *
   * The height of the video
   */
  height?: number | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Width
   *
   * The width of the video
   */
  width?: number | unknown
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number | unknown
}

/**
 * FlashVSRPlusVideoInput
 *
 * Input fields common to FlashVSR+ image/video endpoints.
 */
export type FlashvsrUpscaleVideoInput = {
  /**
   * Video Url
   *
   * The input video to be upscaled
   */
  video_url: string
  /**
   * Acceleration
   *
   * Acceleration mode for VAE decoding. Options: regular (best quality), high (balanced), full (fastest). More accerleation means longer duration videos can be processed too.
   */
  acceleration?: 'regular' | 'high' | 'full'
  /**
   * Quality
   *
   * Quality level for tile blending (0-100). Controls overlap between tiles to prevent grid artifacts. Higher values provide better quality with more overlap. Recommended: 70-85 for high-res videos, 50-70 for faster processing.
   */
  quality?: number
  /**
   * Output Format
   *
   * The format of the output video.
   */
  output_format?:
    | 'X264 (.mp4)'
    | 'VP9 (.webm)'
    | 'PRORES4444 (.mov)'
    | 'GIF (.gif)'
  /**
   * Color Fix
   *
   * Color correction enabled.
   */
  color_fix?: boolean
  /**
   * Output Write Mode
   *
   * The write mode of the output video.
   */
  output_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned inline and not stored in history.
   */
  sync_mode?: boolean
  /**
   * Output Quality
   *
   * The quality of the output video.
   */
  output_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Upscale Factor
   *
   * Upscaling factor to be used.
   */
  upscale_factor?: number
  /**
   * Preserve Audio
   *
   * Copy the original audio tracks into the upscaled video using FFmpeg when possible.
   */
  preserve_audio?: boolean
  /**
   * Seed
   *
   * The random seed used for the generation process.
   */
  seed?: number
}

/**
 * FlashVSRPlusVideoOutput
 */
export type FlashvsrUpscaleVideoOutput = {
  /**
   * Seed
   *
   * The random seed used for the generation process.
   */
  seed: number
  /**
   * Video
   *
   * Upscaled video file after processing
   */
  video: FalAiFlashvsrUpscaleVideoFile
}

/**
 * File
 */
export type FalAiFlashvsrUpscaleVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * AutoSubtitleInput
 *
 * Input model for automatic subtitle generation and styling
 */
export type WorkflowUtilitiesAutoSubtitleInput = {
  /**
   * Video Url
   *
   * URL of the video file to add automatic subtitles to
   */
  video_url: string
  /**
   * Font Weight
   *
   * Font weight (TikTok style typically uses bold or black)
   */
  font_weight?: 'normal' | 'bold' | 'black'
  /**
   * Stroke Width
   *
   * Text stroke/outline width in pixels (0 for no stroke)
   */
  stroke_width?: number
  /**
   * Font Color
   *
   * Subtitle text color for non-active words
   */
  font_color?:
    | 'white'
    | 'black'
    | 'red'
    | 'green'
    | 'blue'
    | 'yellow'
    | 'orange'
    | 'purple'
    | 'pink'
    | 'brown'
    | 'gray'
    | 'cyan'
    | 'magenta'
  /**
   * Font Size
   *
   * Font size for subtitles (TikTok style uses larger text)
   */
  font_size?: number
  /**
   * Language
   *
   * Language code for transcription (e.g., 'en', 'es', 'fr', 'de', 'it', 'pt', 'nl', 'ja', 'zh', 'ko') or 3-letter ISO code (e.g., 'eng', 'spa', 'fra')
   */
  language?: string
  /**
   * Highlight Color
   *
   * Color for the currently speaking word (karaoke-style highlight)
   */
  highlight_color?:
    | 'white'
    | 'black'
    | 'red'
    | 'green'
    | 'blue'
    | 'yellow'
    | 'orange'
    | 'purple'
    | 'pink'
    | 'brown'
    | 'gray'
    | 'cyan'
    | 'magenta'
  /**
   * Background Opacity
   *
   * Background opacity (0.0 = fully transparent, 1.0 = fully opaque)
   */
  background_opacity?: number
  /**
   * Stroke Color
   *
   * Text stroke/outline color
   */
  stroke_color?:
    | 'black'
    | 'white'
    | 'red'
    | 'green'
    | 'blue'
    | 'yellow'
    | 'orange'
    | 'purple'
    | 'pink'
    | 'brown'
    | 'gray'
    | 'cyan'
    | 'magenta'
  /**
   * Y Offset
   *
   * Vertical offset in pixels (positive = move down, negative = move up)
   */
  y_offset?: number
  /**
   * Font Name
   *
   * Any Google Font name from fonts.google.com (e.g., 'Montserrat', 'Poppins', 'BBH Sans Hegarty')
   */
  font_name?: string
  /**
   * Enable Animation
   *
   * Enable animation effects for subtitles (bounce style entrance)
   */
  enable_animation?: boolean
  /**
   * Position
   *
   * Vertical position of subtitles
   */
  position?: 'top' | 'center' | 'bottom'
  /**
   * Words Per Subtitle
   *
   * Maximum number of words per subtitle segment. Use 1 for single-word display, 2-3 for short phrases, or 8-12 for full sentences.
   */
  words_per_subtitle?: number
  /**
   * Background Color
   *
   * Background color behind text ('none' or 'transparent' for no background)
   */
  background_color?:
    | 'black'
    | 'white'
    | 'red'
    | 'green'
    | 'blue'
    | 'yellow'
    | 'orange'
    | 'purple'
    | 'pink'
    | 'brown'
    | 'gray'
    | 'cyan'
    | 'magenta'
    | 'none'
    | 'transparent'
}

/**
 * AutoSubtitleOutput
 *
 * Output model for video with automatic subtitles
 */
export type WorkflowUtilitiesAutoSubtitleOutput = {
  /**
   * Transcription
   *
   * Full transcription text
   */
  transcription: string
  /**
   * Subtitle Count
   *
   * Number of subtitle segments generated
   */
  subtitle_count: number
  /**
   * Transcription Metadata
   *
   * Additional transcription metadata from ElevenLabs (language, segments, etc.)
   */
  transcription_metadata?: {
    [key: string]: unknown
  }
  /**
   * Words
   *
   * Word-level timing information from transcription service
   */
  words?: Array<{
    [key: string]: unknown
  }>
  /**
   * Video
   *
   * The video with automatic subtitles
   */
  video: FalAiWorkflowUtilitiesAutoSubtitleFile
}

/**
 * File
 */
export type FalAiWorkflowUtilitiesAutoSubtitleFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * UpscaleInput
 */
export type BytedanceUpscalerUpscaleVideoInput = {
  /**
   * Target Fps
   *
   * The target FPS of the video to upscale.
   */
  target_fps?: '30fps' | '60fps'
  /**
   * Video Url
   *
   * The URL of the video to upscale.
   */
  video_url: string
  /**
   * Target Resolution
   *
   * The target resolution of the video to upscale.
   */
  target_resolution?: '1080p' | '2k' | '4k'
}

/**
 * UpscaleOutput
 */
export type BytedanceUpscalerUpscaleVideoOutput = {
  /**
   * Duration
   *
   * Duration of audio input/video output as used for billing.
   */
  duration: number
  /**
   * Video
   *
   * Generated video file
   */
  video: FalAiBytedanceUpscalerUpscaleVideoFile
}

/**
 * File
 */
export type FalAiBytedanceUpscalerUpscaleVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * VideoEffectInputWan
 */
export type VideoAsPromptInput = {
  /**
   * Prompt
   *
   * The prompt to generate an image from.
   */
  prompt: string
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: '16:9' | '9:16'
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: '480p' | '580p' | '720p'
  /**
   * Video Url
   *
   * reference video to generate effect video from.
   */
  video_url: string
  /**
   * Image Url
   *
   * Input image to generate the effect video for.
   */
  image_url: string
  /**
   * Frames Per Second
   *
   * Frames per second for the output video. Only applicable if output_type is 'video'.
   */
  fps?: number
  /**
   * Video Description
   *
   * A brief description of the input video content.
   */
  video_description: string
  /**
   * Seed
   *
   * Random seed for reproducible generation. If set none, a random seed will be used.
   */
  seed?: number | unknown
  /**
   * Guidance Scale
   *
   * Guidance scale for generation.
   */
  guidance_scale?: number
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Num Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number
}

/**
 * VideoEffectOutput
 */
export type VideoAsPromptOutput = {
  video: FalAiVideoAsPromptFile
}

/**
 * File
 */
export type FalAiVideoAsPromptFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * VideoInputV2
 */
export type BirefnetV2VideoInput = {
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | 'X264 (.mp4)'
    | 'VP9 (.webm)'
    | 'PRORES4444 (.mov)'
    | 'GIF (.gif)'
  /**
   * Operating Resolution
   *
   * The resolution to operate on. The higher the resolution, the more accurate the output will be for high res input images. The '2304x2304' option is only available for the 'General Use (Dynamic)' model.
   */
  operating_resolution?: '1024x1024' | '2048x2048' | '2304x2304'
  /**
   * Video Url
   *
   * URL of the video to remove background from
   */
  video_url: string
  /**
   * Model
   *
   *
   * Model to use for background removal.
   * The 'General Use (Light)' model is the original model used in the BiRefNet repository.
   * The 'General Use (Light 2K)' model is the original model used in the BiRefNet repository but trained with 2K images.
   * The 'General Use (Heavy)' model is a slower but more accurate model.
   * The 'Matting' model is a model trained specifically for matting images.
   * The 'Portrait' model is a model trained specifically for portrait images.
   * The 'General Use (Dynamic)' model supports dynamic resolutions from 256x256 to 2304x2304.
   * The 'General Use (Light)' model is recommended for most use cases.
   *
   * The corresponding models are as follows:
   * - 'General Use (Light)': BiRefNet
   * - 'General Use (Light 2K)': BiRefNet_lite-2K
   * - 'General Use (Heavy)': BiRefNet_lite
   * - 'Matting': BiRefNet-matting
   * - 'Portrait': BiRefNet-portrait
   * - 'General Use (Dynamic)': BiRefNet_dynamic
   *
   */
  model?:
    | 'General Use (Light)'
    | 'General Use (Light 2K)'
    | 'General Use (Heavy)'
    | 'Matting'
    | 'Portrait'
    | 'General Use (Dynamic)'
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Output Mask
   *
   * Whether to output the mask used to remove the background
   */
  output_mask?: boolean
  /**
   * Refine Foreground
   *
   * Whether to refine the foreground using the estimated mask
   */
  refine_foreground?: boolean
}

/**
 * VideoOutput
 */
export type BirefnetV2VideoOutput = {
  /**
   * Video
   *
   * Video with background removed
   */
  video: FalAiBirefnetV2VideoVideoFile
  /**
   * Mask Video
   *
   * Mask used to remove the background
   */
  mask_video?: FalAiBirefnetV2VideoVideoFile
}

/**
 * VideoFile
 */
export type FalAiBirefnetV2VideoVideoFile = {
  /**
   * Height
   *
   * The height of the video
   */
  height?: number
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number
  /**
   * Width
   *
   * The width of the video
   */
  width?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * Q2VideoExtensionRequest
 */
export type ViduQ2VideoExtensionProInput = {
  /**
   * Prompt
   *
   * text prompt to guide the video extension
   */
  prompt?: string
  /**
   * Duration
   *
   * Duration of the extension in seconds
   */
  duration?: 2 | 3 | 4 | 5 | 6 | 7
  /**
   * Video Url
   *
   * URL of the video to extend
   */
  video_url: string
  /**
   * Resolution
   *
   * Output video resolution
   */
  resolution?: '720p' | '1080p'
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
}

/**
 * Q2VideoExtensionOutput
 */
export type ViduQ2VideoExtensionProOutput = {
  /**
   * Video
   *
   * The extended video using the Q2 model
   */
  video: FalAiViduQ2VideoExtensionProFile
}

/**
 * File
 */
export type FalAiViduQ2VideoExtensionProFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * Input
 */
export type SfxV15VideoToVideoInput = {
  /**
   * Num Samples
   *
   * The number of samples to generate from the model
   */
  num_samples?: number | unknown
  /**
   * Duration
   *
   * The duration of the generated audio in seconds
   */
  duration?: number | unknown
  /**
   * Start Offset
   *
   * The start offset in seconds to start the audio generation from
   */
  start_offset?: number | unknown
  /**
   * Video Url
   *
   * A video url that can accessed from the API to process and add sound effects
   */
  video_url: string
  /**
   * Seed
   *
   * The seed to use for the generation. If not provided, a random seed will be used
   */
  seed?: number | unknown
  /**
   * Text Prompt
   *
   * Additional description to guide the model
   */
  text_prompt?: string | unknown
}

/**
 * Video
 */
export type VideoOutput = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * VideoToVideoInput
 */
export type KreaWan14bVideoToVideoInput = {
  /**
   * Prompt
   *
   * Prompt for the video-to-video generation.
   */
  prompt: string
  /**
   * Video Url
   *
   * URL of the input video. Currently, only outputs of 16:9 aspect ratio and 480p resolution are supported. Video duration should be less than 1000 frames at 16fps, and output frames will be 6 plus a multiple of 12, for example 18, 30, 42, etc.
   */
  video_url: string
  /**
   * Strength
   *
   * Denoising strength for the video-to-video generation. 0.0 preserves the original, 1.0 completely remakes the video.
   */
  strength?: number
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * Seed for the video-to-video generation.
   */
  seed?: number | unknown
}

/**
 * VideoToVideoOutput
 */
export type KreaWan14bVideoToVideoOutput = {
  video: FalAiKreaWan14bVideoToVideoFile
}

/**
 * File
 */
export type FalAiKreaWan14bVideoToVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * RemixInput
 */
export type Sora2VideoToVideoRemixInput = {
  /**
   * Prompt
   *
   * Updated text prompt that directs the remix generation
   */
  prompt: string
  /**
   * Video ID
   *
   * The video_id from a previous Sora 2 generation. Note: You can only remix videos that were generated by Sora (via text-to-video or image-to-video endpoints), not arbitrary uploaded videos.
   */
  video_id: string
  /**
   * Delete Video
   *
   * Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted.
   */
  delete_video?: boolean
}

/**
 * RemixOutput
 */
export type Sora2VideoToVideoRemixOutput = {
  /**
   * Spritesheet
   *
   * Spritesheet image for the video
   */
  spritesheet?: ImageFile
  /**
   * Thumbnail
   *
   * Thumbnail image for the video
   */
  thumbnail?: ImageFile
  /**
   * Video ID
   *
   * The ID of the generated video
   */
  video_id: string
  /**
   * Video
   *
   * The generated video
   */
  video: FalAiSora2VideoToVideoRemixVideoFile
}

/**
 * ImageFile
 */
export type ImageFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * Height
   *
   * The height of the image
   */
  height?: number
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Width
   *
   * The width of the image
   */
  width?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * VideoFile
 */
export type FalAiSora2VideoToVideoRemixVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number
  /**
   * Height
   *
   * The height of the video
   */
  height?: number
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number
  /**
   * Width
   *
   * The width of the video
   */
  width?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * LongWanVACEReframeRequest
 */
export type WanVaceAppsLongReframeInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation. Optional for reframing.
   */
  prompt?: string
  /**
   * Video URL
   *
   * URL to the source video file. This video will be used as a reference for the reframe task.
   */
  video_url: string
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular'
  /**
   * Paste Back
   *
   * Whether to paste back the reframed scene to the original video.
   */
  paste_back?: boolean
  /**
   * Zoom Factor
   *
   * Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom.
   */
  zoom_factor?: number
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Scene Threshold
   *
   * Threshold for scene detection sensitivity (0-100). Lower values detect more scenes.
   */
  scene_threshold?: number
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Auto Downsample Min Fps
   *
   * Minimum FPS for auto downsample.
   */
  auto_downsample_min_fps?: number
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Trim Borders
   *
   * Whether to trim borders from the video.
   */
  trim_borders?: boolean
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * Whether to enable auto downsample.
   */
  enable_auto_downsample?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
}

/**
 * LongWanVACEReframeResponse
 */
export type WanVaceAppsLongReframeOutput = {
  /**
   * Video
   *
   * The output video file.
   */
  video: FalAiWanVaceAppsLongReframeVideoFile
}

/**
 * VideoFile
 */
export type FalAiWanVaceAppsLongReframeVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number
  /**
   * Height
   *
   * The height of the video
   */
  height?: number
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Width
   *
   * The width of the video
   */
  width?: number
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * InfiniTalkVid2VidAudioRequest
 */
export type InfinitalkVideoToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Resolution
   *
   * Resolution of the video to generate. Must be either 480p or 720p.
   */
  resolution?: '480p' | '720p'
  /**
   * Acceleration
   *
   * The acceleration level to use for generation.
   */
  acceleration?: 'none' | 'regular' | 'high'
  /**
   * Video Url
   *
   * URL of the input video.
   */
  video_url: string
  /**
   * Audio URL
   *
   * The URL of the audio file.
   */
  audio_url: string
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.
   */
  num_frames?: number
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
}

/**
 * InfinitalkVid2VidResponse
 */
export type InfinitalkVideoToVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: FalAiInfinitalkVideoToVideoFile
}

/**
 * File
 */
export type FalAiInfinitalkVideoToVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * SeedVRVideoInput
 */
export type SeedvrUpscaleVideoInput = {
  /**
   * Video Url
   *
   * The input video to be processed
   */
  video_url: string
  /**
   * Upscale Mode
   *
   * The mode to use for the upscale. If 'target', the upscale factor will be calculated based on the target resolution. If 'factor', the upscale factor will be used directly.
   */
  upscale_mode?: 'target' | 'factor'
  /**
   * Noise Scale
   *
   * The noise scale to use for the generation process.
   */
  noise_scale?: number
  /**
   * Target Resolution
   *
   * The target resolution to upscale to when `upscale_mode` is `target`.
   */
  target_resolution?: '720p' | '1080p' | '1440p' | '2160p'
  /**
   * Output Write Mode
   *
   * The write mode of the output video.
   */
  output_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Output Format
   *
   * The format of the output video.
   */
  output_format?:
    | 'X264 (.mp4)'
    | 'VP9 (.webm)'
    | 'PRORES4444 (.mov)'
    | 'GIF (.gif)'
  /**
   * Output Quality
   *
   * The quality of the output video.
   */
  output_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Upscale Factor
   *
   * Upscaling factor to be used. Will multiply the dimensions with this factor when `upscale_mode` is `factor`.
   */
  upscale_factor?: number
  /**
   * Seed
   *
   * The random seed used for the generation process.
   */
  seed?: number
}

/**
 * SeedVRVideoOutput
 */
export type SeedvrUpscaleVideoOutput = {
  /**
   * Seed
   *
   * The random seed used for the generation process.
   */
  seed: number
  /**
   * Video
   *
   * Upscaled video file after processing
   */
  video: FalAiSeedvrUpscaleVideoFile
}

/**
 * File
 */
export type FalAiSeedvrUpscaleVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * WanVACEVideoEditRequest
 */
export type WanVaceAppsVideoEditInput = {
  /**
   * Prompt
   *
   * Prompt to edit the video.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL of the input video.
   */
  video_url: string
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular'
  /**
   * Resolution
   *
   * Resolution of the edited video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the edited video.
   */
  aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1'
  /**
   * Return Frames ZIP
   *
   * Whether to include a ZIP archive containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Video Type
   *
   * The type of video you're editing. Use 'general' for most videos, and 'human' for videos emphasizing human subjects and motions. The default value 'auto' means the model will guess based on the first frame of the video.
   */
  video_type?: 'auto' | 'general' | 'human'
  /**
   * Image URLs
   *
   * URLs of the input images to use as a reference for the generation.
   */
  image_urls?: Array<string>
  /**
   * Enable Auto Downsampling
   *
   * Whether to enable automatic downsampling. If your video has a high frame rate or is long, enabling longer sequences to be generated. The video will be interpolated back to the original frame rate after generation.
   */
  enable_auto_downsample?: boolean
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to.
   */
  auto_downsample_min_fps?: number
}

/**
 * WanVACEVideoEditResponse
 */
export type WanVaceAppsVideoEditOutput = {
  /**
   * Frames Zip
   *
   * ZIP archive of generated frames if requested.
   */
  frames_zip?: FalAiWanVaceAppsVideoEditFile
  /**
   * Video
   *
   * The edited video.
   */
  video: FalAiWanVaceAppsVideoEditVideoFile
}

/**
 * File
 */
export type FalAiWanVaceAppsVideoEditFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * VideoFile
 */
export type FalAiWanVaceAppsVideoEditVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number
  /**
   * Height
   *
   * The height of the video
   */
  height?: number
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Width
   *
   * The width of the video
   */
  width?: number
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * WanAnimateMoveRequest
 */
export type WanV2214bAnimateReplaceInput = {
  /**
   * Shift
   *
   * Shift value for the video. Must be between 1.0 and 10.0.
   */
  shift?: number
  /**
   * Video URL
   *
   * URL of the input video.
   */
  video_url: string
  /**
   * Resolution
   *
   * Resolution of the generated video (480p, 580p, or 720p).
   */
  resolution?: '480p' | '580p' | '720p'
  /**
   * Return Frames ZIP
   *
   * If true, also return a ZIP archive containing per-frame images generated on GPU (lossless).
   */
  return_frames_zip?: boolean
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Enable Output Safety Checker
   *
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean
  /**
   * Image URL
   *
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string
  /**
   * Video Quality
   *
   * The quality of the output video. Higher quality means better visual quality but larger file size.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Guidance Scale
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guidance_scale?: number
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Use Turbo
   *
   * If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized for best results.
   */
  use_turbo?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Enable Safety Checker
   *
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean
}

/**
 * WanAnimateReplaceResponse
 */
export type WanV2214bAnimateReplaceOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation (auto-generated by the model)
   */
  prompt: string
  /**
   * ZIP archive of generated frames (if requested).
   */
  frames_zip?: FalAiWanV2214bAnimateReplaceFile | unknown
  /**
   * Seed
   *
   * The seed used for generation
   */
  seed: number
  video: FalAiWanV2214bAnimateReplaceFile
}

/**
 * File
 */
export type FalAiWanV2214bAnimateReplaceFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * WanAnimateMoveRequest
 */
export type WanV2214bAnimateMoveInput = {
  /**
   * Shift
   *
   * Shift value for the video. Must be between 1.0 and 10.0.
   */
  shift?: number
  /**
   * Video URL
   *
   * URL of the input video.
   */
  video_url: string
  /**
   * Resolution
   *
   * Resolution of the generated video (480p, 580p, or 720p).
   */
  resolution?: '480p' | '580p' | '720p'
  /**
   * Return Frames ZIP
   *
   * If true, also return a ZIP archive containing per-frame images generated on GPU (lossless).
   */
  return_frames_zip?: boolean
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Enable Output Safety Checker
   *
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean
  /**
   * Image URL
   *
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string
  /**
   * Video Quality
   *
   * The quality of the output video. Higher quality means better visual quality but larger file size.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Guidance Scale
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guidance_scale?: number
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Use Turbo
   *
   * If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized for best results.
   */
  use_turbo?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Enable Safety Checker
   *
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean
}

/**
 * WanAnimateMoveResponse
 */
export type WanV2214bAnimateMoveOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation (auto-generated by the model)
   */
  prompt: string
  /**
   * ZIP archive of generated frames (if requested).
   */
  frames_zip?: FalAiWanV2214bAnimateMoveFile | unknown
  /**
   * Seed
   *
   * The seed used for generation
   */
  seed: number
  video: FalAiWanV2214bAnimateMoveFile
}

/**
 * File
 */
export type FalAiWanV2214bAnimateMoveFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * LucyEditProInput
 */
export type LucyEditProInput = {
  /**
   * Sync Mode
   *
   *
   * If set to true, the function will wait for the video to be generated
   * and uploaded before returning the response. This will increase the
   * latency of the function but it allows you to get the video directly
   * in the response without going through the CDN.
   *
   */
  sync_mode?: boolean
  /**
   * Video Url
   *
   * URL of the video to edit
   */
  video_url: string
  /**
   * Prompt
   *
   * Text description of the desired video content
   */
  prompt: string
  /**
   * Resolution
   *
   * Resolution of the generated video
   */
  resolution?: '720p'
  /**
   * Enhance Prompt
   *
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean
}

/**
 * LucyEditProOutput
 */
export type LucyEditProOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: DecartLucyEditProFile
}

/**
 * File
 */
export type DecartLucyEditProFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * LucyEditDevInput
 */
export type LucyEditDevInput = {
  /**
   * Sync Mode
   *
   *
   * If set to true, the function will wait for the video to be generated
   * and uploaded before returning the response. This will increase the
   * latency of the function but it allows you to get the video directly
   * in the response without going through the CDN.
   *
   */
  sync_mode?: boolean
  /**
   * Video Url
   *
   * URL of the video to edit
   */
  video_url: string
  /**
   * Prompt
   *
   * Text description of the desired video content
   */
  prompt: string
  /**
   * Enhance Prompt
   *
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean
}

/**
 * LucyEditDevOutput
 */
export type LucyEditDevOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: DecartLucyEditDevFile
}

/**
 * File
 */
export type DecartLucyEditDevFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * WanVACEReframeRequest
 */
export type Wan22VaceFunA14bReframeInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation. Optional for reframing.
   */
  prompt?: string
  /**
   * Video URL
   *
   * URL to the source video file. This video will be used as a reference for the reframe task.
   */
  video_url: string
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number
  /**
   * Trim Borders
   *
   * Whether to trim borders from the video.
   */
  trim_borders?: boolean
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Zoom Factor
   *
   * Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom.
   */
  zoom_factor?: number
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown
}

/**
 * WanVACEReframeResponse
 */
export type Wan22VaceFunA14bReframeOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FalAiWan22VaceFunA14bReframeFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: FalAiWan22VaceFunA14bReframeVideoFile
}

/**
 * File
 */
export type FalAiWan22VaceFunA14bReframeFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * VideoFile
 */
export type FalAiWan22VaceFunA14bReframeVideoFile = {
  /**
   * Height
   *
   * The height of the video
   */
  height?: number | unknown
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number | unknown
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number | unknown
  /**
   * Width
   *
   * The width of the video
   */
  width?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number | unknown
}

/**
 * WanVACEOutpaintingRequest
 */
export type Wan22VaceFunA14bOutpaintingInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL to the source video file. Required for outpainting.
   */
  video_url: string
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown
  /**
   * Reference Image URLs
   *
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>
  /**
   * Expand Ratio
   *
   * Amount of expansion. This is a float value between 0 and 1, where 0.25 adds 25% to the original video size on the specified sides.
   */
  expand_ratio?: number
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Expand Bottom
   *
   * Whether to expand the video to the bottom.
   */
  expand_bottom?: boolean
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Expand Left
   *
   * Whether to expand the video to the left.
   */
  expand_left?: boolean
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
  /**
   * Expand Top
   *
   * Whether to expand the video to the top.
   */
  expand_top?: boolean
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Expand Right
   *
   * Whether to expand the video to the right.
   */
  expand_right?: boolean
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown
}

/**
 * WanVACEOutpaintingResponse
 */
export type Wan22VaceFunA14bOutpaintingOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FalAiWan22VaceFunA14bOutpaintingFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: FalAiWan22VaceFunA14bOutpaintingVideoFile
}

/**
 * File
 */
export type FalAiWan22VaceFunA14bOutpaintingFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * VideoFile
 */
export type FalAiWan22VaceFunA14bOutpaintingVideoFile = {
  /**
   * Height
   *
   * The height of the video
   */
  height?: number | unknown
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number | unknown
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number | unknown
  /**
   * Width
   *
   * The width of the video
   */
  width?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number | unknown
}

/**
 * WanVACEInpaintingRequest
 */
export type Wan22VaceFunA14bInpaintingInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL to the source video file. Required for inpainting.
   */
  video_url: string
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown
  /**
   * Reference Image URLs
   *
   * Urls to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Mask Video URL
   *
   * URL to the source mask file. Required for inpainting.
   */
  mask_video_url: string | unknown
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Mask Image URL
   *
   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video using salient mask tracking. Will be ignored if mask_video_url is provided.
   */
  mask_image_url?: string | unknown
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown
}

/**
 * WanVACEInpaintingResponse
 */
export type Wan22VaceFunA14bInpaintingOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FalAiWan22VaceFunA14bInpaintingFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: FalAiWan22VaceFunA14bInpaintingVideoFile
}

/**
 * File
 */
export type FalAiWan22VaceFunA14bInpaintingFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * VideoFile
 */
export type FalAiWan22VaceFunA14bInpaintingVideoFile = {
  /**
   * Height
   *
   * The height of the video
   */
  height?: number | unknown
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number | unknown
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number | unknown
  /**
   * Width
   *
   * The width of the video
   */
  width?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number | unknown
}

/**
 * WanVACEDepthRequest
 */
export type Wan22VaceFunA14bDepthInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL to the source video file. Required for depth task.
   */
  video_url: string
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown
  /**
   * Reference Image URLs
   *
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown
}

/**
 * WanVACEDepthResponse
 */
export type Wan22VaceFunA14bDepthOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FalAiWan22VaceFunA14bDepthFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: FalAiWan22VaceFunA14bDepthVideoFile
}

/**
 * File
 */
export type FalAiWan22VaceFunA14bDepthFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * VideoFile
 */
export type FalAiWan22VaceFunA14bDepthVideoFile = {
  /**
   * Height
   *
   * The height of the video
   */
  height?: number | unknown
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number | unknown
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number | unknown
  /**
   * Width
   *
   * The width of the video
   */
  width?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number | unknown
}

/**
 * WanVACEPoseRequest
 */
export type Wan22VaceFunA14bPoseInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation. For pose task, the prompt should describe the desired pose and action of the subject in the video.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL to the source video file. Required for pose task.
   */
  video_url: string
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown
  /**
   * Reference Image URLs
   *
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown
}

/**
 * WanVACEPoseResponse
 */
export type Wan22VaceFunA14bPoseOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FalAiWan22VaceFunA14bPoseFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: FalAiWan22VaceFunA14bPoseVideoFile
}

/**
 * File
 */
export type FalAiWan22VaceFunA14bPoseFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * VideoFile
 */
export type FalAiWan22VaceFunA14bPoseVideoFile = {
  /**
   * Height
   *
   * The height of the video
   */
  height?: number | unknown
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number | unknown
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number | unknown
  /**
   * Width
   *
   * The width of the video
   */
  width?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number | unknown
}

/**
 * HunyuanFoleyRequest
 */
export type HunyuanVideoFoleyInput = {
  /**
   * Video Url
   *
   * The URL of the video to generate audio for.
   */
  video_url: string
  /**
   * Guidance Scale
   *
   * Guidance scale for audio generation.
   */
  guidance_scale?: number
  /**
   * Num Inference Steps
   *
   * Number of inference steps for generation.
   */
  num_inference_steps?: number
  /**
   * Seed
   *
   * Random seed for reproducible generation.
   */
  seed?: number
  /**
   * Negative Prompt
   *
   * Negative prompt to avoid certain audio characteristics.
   */
  negative_prompt?: string
  /**
   * Text Prompt
   *
   * Text description of the desired audio (optional).
   */
  text_prompt: string
}

/**
 * HunyuanFoleyResponse
 */
export type HunyuanVideoFoleyOutput = {
  /**
   * Video
   *
   * List of generated video files with audio.
   */
  video: FalAiHunyuanVideoFoleyFile
}

/**
 * File
 */
export type FalAiHunyuanVideoFoleyFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * LipSyncV2ProInput
 */
export type SyncLipsyncV2ProInput = {
  /**
   * Sync Mode
   *
   * Lipsync mode when audio and video durations are out of sync.
   */
  sync_mode?: 'cut_off' | 'loop' | 'bounce' | 'silence' | 'remap'
  /**
   * Video Url
   *
   * URL of the input video
   */
  video_url: string
  /**
   * Audio Url
   *
   * URL of the input audio
   */
  audio_url: string
}

/**
 * LipSyncV2ProOutput
 */
export type SyncLipsyncV2ProOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: FalAiSyncLipsyncV2ProFile
}

/**
 * File
 */
export type FalAiSyncLipsyncV2ProFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * WanFunControlRequest
 */
export type WanFunControlInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video.
   */
  prompt: string
  /**
   * Shift
   *
   * The shift for the scheduler.
   */
  shift?: number
  /**
   * Preprocess Video
   *
   * Whether to preprocess the video. If True, the video will be preprocessed to depth or pose.
   */
  preprocess_video?: boolean
  /**
   * Reference Image URL
   *
   * The URL of the reference image to use as a reference for the video generation.
   */
  reference_image_url?: string
  /**
   * FPS
   *
   * The fps to generate. Only used when match_input_fps is False.
   */
  fps?: number
  /**
   * Match Input Number of Frames
   *
   * Whether to match the number of frames in the input video.
   */
  match_input_num_frames?: boolean
  /**
   * Guidance Scale
   *
   * The guidance scale.
   */
  guidance_scale?: number
  /**
   * Preprocess Type
   *
   * The type of preprocess to apply to the video. Only used when preprocess_video is True.
   */
  preprocess_type?: 'depth' | 'pose'
  /**
   * Control Video URL
   *
   * The URL of the control video to use as a reference for the video generation.
   */
  control_video_url: string
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video.
   */
  negative_prompt?: string
  /**
   * Number of Frames
   *
   * The number of frames to generate. Only used when match_input_num_frames is False.
   */
  num_frames?: number
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number
  /**
   * Number of Inference Steps
   *
   * The number of inference steps.
   */
  num_inference_steps?: number
  /**
   * Match Input FPS
   *
   * Whether to match the fps in the input video.
   */
  match_input_fps?: boolean
}

/**
 * WanFunControlResponse
 */
export type WanFunControlOutput = {
  /**
   * Video
   *
   * The video generated by the model.
   */
  video: FalAiWanFunControlFile
}

/**
 * File
 */
export type FalAiWanFunControlFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * InputIncreaseResolutionModel
 */
export type VideoIncreaseResolutionInput = {
  /**
   * Video Url
   *
   * Input video to increase resolution. Size should be less than 14142x14142 and duration less than 30s.
   */
  video_url: string
  /**
   * Output Container And Codec
   *
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, mov_h265, mov_proresks, mkv_h265, mkv_h264, mkv_vp9, gif.
   */
  output_container_and_codec?:
    | 'mp4_h265'
    | 'mp4_h264'
    | 'webm_vp9'
    | 'mov_h265'
    | 'mov_proresks'
    | 'mkv_h265'
    | 'mkv_h264'
    | 'mkv_vp9'
    | 'gif'
  /**
   * Desired Increase
   *
   * desired_increase factor. Options: 2x, 4x.
   */
  desired_increase?: '2' | '4'
}

/**
 * OutputIncreaseResolutionModel
 */
export type VideoIncreaseResolutionOutput = {
  /**
   * Video
   *
   * Video with removed background and audio.
   */
  video: BriaVideoIncreaseResolutionVideo | BriaVideoIncreaseResolutionFile
}

/**
 * Video
 */
export type BriaVideoIncreaseResolutionVideo = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * File
 */
export type BriaVideoIncreaseResolutionFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * InfiniTalkSingleAudioRequest
 */
export type InfinitalkInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Resolution
   *
   * Resolution of the video to generate. Must be either 480p or 720p.
   */
  resolution?: '480p' | '720p'
  /**
   * Acceleration
   *
   * The acceleration level to use for generation.
   */
  acceleration?: 'none' | 'regular' | 'high'
  /**
   * Image URL
   *
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string
  /**
   * Audio URL
   *
   * The URL of the audio file.
   */
  audio_url: string
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 41 to 721.
   */
  num_frames?: number
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
}

/**
 * AvatarSingleAudioResponse
 */
export type InfinitalkOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: FalAiInfinitalkFile
}

/**
 * File
 */
export type FalAiInfinitalkFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * Input
 */
export type SfxV1VideoToVideoInput = {
  /**
   * Num Samples
   *
   * The number of samples to generate from the model
   */
  num_samples?: number | unknown
  /**
   * Video Url
   *
   * A video url that can accessed from the API to process and add sound effects
   */
  video_url: string
  /**
   * Duration
   *
   * The duration of the generated audio in seconds
   */
  duration?: number | unknown
  /**
   * Seed
   *
   * The seed to use for the generation. If not provided, a random seed will be used
   */
  seed?: number | unknown
  /**
   * Text Prompt
   *
   * Additional description to guide the model
   */
  text_prompt?: string | unknown
}

/**
 * VideoOutput
 */
export type SfxV1VideoToVideoOutput = {
  /**
   * Video
   *
   * The processed video with sound effects
   */
  video: Array<MireloAiSfxV1VideoToVideoVideo>
}

/**
 * Video
 */
export type MireloAiSfxV1VideoToVideoVideo = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * MareyInputPoseTransfer
 */
export type MareyPoseTransferInput = {
  /**
   * Prompt
   *
   * The prompt to generate a video from
   */
  prompt: string
  /**
   * Video Url
   *
   * The URL of the video to use as the control video.
   */
  video_url: string
  /**
   * Seed
   *
   * Seed for random number generation. Use -1 for random seed each run.
   */
  seed?: number | unknown
  /**
   * Reference Image Url
   *
   * Optional reference image URL to use for pose control or as a starting frame
   */
  reference_image_url?: string | unknown
  /**
   * Negative Prompt
   *
   * Negative prompt used to guide the model away from undesirable features.
   */
  negative_prompt?: string | unknown
  /**
   * First Frame Image Url
   *
   * Optional first frame image URL to use as the first frame of the generated video
   */
  first_frame_image_url?: string | unknown
}

/**
 * MareyOutput
 */
export type MareyPoseTransferOutput = {
  video: MoonvalleyMareyPoseTransferFile
}

/**
 * File
 */
export type MoonvalleyMareyPoseTransferFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * MareyInputMotionTransfer
 */
export type MareyMotionTransferInput = {
  /**
   * Prompt
   *
   * The prompt to generate a video from
   */
  prompt: string
  /**
   * Video Url
   *
   * The URL of the video to use as the control video.
   */
  video_url: string
  /**
   * Seed
   *
   * Seed for random number generation. Use -1 for random seed each run.
   */
  seed?: number | unknown
  /**
   * Reference Image Url
   *
   * Optional reference image URL to use for pose control or as a starting frame
   */
  reference_image_url?: string | unknown
  /**
   * Negative Prompt
   *
   * Negative prompt used to guide the model away from undesirable features.
   */
  negative_prompt?: string | unknown
  /**
   * First Frame Image Url
   *
   * Optional first frame image URL to use as the first frame of the generated video
   */
  first_frame_image_url?: string | unknown
}

/**
 * MareyOutput
 */
export type MareyMotionTransferOutput = {
  video: MoonvalleyMareyMotionTransferFile
}

/**
 * File
 */
export type MoonvalleyMareyMotionTransferFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * MergeVideosInput
 */
export type FfmpegApiMergeVideosInput = {
  /**
   * Target Fps
   *
   * Target FPS for the output video. If not provided, uses the lowest FPS from input videos.
   */
  target_fps?: number | unknown
  /**
   * Video Urls
   *
   * List of video URLs to merge in order
   */
  video_urls: Array<string>
  /**
   * Resolution
   *
   * Resolution of the final video. Width and height must be between 512 and 2048.
   */
  resolution?:
    | FalAiFfmpegApiMergeVideosImageSize
    | 'square_hd'
    | 'square'
    | 'portrait_4_3'
    | 'portrait_16_9'
    | 'landscape_4_3'
    | 'landscape_16_9'
    | unknown
}

/**
 * ImageSize
 */
export type FalAiFfmpegApiMergeVideosImageSize = {
  /**
   * Height
   *
   * The height of the generated image.
   */
  height?: number
  /**
   * Width
   *
   * The width of the generated image.
   */
  width?: number
}

/**
 * MergeVideosOutput
 */
export type FfmpegApiMergeVideosOutput = {
  /**
   * Metadata
   *
   * Metadata about the merged video including original video info
   */
  metadata: {
    [key: string]: unknown
  }
  video: FalAiFfmpegApiMergeVideosFile
}

/**
 * File
 */
export type FalAiFfmpegApiMergeVideosFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * WanV2VRequest
 */
export type WanV22A14bVideoToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL of the input video.
   */
  video_url: string
  /**
   * Acceleration
   *
   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.
   */
  acceleration?: 'none' | 'regular'
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
   */
  num_interpolated_frames?: number
  /**
   * Shift
   *
   * Shift value for the video. Must be between 1.0 and 10.0.
   */
  shift?: number
  /**
   * Resample Video Frame Rate
   *
   * If true, the video will be resampled to the passed frames per second. If false, the video will not be resampled.
   */
  resample_fps?: boolean
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.
   */
  frames_per_second?: number | unknown
  /**
   * Enable Safety Checker
   *
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 17 to 161 (inclusive).
   */
  num_frames?: number
  /**
   * Guidance Scale (1st Stage)
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guidance_scale?: number
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Resolution
   *
   * Resolution of the generated video (480p, 580p, or 720p).
   */
  resolution?: '480p' | '580p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input video.
   */
  aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1'
  /**
   * Enable Output Safety Checker
   *
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean
  /**
   * Guidance Scale (2nd Stage)
   *
   * Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model.
   */
  guidance_scale_2?: number
  /**
   * Strength
   *
   * Strength of the video transformation. A value of 1.0 means the output will be completely based on the prompt, while a value of 0.0 means the output will be identical to the input video.
   */
  strength?: number
  /**
   * Video Quality
   *
   * The quality of the output video. Higher quality means better visual quality but larger file size.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. If None, no interpolation is applied.
   */
  interpolator_model?: 'none' | 'film' | 'rife'
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Adjust FPS for Interpolation
   *
   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.
   */
  adjust_fps_for_interpolation?: boolean
}

/**
 * WanV2VResponse
 */
export type WanV22A14bVideoToVideoOutput = {
  /**
   * Prompt
   *
   * The text prompt used for video generation.
   */
  prompt?: string
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: FalAiWanV22A14bVideoToVideoFile
}

/**
 * File
 */
export type FalAiWanV22A14bVideoToVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * ExtendVideoConditioningInput
 */
export type ExtendVideoConditioningInput = {
  /**
   * Video URL
   *
   * URL of video to use as conditioning
   */
  video_url: string
  /**
   * Start Frame Number
   *
   * Frame number of the video from which the conditioning starts. Must be a multiple of 8.
   */
  start_frame_num?: number
  /**
   * Reverse Video
   *
   * Whether to reverse the video. This is useful for tasks where the video conditioning should be applied in reverse order.
   */
  reverse_video?: boolean
  /**
   * Limit Number of Frames
   *
   * Whether to limit the number of frames used from the video. If True, the `max_num_frames` parameter will be used to limit the number of frames.
   */
  limit_num_frames?: boolean
  /**
   * Resample FPS
   *
   * Whether to resample the video to a specific FPS. If True, the `target_fps` parameter will be used to resample the video.
   */
  resample_fps?: boolean
  /**
   * Strength
   *
   * Strength of the conditioning. 0.0 means no conditioning, 1.0 means full conditioning.
   */
  strength?: number
  /**
   * Target FPS
   *
   * Target FPS to resample the video to. Only relevant if `resample_fps` is True.
   */
  target_fps?: number
  /**
   * Maximum Number of Frames
   *
   * Maximum number of frames to use from the video. If None, all frames will be used.
   */
  max_num_frames?: number
}

/**
 * ExtendVideoOutput
 */
export type Ltxv13B098DistilledExtendOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: FalAiLtxv13B098DistilledExtendFile
}

/**
 * File
 */
export type FalAiLtxv13B098DistilledExtendFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * RIFEVideoInput
 */
export type RifeVideoInput = {
  /**
   * Video URL
   *
   * The URL of the video to use for interpolation.
   */
  video_url: string
  /**
   * Use Scene Detection
   *
   * If True, the input video will be split into scenes before interpolation. This removes smear frames between scenes, but can result in false positives if the scene detection is not accurate. If False, the entire video will be treated as a single scene.
   */
  use_scene_detection?: boolean
  /**
   * Loop
   *
   * If True, the final frame will be looped back to the first frame to create a seamless loop. If False, the final frame will not loop back.
   */
  loop?: boolean
  /**
   * Number of Frames
   *
   * The number of frames to generate between the input video frames.
   */
  num_frames?: number
  /**
   * Use Calculated FPS
   *
   * If True, the function will use the calculated FPS of the input video multiplied by the number of frames to determine the output FPS. If False, the passed FPS will be used.
   */
  use_calculated_fps?: boolean
  /**
   * Frames Per Second
   *
   * Frames per second for the output video. Only applicable if use_calculated_fps is False.
   */
  fps?: number
}

/**
 * RIFEVideoOutput
 */
export type RifeVideoOutput = {
  /**
   * Video
   *
   * The generated video file with interpolated frames.
   */
  video: FalAiRifeVideoFile
}

/**
 * File
 */
export type FalAiRifeVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * FILMVideoInput
 */
export type FilmVideoInput = {
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Only applicable if output_type is 'video'.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Video URL
   *
   * The URL of the video to use for interpolation.
   */
  video_url: string
  /**
   * Use Calculated FPS
   *
   * If True, the function will use the calculated FPS of the input video multiplied by the number of frames to determine the output FPS. If False, the passed FPS will be used.
   */
  use_calculated_fps?: boolean
  /**
   * Loop
   *
   * If True, the final frame will be looped back to the first frame to create a seamless loop. If False, the final frame will not loop back.
   */
  loop?: boolean
  /**
   * Frames Per Second
   *
   * Frames per second for the output video. Only applicable if use_calculated_fps is False.
   */
  fps?: number
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Video Quality
   *
   * The quality of the output video. Only applicable if output_type is 'video'.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Use Scene Detection
   *
   * If True, the input video will be split into scenes before interpolation. This removes smear frames between scenes, but can result in false positives if the scene detection is not accurate. If False, the entire video will be treated as a single scene.
   */
  use_scene_detection?: boolean
  /**
   * Number of Frames
   *
   * The number of frames to generate between the input video frames.
   */
  num_frames?: number
}

/**
 * FILMVideoOutput
 */
export type FilmVideoOutput = {
  /**
   * Video
   *
   * The generated video file with interpolated frames.
   */
  video: FalAiFilmVideoVideoFile
}

/**
 * VideoFile
 */
export type FalAiFilmVideoVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number
  /**
   * Height
   *
   * The height of the video
   */
  height?: number
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Width
   *
   * The width of the video
   */
  width?: number
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * ModifyVideoRequest
 */
export type LumaDreamMachineRay2FlashModifyInput = {
  /**
   * Prompt
   *
   * Instruction for modifying the video
   */
  prompt?: string
  /**
   * Video Url
   *
   * URL of the input video to modify
   */
  video_url: string
  /**
   * Mode
   *
   * Amount of modification to apply to the video, adhere_1 is the least amount of modification, reimagine_3 is the most
   */
  mode?:
    | 'adhere_1'
    | 'adhere_2'
    | 'adhere_3'
    | 'flex_1'
    | 'flex_2'
    | 'flex_3'
    | 'reimagine_1'
    | 'reimagine_2'
    | 'reimagine_3'
  /**
   * Image Url
   *
   * Optional URL of the first frame image for modification
   */
  image_url?: string
}

/**
 * ModifyOutput
 */
export type LumaDreamMachineRay2FlashModifyOutput = {
  /**
   * Video
   *
   * URL of the modified video
   */
  video: FalAiLumaDreamMachineRay2FlashModifyFile
}

/**
 * File
 */
export type FalAiLumaDreamMachineRay2FlashModifyFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * VideoConditioningInput
 */
export type VideoConditioningInput = {
  /**
   * Video URL
   *
   * URL of video to use as conditioning
   */
  video_url: string
  /**
   * Start Frame Number
   *
   * Frame number of the video from which the conditioning starts. Must be a multiple of 8.
   */
  start_frame_num?: number
  /**
   * Reverse Video
   *
   * Whether to reverse the video. This is useful for tasks where the video conditioning should be applied in reverse order.
   */
  reverse_video?: boolean
  /**
   * Limit Number of Frames
   *
   * Whether to limit the number of frames used from the video. If True, the `max_num_frames` parameter will be used to limit the number of frames.
   */
  limit_num_frames?: boolean
  /**
   * Resample FPS
   *
   * Whether to resample the video to a specific FPS. If True, the `target_fps` parameter will be used to resample the video.
   */
  resample_fps?: boolean
  /**
   * Strength
   *
   * Strength of the conditioning. 0.0 means no conditioning, 1.0 means full conditioning.
   */
  strength?: number
  /**
   * Target FPS
   *
   * Target FPS to resample the video to. Only relevant if `resample_fps` is True.
   */
  target_fps?: number
  /**
   * Maximum Number of Frames
   *
   * Maximum number of frames to use from the video. If None, all frames will be used.
   */
  max_num_frames?: number
  /**
   * Conditioning Type
   *
   * Type of conditioning this video provides. This is relevant to ensure in-context LoRA weights are applied correctly, as well as selecting the correct preprocessing pipeline, when enabled.
   */
  conditioning_type?: 'rgb' | 'depth' | 'pose' | 'canny'
  /**
   * Preprocess
   *
   * Whether to preprocess the video. If True, the video will be preprocessed to match the conditioning type. This is a no-op for RGB conditioning.
   */
  preprocess?: boolean
}

/**
 * MultiConditioningVideoOutput
 */
export type Ltxv13B098DistilledMulticonditioningOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: FalAiLtxv13B098DistilledMulticonditioningFile
}

/**
 * File
 */
export type FalAiLtxv13B098DistilledMulticonditioningFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * SoundEffectRequest
 */
export type PixverseSoundEffectsInput = {
  /**
   * Prompt
   *
   * Description of the sound effect to generate. If empty, a random sound effect will be generated
   */
  prompt?: string
  /**
   * Video Url
   *
   * URL of the input video to add sound effects to
   */
  video_url: string
  /**
   * Original Sound Switch
   *
   * Whether to keep the original audio from the video
   */
  original_sound_switch?: boolean
}

/**
 * SoundEffectOutput
 */
export type PixverseSoundEffectsOutput = {
  /**
   * Video
   *
   * The video with added sound effects
   */
  video: FalAiPixverseSoundEffectsFile
}

/**
 * File
 */
export type FalAiPixverseSoundEffectsFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * Input
 */
export type ThinksoundAudioInput = {
  /**
   * Prompt
   *
   * A prompt to guide the audio generation. If not provided, it will be extracted from the video.
   */
  prompt?: string
  /**
   * Video Url
   *
   * The URL of the video to generate the audio for.
   */
  video_url: string
  /**
   * Seed
   *
   * The seed for the random number generator
   */
  seed?: number
  /**
   * Number of Inference Steps
   *
   * The number of inference steps for audio generation.
   */
  num_inference_steps?: number
  /**
   * CFG Scale
   *
   * The classifier-free guidance scale for audio generation.
   */
  cfg_scale?: number
}

/**
 * AudioOutput
 */
export type ThinksoundAudioOutput = {
  /**
   * Prompt
   *
   * The prompt used to generate the audio.
   */
  prompt: string
  /**
   * Audio
   *
   * The generated audio file.
   */
  audio: FalAiThinksoundAudioFile
}

/**
 * File
 */
export type FalAiThinksoundAudioFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * Input
 */
export type ThinksoundInput = {
  /**
   * Prompt
   *
   * A prompt to guide the audio generation. If not provided, it will be extracted from the video.
   */
  prompt?: string
  /**
   * Video Url
   *
   * The URL of the video to generate the audio for.
   */
  video_url: string
  /**
   * Seed
   *
   * The seed for the random number generator
   */
  seed?: number
  /**
   * Number of Inference Steps
   *
   * The number of inference steps for audio generation.
   */
  num_inference_steps?: number
  /**
   * CFG Scale
   *
   * The classifier-free guidance scale for audio generation.
   */
  cfg_scale?: number
}

/**
 * Output
 */
export type ThinksoundOutput = {
  /**
   * Prompt
   *
   * The prompt used to generate the audio.
   */
  prompt: string
  /**
   * Video
   *
   * The generated video with audio.
   */
  video: FalAiThinksoundFile
}

/**
 * File
 */
export type FalAiThinksoundFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * FastExtendRequest
 */
export type PixverseExtendFastInput = {
  /**
   * Prompt
   *
   * Prompt describing how to extend the video
   */
  prompt: string
  /**
   * Video Url
   *
   * URL of the input video to extend
   */
  video_url: string
  /**
   * Resolution
   *
   * The resolution of the generated video. Fast mode doesn't support 1080p
   */
  resolution?: '360p' | '540p' | '720p'
  /**
   * Style
   *
   * The style of the extended video
   */
  style?: 'anime' | '3d_animation' | 'clay' | 'comic' | 'cyberpunk'
  /**
   * Model
   *
   * The model version to use for generation
   */
  model?: 'v3.5' | 'v4' | 'v4.5' | 'v5' | 'v5.5'
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string
}

/**
 * ExtendOutput
 */
export type PixverseExtendFastOutput = {
  /**
   * Video
   *
   * The extended video
   */
  video: FalAiPixverseExtendFastFile
}

/**
 * File
 */
export type FalAiPixverseExtendFastFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * ExtendRequest
 */
export type PixverseExtendInput = {
  /**
   * Prompt
   *
   * Prompt describing how to extend the video
   */
  prompt: string
  /**
   * Duration
   *
   * The duration of the generated video in seconds. 1080p videos are limited to 5 seconds
   */
  duration?: '5' | '8'
  /**
   * Video Url
   *
   * URL of the input video to extend
   */
  video_url: string
  /**
   * Style
   *
   * The style of the extended video
   */
  style?: 'anime' | '3d_animation' | 'clay' | 'comic' | 'cyberpunk'
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: '360p' | '540p' | '720p' | '1080p'
  /**
   * Model
   *
   * The model version to use for generation
   */
  model?: 'v3.5' | 'v4' | 'v4.5' | 'v5' | 'v5.5'
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string
}

/**
 * ExtendOutput
 */
export type PixverseExtendOutput = {
  /**
   * Video
   *
   * The extended video
   */
  video: FalAiPixverseExtendFile
}

/**
 * File
 */
export type FalAiPixverseExtendFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * LipsyncRequest
 */
export type PixverseLipsyncInput = {
  /**
   * Text
   *
   * Text content for TTS when audio_url is not provided
   */
  text?: string
  /**
   * Video Url
   *
   * URL of the input video
   */
  video_url: string
  /**
   * Audio Url
   *
   * URL of the input audio. If not provided, TTS will be used.
   */
  audio_url?: string
  /**
   * Voice Id
   *
   * Voice to use for TTS when audio_url is not provided
   */
  voice_id?:
    | 'Emily'
    | 'James'
    | 'Isabella'
    | 'Liam'
    | 'Chloe'
    | 'Adrian'
    | 'Harper'
    | 'Ava'
    | 'Sophia'
    | 'Julia'
    | 'Mason'
    | 'Jack'
    | 'Oliver'
    | 'Ethan'
    | 'Auto'
}

/**
 * LipsyncOutput
 */
export type PixverseLipsyncOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: FalAiPixverseLipsyncFile
}

/**
 * File
 */
export type FalAiPixverseLipsyncFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * ModifyVideoRequest
 */
export type LumaDreamMachineRay2ModifyInput = {
  /**
   * Prompt
   *
   * Instruction for modifying the video
   */
  prompt?: string
  /**
   * Video Url
   *
   * URL of the input video to modify
   */
  video_url: string
  /**
   * Mode
   *
   * Amount of modification to apply to the video, adhere_1 is the least amount of modification, reimagine_3 is the most
   */
  mode?:
    | 'adhere_1'
    | 'adhere_2'
    | 'adhere_3'
    | 'flex_1'
    | 'flex_2'
    | 'flex_3'
    | 'reimagine_1'
    | 'reimagine_2'
    | 'reimagine_3'
  /**
   * Image Url
   *
   * Optional URL of the first frame image for modification
   */
  image_url?: string
}

/**
 * ModifyOutput
 */
export type LumaDreamMachineRay2ModifyOutput = {
  /**
   * Video
   *
   * URL of the modified video
   */
  video: FalAiLumaDreamMachineRay2ModifyFile
}

/**
 * File
 */
export type FalAiLumaDreamMachineRay2ModifyFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * WanVACEReframeRequest
 */
export type WanVace14bReframeInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation. Optional for reframing.
   */
  prompt?: string
  /**
   * Video URL
   *
   * URL to the source video file. This video will be used as a reference for the reframe task.
   */
  video_url: string
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Trim Borders
   *
   * Whether to trim borders from the video.
   */
  trim_borders?: boolean
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Zoom Factor
   *
   * Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom.
   */
  zoom_factor?: number
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown
}

/**
 * WanVACEReframeResponse
 */
export type WanVace14bReframeOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FalAiWanVace14bReframeFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: FalAiWanVace14bReframeVideoFile
}

/**
 * File
 */
export type FalAiWanVace14bReframeFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * VideoFile
 */
export type FalAiWanVace14bReframeVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number | unknown
  /**
   * Height
   *
   * The height of the video
   */
  height?: number | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Width
   *
   * The width of the video
   */
  width?: number | unknown
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
}

/**
 * WanVACEOutpaintingRequest
 */
export type WanVace14bOutpaintingInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL to the source video file. Required for outpainting.
   */
  video_url: string
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown
  /**
   * Reference Image URLs
   *
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>
  /**
   * Expand Ratio
   *
   * Amount of expansion. This is a float value between 0 and 1, where 0.25 adds 25% to the original video size on the specified sides.
   */
  expand_ratio?: number
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number
  /**
   * Expand Bottom
   *
   * Whether to expand the video to the bottom.
   */
  expand_bottom?: boolean
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
  /**
   * Expand Top
   *
   * Whether to expand the video to the top.
   */
  expand_top?: boolean
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Expand Left
   *
   * Whether to expand the video to the left.
   */
  expand_left?: boolean
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Expand Right
   *
   * Whether to expand the video to the right.
   */
  expand_right?: boolean
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown
}

/**
 * WanVACEOutpaintingResponse
 */
export type WanVace14bOutpaintingOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FalAiWanVace14bOutpaintingFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: FalAiWanVace14bOutpaintingVideoFile
}

/**
 * File
 */
export type FalAiWanVace14bOutpaintingFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * VideoFile
 */
export type FalAiWanVace14bOutpaintingVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number | unknown
  /**
   * Height
   *
   * The height of the video
   */
  height?: number | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Width
   *
   * The width of the video
   */
  width?: number | unknown
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
}

/**
 * WanVACEInpaintingRequest
 */
export type WanVace14bInpaintingInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL to the source video file. Required for inpainting.
   */
  video_url: string
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown
  /**
   * Reference Image URLs
   *
   * Urls to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Mask Video URL
   *
   * URL to the source mask file. Required for inpainting.
   */
  mask_video_url: string | unknown
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Mask Image URL
   *
   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video using salient mask tracking. Will be ignored if mask_video_url is provided.
   */
  mask_image_url?: string | unknown
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown
}

/**
 * WanVACEInpaintingResponse
 */
export type WanVace14bInpaintingOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FalAiWanVace14bInpaintingFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: FalAiWanVace14bInpaintingVideoFile
}

/**
 * File
 */
export type FalAiWanVace14bInpaintingFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * VideoFile
 */
export type FalAiWanVace14bInpaintingVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number | unknown
  /**
   * Height
   *
   * The height of the video
   */
  height?: number | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Width
   *
   * The width of the video
   */
  width?: number | unknown
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
}

/**
 * WanVACEPoseRequest
 */
export type WanVace14bPoseInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation. For pose task, the prompt should describe the desired pose and action of the subject in the video.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL to the source video file. Required for pose task.
   */
  video_url: string
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown
  /**
   * Reference Image URLs
   *
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown
}

/**
 * WanVACEPoseResponse
 */
export type WanVace14bPoseOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FalAiWanVace14bPoseFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: FalAiWanVace14bPoseVideoFile
}

/**
 * File
 */
export type FalAiWanVace14bPoseFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * VideoFile
 */
export type FalAiWanVace14bPoseVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number | unknown
  /**
   * Height
   *
   * The height of the video
   */
  height?: number | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Width
   *
   * The width of the video
   */
  width?: number | unknown
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
}

/**
 * WanVACEDepthRequest
 */
export type WanVace14bDepthInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL to the source video file. Required for depth task.
   */
  video_url: string
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown
  /**
   * Reference Image URLs
   *
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown
}

/**
 * WanVACEDepthResponse
 */
export type WanVace14bDepthOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FalAiWanVace14bDepthFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: FalAiWanVace14bDepthVideoFile
}

/**
 * File
 */
export type FalAiWanVace14bDepthFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * VideoFile
 */
export type FalAiWanVace14bDepthVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number | unknown
  /**
   * Height
   *
   * The height of the video
   */
  height?: number | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Width
   *
   * The width of the video
   */
  width?: number | unknown
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
}

/**
 * DWPoseVideoInput
 */
export type DwposeVideoInput = {
  /**
   * Video Url
   *
   * URL of video to be used for pose estimation
   */
  video_url: string
  /**
   * Draw Mode
   *
   * Mode of drawing the pose on the video. Options are: 'full-pose', 'body-pose', 'face-pose', 'hand-pose', 'face-hand-mask', 'face-mask', 'hand-mask'.
   */
  draw_mode?:
    | 'full-pose'
    | 'body-pose'
    | 'face-pose'
    | 'hand-pose'
    | 'face-hand-mask'
    | 'face-mask'
    | 'hand-mask'
}

/**
 * DWPoseVideoOutput
 */
export type DwposeVideoOutput = {
  /**
   * Video
   *
   * The output video with pose estimation.
   */
  video: FalAiDwposeVideoFile
}

/**
 * File
 */
export type FalAiDwposeVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * CombineInput
 */
export type FfmpegApiMergeAudioVideoInput = {
  /**
   * Video Url
   *
   * URL of the video file to use as the video track
   */
  video_url: string
  /**
   * Start Offset
   *
   * Offset in seconds for when the audio should start relative to the video
   */
  start_offset?: number
  /**
   * Audio Url
   *
   * URL of the audio file to use as the audio track
   */
  audio_url: string
}

/**
 * CombineOutput
 */
export type FfmpegApiMergeAudioVideoOutput = {
  video: FalAiFfmpegApiMergeAudioVideoFile
}

/**
 * File
 */
export type FalAiFfmpegApiMergeAudioVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * WanT2VRequest
 */
export type WanVace13bInput = {
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Video Url
   *
   * URL to the source video file. If provided, the model will use this video as a reference.
   */
  video_url?: string
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Mask Image Url
   *
   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored.
   */
  mask_image_url?: string
  /**
   * Task
   *
   * Task type for the model.
   */
  task?: 'depth' | 'inpainting' | 'pose'
  /**
   * Frames Per Second
   *
   * Frames per second of the generated video. Must be between 5 to 24.
   */
  frames_per_second?: number
  /**
   * Ref Image Urls
   *
   * Urls to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Num Frames
   *
   * Number of frames to generate. Must be between 81 to 100 (inclusive). Works only with only reference images as input if source video or mask video is provided output len would be same as source up to 241 frames
   */
  num_frames?: number
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Resolution
   *
   * Resolution of the generated video (480p,580p, or 720p).
   */
  resolution?: '480p' | '580p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video (16:9 or 9:16).
   */
  aspect_ratio?: 'auto' | '9:16' | '16:9'
  /**
   * Mask Video Url
   *
   * URL to the source mask file. If provided, the model will use this mask as a reference.
   */
  mask_video_url?: string
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
}

/**
 * WanT2VResponse
 */
export type WanVace13bOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: FalAiWanVace13bFile
}

/**
 * File
 */
export type FalAiWanVace13bFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * ReframeVideoRequest
 */
export type LumaDreamMachineRay2FlashReframeInput = {
  /**
   * Prompt
   *
   * Optional prompt for reframing
   */
  prompt?: string
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the reframed video
   */
  aspect_ratio: '1:1' | '16:9' | '9:16' | '4:3' | '3:4' | '21:9' | '9:21'
  /**
   * Y Start
   *
   * Start Y coordinate for reframing
   */
  y_start?: number
  /**
   * X End
   *
   * End X coordinate for reframing
   */
  x_end?: number
  /**
   * Video Url
   *
   * URL of the input video to reframe
   */
  video_url: string
  /**
   * Y End
   *
   * End Y coordinate for reframing
   */
  y_end?: number
  /**
   * X Start
   *
   * Start X coordinate for reframing
   */
  x_start?: number
  /**
   * Grid Position Y
   *
   * Y position of the grid for reframing
   */
  grid_position_y?: number
  /**
   * Grid Position X
   *
   * X position of the grid for reframing
   */
  grid_position_x?: number
  /**
   * Image Url
   *
   * Optional URL of the first frame image for reframing
   */
  image_url?: string
}

/**
 * ReframeOutput
 */
export type LumaDreamMachineRay2FlashReframeOutput = {
  /**
   * Video
   *
   * URL of the reframed video
   */
  video: FalAiLumaDreamMachineRay2FlashReframeFile
}

/**
 * File
 */
export type FalAiLumaDreamMachineRay2FlashReframeFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * ReframeVideoRequest
 */
export type LumaDreamMachineRay2ReframeInput = {
  /**
   * Prompt
   *
   * Optional prompt for reframing
   */
  prompt?: string
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the reframed video
   */
  aspect_ratio: '1:1' | '16:9' | '9:16' | '4:3' | '3:4' | '21:9' | '9:21'
  /**
   * Y Start
   *
   * Start Y coordinate for reframing
   */
  y_start?: number
  /**
   * X End
   *
   * End X coordinate for reframing
   */
  x_end?: number
  /**
   * Video Url
   *
   * URL of the input video to reframe
   */
  video_url: string
  /**
   * Y End
   *
   * End Y coordinate for reframing
   */
  y_end?: number
  /**
   * X Start
   *
   * Start X coordinate for reframing
   */
  x_start?: number
  /**
   * Grid Position Y
   *
   * Y position of the grid for reframing
   */
  grid_position_y?: number
  /**
   * Grid Position X
   *
   * X position of the grid for reframing
   */
  grid_position_x?: number
  /**
   * Image Url
   *
   * Optional URL of the first frame image for reframing
   */
  image_url?: string
}

/**
 * ReframeOutput
 */
export type LumaDreamMachineRay2ReframeOutput = {
  /**
   * Video
   *
   * URL of the reframed video
   */
  video: FalAiLumaDreamMachineRay2ReframeFile
}

/**
 * File
 */
export type FalAiLumaDreamMachineRay2ReframeFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * LipsyncInput
 */
export type LipsyncInput = {
  /**
   * Video Url
   */
  video_url: string
  /**
   * Audio Url
   */
  audio_url: string
}

/**
 * LipsyncAppOutput
 */
export type LipsyncOutput = {
  video: VeedLipsyncFile
}

/**
 * File
 */
export type VeedLipsyncFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * WanVACERequest
 */
export type WanVace14bInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL to the source video file. If provided, the model will use this video as a reference.
   */
  video_url?: string | unknown
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown
  /**
   * Reference Image URLs
   *
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Mask Video URL
   *
   * URL to the source mask file. If provided, the model will use this mask as a reference.
   */
  mask_video_url?: string | unknown
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Mask Image URL
   *
   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored.
   */
  mask_image_url?: string | unknown
  /**
   * Task
   *
   * Task type for the model.
   */
  task?: 'depth' | 'pose' | 'inpainting' | 'outpainting' | 'reframe'
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown
}

/**
 * WanVACEResponse
 */
export type WanVace14bOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FalAiWanVace14bFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: FalAiWanVace14bVideoFile
}

/**
 * File
 */
export type FalAiWanVace14bFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * VideoFile
 */
export type FalAiWanVace14bVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number | unknown
  /**
   * Height
   *
   * The height of the video
   */
  height?: number | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Width
   *
   * The width of the video
   */
  width?: number | unknown
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
}

/**
 * VideoConditioningInput
 */
export type FalAiLtxVideo13bDistilledExtendVideoConditioningInput = {
  /**
   * Video URL
   *
   * URL of video to use as conditioning
   */
  video_url: string
  /**
   * Reverse Video
   *
   * Whether to reverse the video. This is useful for tasks where the video conditioning should be applied in reverse order.
   */
  reverse_video?: boolean
  /**
   * Start Frame Number
   *
   * Frame number of the video from which the conditioning starts. Must be a multiple of 8.
   */
  start_frame_num?: number
  /**
   * Limit Number of Frames
   *
   * Whether to limit the number of frames used from the video. If True, the `max_num_frames` parameter will be used to limit the number of frames.
   */
  limit_num_frames?: boolean
  /**
   * Resample FPS
   *
   * Whether to resample the video to a specific FPS. If True, the `target_fps` parameter will be used to resample the video.
   */
  resample_fps?: boolean
  /**
   * Strength
   *
   * Strength of the conditioning. 0.0 means no conditioning, 1.0 means full conditioning.
   */
  strength?: number
  /**
   * Target FPS
   *
   * Target FPS to resample the video to. Only relevant if `resample_fps` is True.
   */
  target_fps?: number
  /**
   * Maximum Number of Frames
   *
   * Maximum number of frames to use from the video. If None, all frames will be used.
   */
  max_num_frames?: number
  /**
   * Conditioning Type
   *
   * Type of conditioning this video provides. This is relevant to ensure in-context LoRA weights are applied correctly, as well as selecting the correct preprocessing pipeline, when enabled.
   */
  conditioning_type?: 'rgb' | 'depth' | 'pose' | 'canny'
  /**
   * Preprocess
   *
   * Whether to preprocess the video. If True, the video will be preprocessed to match the conditioning type. This is a no-op for RGB conditioning.
   */
  preprocess?: boolean
}

/**
 * ExtendVideoOutput
 */
export type LtxVideo13bDistilledExtendOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: FalAiLtxVideo13bDistilledExtendFile
}

/**
 * File
 */
export type FalAiLtxVideo13bDistilledExtendFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * VideoConditioningInput
 */
export type FalAiLtxVideo13bDistilledMulticonditioningVideoConditioningInput = {
  /**
   * Video URL
   *
   * URL of video to use as conditioning
   */
  video_url: string
  /**
   * Reverse Video
   *
   * Whether to reverse the video. This is useful for tasks where the video conditioning should be applied in reverse order.
   */
  reverse_video?: boolean
  /**
   * Start Frame Number
   *
   * Frame number of the video from which the conditioning starts. Must be a multiple of 8.
   */
  start_frame_num?: number
  /**
   * Limit Number of Frames
   *
   * Whether to limit the number of frames used from the video. If True, the `max_num_frames` parameter will be used to limit the number of frames.
   */
  limit_num_frames?: boolean
  /**
   * Resample FPS
   *
   * Whether to resample the video to a specific FPS. If True, the `target_fps` parameter will be used to resample the video.
   */
  resample_fps?: boolean
  /**
   * Strength
   *
   * Strength of the conditioning. 0.0 means no conditioning, 1.0 means full conditioning.
   */
  strength?: number
  /**
   * Target FPS
   *
   * Target FPS to resample the video to. Only relevant if `resample_fps` is True.
   */
  target_fps?: number
  /**
   * Maximum Number of Frames
   *
   * Maximum number of frames to use from the video. If None, all frames will be used.
   */
  max_num_frames?: number
  /**
   * Conditioning Type
   *
   * Type of conditioning this video provides. This is relevant to ensure in-context LoRA weights are applied correctly, as well as selecting the correct preprocessing pipeline, when enabled.
   */
  conditioning_type?: 'rgb' | 'depth' | 'pose' | 'canny'
  /**
   * Preprocess
   *
   * Whether to preprocess the video. If True, the video will be preprocessed to match the conditioning type. This is a no-op for RGB conditioning.
   */
  preprocess?: boolean
}

/**
 * MultiConditioningVideoOutput
 */
export type LtxVideo13bDistilledMulticonditioningOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: FalAiLtxVideo13bDistilledMulticonditioningFile
}

/**
 * File
 */
export type FalAiLtxVideo13bDistilledMulticonditioningFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * VideoConditioningInput
 */
export type FalAiLtxVideo13bDevMulticonditioningVideoConditioningInput = {
  /**
   * Video URL
   *
   * URL of video to use as conditioning
   */
  video_url: string
  /**
   * Start Frame Number
   *
   * Frame number of the video from which the conditioning starts. Must be a multiple of 8.
   */
  start_frame_num?: number
  /**
   * Reverse Video
   *
   * Whether to reverse the video. This is useful for tasks where the video conditioning should be applied in reverse order.
   */
  reverse_video?: boolean
  /**
   * Limit Number of Frames
   *
   * Whether to limit the number of frames used from the video. If True, the `max_num_frames` parameter will be used to limit the number of frames.
   */
  limit_num_frames?: boolean
  /**
   * Resample FPS
   *
   * Whether to resample the video to a specific FPS. If True, the `target_fps` parameter will be used to resample the video.
   */
  resample_fps?: boolean
  /**
   * Strength
   *
   * Strength of the conditioning. 0.0 means no conditioning, 1.0 means full conditioning.
   */
  strength?: number
  /**
   * Target FPS
   *
   * Target FPS to resample the video to. Only relevant if `resample_fps` is True.
   */
  target_fps?: number
  /**
   * Maximum Number of Frames
   *
   * Maximum number of frames to use from the video. If None, all frames will be used.
   */
  max_num_frames?: number
  /**
   * Conditioning Type
   *
   * Type of conditioning this video provides. This is relevant to ensure in-context LoRA weights are applied correctly, as well as selecting the correct preprocessing pipeline, when enabled.
   */
  conditioning_type?: 'rgb' | 'depth' | 'pose' | 'canny'
  /**
   * Preprocess
   *
   * Whether to preprocess the video. If True, the video will be preprocessed to match the conditioning type. This is a no-op for RGB conditioning.
   */
  preprocess?: boolean
}

/**
 * MultiConditioningVideoOutput
 */
export type LtxVideo13bDevMulticonditioningOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: FalAiLtxVideo13bDevMulticonditioningFile
}

/**
 * File
 */
export type FalAiLtxVideo13bDevMulticonditioningFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * VideoConditioningInput
 */
export type FalAiLtxVideo13bDevExtendVideoConditioningInput = {
  /**
   * Video URL
   *
   * URL of video to use as conditioning
   */
  video_url: string
  /**
   * Start Frame Number
   *
   * Frame number of the video from which the conditioning starts. Must be a multiple of 8.
   */
  start_frame_num?: number
  /**
   * Reverse Video
   *
   * Whether to reverse the video. This is useful for tasks where the video conditioning should be applied in reverse order.
   */
  reverse_video?: boolean
  /**
   * Limit Number of Frames
   *
   * Whether to limit the number of frames used from the video. If True, the `max_num_frames` parameter will be used to limit the number of frames.
   */
  limit_num_frames?: boolean
  /**
   * Resample FPS
   *
   * Whether to resample the video to a specific FPS. If True, the `target_fps` parameter will be used to resample the video.
   */
  resample_fps?: boolean
  /**
   * Strength
   *
   * Strength of the conditioning. 0.0 means no conditioning, 1.0 means full conditioning.
   */
  strength?: number
  /**
   * Target FPS
   *
   * Target FPS to resample the video to. Only relevant if `resample_fps` is True.
   */
  target_fps?: number
  /**
   * Maximum Number of Frames
   *
   * Maximum number of frames to use from the video. If None, all frames will be used.
   */
  max_num_frames?: number
  /**
   * Conditioning Type
   *
   * Type of conditioning this video provides. This is relevant to ensure in-context LoRA weights are applied correctly, as well as selecting the correct preprocessing pipeline, when enabled.
   */
  conditioning_type?: 'rgb' | 'depth' | 'pose' | 'canny'
  /**
   * Preprocess
   *
   * Whether to preprocess the video. If True, the video will be preprocessed to match the conditioning type. This is a no-op for RGB conditioning.
   */
  preprocess?: boolean
}

/**
 * ExtendVideoOutput
 */
export type LtxVideo13bDevExtendOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: FalAiLtxVideo13bDevExtendFile
}

/**
 * File
 */
export type FalAiLtxVideo13bDevExtendFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * MulticonditioningVideoInput
 *
 * Request model for text-to-video generation with multiple conditions.
 */
export type LtxVideoLoraMulticonditioningInput = {
  /**
   * Number Of Steps
   *
   * The number of inference steps to use.
   */
  number_of_steps?: number
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * Reverse Video
   *
   * Whether to reverse the video.
   */
  reverse_video?: boolean
  /**
   * Frame Rate
   *
   * The frame rate of the video.
   */
  frame_rate?: number
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using the LLM.
   */
  expand_prompt?: boolean
  /**
   * Number Of Frames
   *
   * The number of frames in the video.
   */
  number_of_frames?: number
  /**
   * Loras
   *
   * The LoRA weights to use for generation.
   */
  loras?: Array<LoRaWeight>
  /**
   * Images
   *
   * The image conditions to use for generation.
   */
  images?: Array<ImageCondition>
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * The negative prompt to use.
   */
  negative_prompt?: string
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the video.
   */
  aspect_ratio?: '16:9' | '1:1' | '9:16' | 'auto'
  /**
   * Resolution
   *
   * The resolution of the video.
   */
  resolution?: '480p' | '720p'
  /**
   * Videos
   *
   * The video conditions to use for generation.
   */
  videos?: Array<VideoCondition>
  /**
   * Seed
   *
   * The seed to use for generation.
   */
  seed?: number
}

/**
 * LoRAWeight
 *
 * LoRA weight to use for generation.
 */
export type LoRaWeight = {
  /**
   * Path
   *
   * URL or path to the LoRA weights.
   */
  path: string
  /**
   * Scale
   *
   * Scale of the LoRA weight. This is a multiplier applied to the LoRA weight when loading it.
   */
  scale?: number
  /**
   * Weight Name
   *
   * Name of the LoRA weight. Only used if `path` is a HuggingFace repository, and is only required when the repository contains multiple LoRA weights.
   */
  weight_name?: string
}

/**
 * ImageCondition
 *
 * Image condition to use for generation.
 */
export type ImageCondition = {
  /**
   * Strength
   *
   * The strength of the condition.
   */
  strength?: number
  /**
   * Start Frame Number
   *
   * The frame number to start the condition on.
   */
  start_frame_number?: number
  /**
   * Image Url
   *
   * The URL of the image to use as input.
   */
  image_url: string
}

/**
 * VideoCondition
 *
 * Video condition to use for generation.
 */
export type VideoCondition = {
  /**
   * Strength
   *
   * The strength of the condition.
   */
  strength?: number
  /**
   * Start Frame Number
   *
   * The frame number to start the condition on.
   */
  start_frame_number?: number
  /**
   * Video Url
   *
   * The URL of the video to use as input.
   */
  video_url: string
}

/**
 * MulticonditioningVideoOutput
 */
export type LtxVideoLoraMulticonditioningOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video.
   */
  video: FalAiLtxVideoLoraMulticonditioningFile
}

/**
 * File
 */
export type FalAiLtxVideoLoraMulticonditioningFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * MagiVideoExtensionRequest
 */
export type MagiExtendVideoInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.
   */
  resolution?: '480p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
   */
  aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1'
  /**
   * Video Url
   *
   * URL of the input video to represent the beginning of the video. If the input video does not match the chosen aspect ratio, it is resized and center cropped.
   */
  video_url: string
  /**
   * Start Frame
   *
   * The frame to begin the generation from, with the remaining frames will be treated as the prefix video. The final video will contain the frames up until this number unchanged, followed by the generated frames. The default start frame is 32 frames before the end of the video, which gives optimal results.
   */
  start_frame?: number
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: 4 | 8 | 16 | 32 | 64
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
  /**
   * Num Frames
   *
   * Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit.
   */
  num_frames?: number
}

/**
 * MagiVideoExtensionResponse
 */
export type MagiExtendVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: FalAiMagiExtendVideoFile
}

/**
 * File
 */
export type FalAiMagiExtendVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * MagiVideoExtensionRequest
 */
export type MagiDistilledExtendVideoInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.
   */
  resolution?: '480p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
   */
  aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1'
  /**
   * Video Url
   *
   * URL of the input video to represent the beginning of the video. If the input video does not match the chosen aspect ratio, it is resized and center cropped.
   */
  video_url: string
  /**
   * Start Frame
   *
   * The frame to begin the generation from, with the remaining frames will be treated as the prefix video. The final video will contain the frames up until this number unchanged, followed by the generated frames. The default start frame is 32 frames before the end of the video, which gives optimal results.
   */
  start_frame?: number
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: 4 | 8 | 16 | 32
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
  /**
   * Num Frames
   *
   * Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit.
   */
  num_frames?: number
}

/**
 * MagiVideoExtensionResponse
 */
export type MagiDistilledExtendVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: FalAiMagiDistilledExtendVideoFile
}

/**
 * File
 */
export type FalAiMagiDistilledExtendVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * WanT2VRequest
 */
export type WanVaceInput = {
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Video Url
   *
   * URL to the source video file. If provided, the model will use this video as a reference.
   */
  video_url?: string
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Ref Image Urls
   *
   * Urls to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>
  /**
   * Task
   *
   * Task type for the model.
   */
  task?: 'depth' | 'inpainting'
  /**
   * Frames Per Second
   *
   * Frames per second of the generated video. Must be between 5 to 24.
   */
  frames_per_second?: number
  /**
   * Mask Image Url
   *
   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored.
   */
  mask_image_url?: string
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Num Frames
   *
   * Number of frames to generate. Must be between 81 to 100 (inclusive). Works only with only reference images as input if source video or mask video is provided output len would be same as source up to 241 frames
   */
  num_frames?: number
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video (16:9 or 9:16).
   */
  aspect_ratio?: 'auto' | '9:16' | '16:9'
  /**
   * Resolution
   *
   * Resolution of the generated video (480p,580p, or 720p).
   */
  resolution?: '480p' | '580p' | '720p'
  /**
   * Mask Video Url
   *
   * URL to the source mask file. If provided, the model will use this mask as a reference.
   */
  mask_video_url?: string
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
}

/**
 * WanT2VResponse
 */
export type WanVaceOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: FalAiWanVaceFile
}

/**
 * File
 */
export type FalAiWanVaceFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * VideoInput
 *
 * Pydantic model for receiving a video file to analyze and re-sound.
 */
export type VideoSoundEffectsGeneratorInput = {
  video_url: CassetteaiVideoSoundEffectsGeneratorVideo
}

/**
 * Video
 *
 * Represents a video file.
 */
export type CassetteaiVideoSoundEffectsGeneratorVideo = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File | unknown
}

/**
 * VideoOutput
 *
 * Pydantic model for returning the re-sounded video back to the client.
 */
export type VideoSoundEffectsGeneratorOutput = {
  video: CassetteaiVideoSoundEffectsGeneratorFile
}

/**
 * File
 */
export type CassetteaiVideoSoundEffectsGeneratorFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * LipSyncV2Input
 */
export type SyncLipsyncV2Input = {
  /**
   * Model
   *
   * The model to use for lipsyncing. `lipsync-2-pro` will cost roughly 1.67 times as much as `lipsync-2` for the same duration.
   */
  model?: 'lipsync-2' | 'lipsync-2-pro'
  /**
   * Video Url
   *
   * URL of the input video
   */
  video_url: string
  /**
   * Sync Mode
   *
   * Lipsync mode when audio and video durations are out of sync.
   */
  sync_mode?: 'cut_off' | 'loop' | 'bounce' | 'silence' | 'remap'
  /**
   * Audio Url
   *
   * URL of the input audio
   */
  audio_url: string
}

/**
 * LipSyncV2Output
 */
export type SyncLipsyncV2Output = {
  /**
   * Video
   *
   * The generated video
   */
  video: FalAiSyncLipsyncV2File
}

/**
 * File
 */
export type FalAiSyncLipsyncV2File = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * Input
 */
export type LatentsyncInput = {
  /**
   * Video Url
   *
   * The URL of the video to generate the lip sync for.
   */
  video_url: string
  /**
   * Guidance Scale
   *
   * Guidance scale for the model inference
   */
  guidance_scale?: number
  /**
   * Seed
   *
   * Random seed for generation. If None, a random seed will be used.
   */
  seed?: number
  /**
   * Audio Url
   *
   * The URL of the audio to generate the lip sync for.
   */
  audio_url: string
  /**
   * Loop Mode
   *
   * Video loop mode when audio is longer than video. Options: pingpong, loop
   */
  loop_mode?: 'pingpong' | 'loop'
}

/**
 * Output
 */
export type LatentsyncOutput = {
  /**
   * Video
   *
   * The generated video with the lip sync.
   */
  video: FalAiLatentsyncFile
}

/**
 * File
 */
export type FalAiLatentsyncFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * PikadditionsRequest
 *
 * Request model for Pikadditions endpoint
 */
export type PikaV2PikadditionsInput = {
  /**
   * Prompt
   *
   * Text prompt describing what to add
   */
  prompt?: string
  /**
   * Video Url
   *
   * URL of the input video
   */
  video_url: string
  /**
   * Seed
   *
   * The seed for the random number generator
   */
  seed?: number
  /**
   * Negative Prompt
   *
   * Negative prompt to guide the model
   */
  negative_prompt?: string
  /**
   * Image Url
   *
   * URL of the image to add
   */
  image_url: string
}

/**
 * PikadditionsOutput
 *
 * Output from Pikadditions generation
 */
export type PikaV2PikadditionsOutput = {
  /**
   * Video
   *
   * The generated video with added objects/images
   */
  video: FalAiPikaV2PikadditionsFile
}

/**
 * File
 */
export type FalAiPikaV2PikadditionsFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * VideoConditioningInput
 */
export type FalAiLtxVideoV095MulticonditioningVideoConditioningInput = {
  /**
   * Video Url
   *
   * URL of video to be extended
   */
  video_url: string
  /**
   * Start Frame Num
   *
   * Frame number of the video from which the conditioning starts. Must be a multiple of 8.
   */
  start_frame_num: number
}

/**
 * MulticonditioningVideoOutput
 */
export type LtxVideoV095MulticonditioningOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: FalAiLtxVideoV095MulticonditioningFile
}

/**
 * File
 */
export type FalAiLtxVideoV095MulticonditioningFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * VideoConditioningInput
 */
export type FalAiLtxVideoV095ExtendVideoConditioningInput = {
  /**
   * Video Url
   *
   * URL of video to be extended
   */
  video_url: string
  /**
   * Start Frame Num
   *
   * Frame number of the video from which the conditioning starts. Must be a multiple of 8.
   */
  start_frame_num: number
}

/**
 * ExtendVideoOutput
 */
export type LtxVideoV095ExtendOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: FalAiLtxVideoV095ExtendFile
}

/**
 * File
 */
export type FalAiLtxVideoV095ExtendFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * VideoUpscaleRequest
 */
export type TopazUpscaleVideoInput = {
  /**
   * H264 Output
   *
   * Whether to use H264 codec for output video. Default is H265.
   */
  H264_output?: boolean
  /**
   * Video Url
   *
   * URL of the video to upscale
   */
  video_url: string
  /**
   * Upscale Factor
   *
   * Factor to upscale the video by (e.g. 2.0 doubles width and height)
   */
  upscale_factor?: number
  /**
   * Target Fps
   *
   * Target FPS for frame interpolation. If set, frame interpolation will be enabled.
   */
  target_fps?: number
}

/**
 * VideoUpscaleOutput
 */
export type TopazUpscaleVideoOutput = {
  /**
   * Video
   *
   * The upscaled video file
   */
  video: FalAiTopazUpscaleVideoFile
}

/**
 * File
 */
export type FalAiTopazUpscaleVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * Ben2InputVideo
 */
export type BenV2VideoInput = {
  /**
   * Video Url
   *
   * URL of video to be used for background removal.
   */
  video_url: string
  /**
   * Seed
   *
   * Random seed for reproducible generation.
   */
  seed?: number
  /**
   * Background Color
   *
   * Optional RGB values (0-255) for the background color. If not provided, the background will be transparent. For ex: [0, 0, 0]
   */
  background_color?: [unknown, unknown, unknown]
}

/**
 * Ben2OutputVideo
 */
export type BenV2VideoOutput = {
  /**
   * Seed
   *
   *
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   *
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: FalAiBenV2VideoFile
}

/**
 * File
 */
export type FalAiBenV2VideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * HunyuanV2VRequest
 */
export type HunyuanVideoVideoToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * Aspect Ratio (W:H)
   *
   * The aspect ratio of the video to generate.
   */
  aspect_ratio?: '16:9' | '9:16'
  /**
   * Resolution
   *
   * The resolution of the video to generate.
   */
  resolution?: '480p' | '580p' | '720p'
  /**
   * Video Url
   *
   * URL of the video input.
   */
  video_url: string
  /**
   * Strength
   *
   * Strength for Video-to-Video
   */
  strength?: number
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Num Inference Steps
   *
   * The number of inference steps to run. Lower gets faster results, higher gets better results.
   */
  num_inference_steps?: number
  /**
   * Seed
   *
   * The seed to use for generating the video.
   */
  seed?: number
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: '129' | '85'
  /**
   * Pro Mode
   *
   * By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.
   */
  pro_mode?: boolean
}

/**
 * HunyuanT2VResponse
 */
export type HunyuanVideoVideoToVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generating the video.
   */
  seed: number
  /**
   * Video
   */
  video: FalAiHunyuanVideoVideoToVideoFile
}

/**
 * File
 */
export type FalAiHunyuanVideoVideoToVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * HunyuanV2VRequest
 */
export type HunyuanVideoLoraVideoToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * Aspect Ratio (W:H)
   *
   * The aspect ratio of the video to generate.
   */
  aspect_ratio?: '16:9' | '9:16'
  /**
   * Resolution
   *
   * The resolution of the video to generate.
   */
  resolution?: '480p' | '580p' | '720p'
  /**
   * Video Url
   *
   * URL of the video
   */
  video_url: string
  /**
   * Loras
   *
   *
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   *
   */
  loras?: Array<LoraWeight>
  /**
   * Strength
   *
   * Strength of video-to-video
   */
  strength?: number
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Seed
   *
   * The seed to use for generating the video.
   */
  seed?: number
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: '129' | '85'
  /**
   * Pro Mode
   *
   * By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.
   */
  pro_mode?: boolean
}

/**
 * LoraWeight
 */
export type LoraWeight = {
  /**
   * Path
   *
   * URL or the path to the LoRA weights.
   */
  path: string
  /**
   * Scale
   *
   *
   * The scale of the LoRA weight. This is used to scale the LoRA weight
   * before merging it with the base model.
   *
   */
  scale?: number
}

/**
 * HunyuanV2VResponse
 */
export type HunyuanVideoLoraVideoToVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generating the video.
   */
  seed: number
  /**
   * Video
   */
  video: FalAiHunyuanVideoLoraVideoToVideoFile
}

/**
 * File
 */
export type FalAiHunyuanVideoLoraVideoToVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * Input
 */
export type FfmpegApiComposeInput = {
  /**
   * Tracks
   *
   * List of tracks to be combined into the final media
   */
  tracks: Array<Track>
}

/**
 * Track
 */
export type Track = {
  /**
   * Type
   *
   * Type of track ('video' or 'audio')
   */
  type: string
  /**
   * Id
   *
   * Unique identifier for the track
   */
  id: string
  /**
   * Keyframes
   *
   * List of keyframes that make up this track
   */
  keyframes: Array<Keyframe>
}

/**
 * Keyframe
 */
export type Keyframe = {
  /**
   * Duration
   *
   * The duration in milliseconds of this keyframe
   */
  duration: number
  /**
   * Timestamp
   *
   * The timestamp in milliseconds where this keyframe starts
   */
  timestamp: number
  /**
   * Url
   *
   * The URL where this keyframe's media file can be accessed
   */
  url: string
}

/**
 * ComposeOutput
 */
export type FfmpegApiComposeOutput = {
  /**
   * Video Url
   *
   * URL of the processed video file
   */
  video_url: string
  /**
   * Thumbnail Url
   *
   * URL of the video's thumbnail image
   */
  thumbnail_url: string
}

/**
 * LipSyncInput
 */
export type SyncLipsyncInput = {
  /**
   * Model
   *
   * The model to use for lipsyncing
   */
  model?: 'lipsync-1.8.0' | 'lipsync-1.7.1' | 'lipsync-1.9.0-beta'
  /**
   * Video Url
   *
   * URL of the input video
   */
  video_url: string
  /**
   * Sync Mode
   *
   * Lipsync mode when audio and video durations are out of sync.
   */
  sync_mode?: 'cut_off' | 'loop' | 'bounce' | 'silence' | 'remap'
  /**
   * Audio Url
   *
   * URL of the input audio
   */
  audio_url: string
}

/**
 * LipSyncOutput
 */
export type SyncLipsyncOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: FalAiSyncLipsyncFile
}

/**
 * File
 */
export type FalAiSyncLipsyncFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * CaptionInput
 */
export type AutoCaptionInput = {
  /**
   * Txt Font
   *
   * Font for generated captions. Choose one in 'Arial','Standard','Garamond', 'Times New Roman','Georgia', or pass a url to a .ttf file
   */
  txt_font?: string
  /**
   * Video Url
   *
   * URL to the .mp4 video with audio. Only videos of size <100MB are allowed.
   */
  video_url: string
  /**
   * Top Align
   *
   * Top-to-bottom alignment of the text. Can be a string ('top', 'center', 'bottom') or a float (0.0-1.0)
   */
  top_align?: string | number
  /**
   * Txt Color
   *
   * Colour of the text. Can be a RGB tuple, a color name, or an hexadecimal notation.
   */
  txt_color?: string
  /**
   * Stroke Width
   *
   * Width of the text strokes in pixels
   */
  stroke_width?: number
  /**
   * Refresh Interval
   *
   * Number of seconds the captions should stay on screen. A higher number will also result in more text being displayed at once.
   */
  refresh_interval?: number
  /**
   * Font Size
   *
   * Size of text in generated captions.
   */
  font_size?: number
  /**
   * Left Align
   *
   * Left-to-right alignment of the text. Can be a string ('left', 'center', 'right') or a float (0.0-1.0)
   */
  left_align?: string | number
}

/**
 * Output
 */
export type AutoCaptionOutput = {
  /**
   * Video Url
   *
   * URL to the caption .mp4 video.
   */
  video_url: string
}

/**
 * InputModel
 */
export type DubbingInput = {
  /**
   * Do Lipsync
   *
   * Whether to lip sync the audio to the video
   */
  do_lipsync?: boolean
  /**
   * Video Url
   *
   * Input video URL to be dubbed.
   */
  video_url: string
  /**
   * Target Language
   *
   * Target language to dub the video to
   */
  target_language?: 'hindi' | 'turkish' | 'english'
}

/**
 * OutputModel
 */
export type DubbingOutput = {
  /**
   * Video
   *
   * The generated video with the lip sync.
   */
  video: FalAiDubbingFile
}

/**
 * File
 */
export type FalAiDubbingFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * Input
 */
export type VideoUpscalerInput = {
  /**
   * Video Url
   *
   * The URL of the video to upscale
   */
  video_url: string
  /**
   * Scale
   *
   * The scale factor
   */
  scale?: number
}

/**
 * Output
 */
export type VideoUpscalerOutput = {
  /**
   * Video
   *
   * The stitched video
   */
  video: FalAiVideoUpscalerFile
}

/**
 * File
 */
export type FalAiVideoUpscalerFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * VideoToVideoInput
 */
export type Cogvideox5bVideoToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * Input Video Url
   *
   * The video to generate the video from.
   */
  video_url: string
  /**
   * Use Rife
   *
   * Use RIFE for video interpolation
   */
  use_rife?: boolean
  /**
   * Loras
   *
   *
   * The LoRAs to use for the image generation. We currently support one lora.
   *
   */
  loras?: Array<FalAiCogvideox5bVideoToVideoLoraWeight>
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | FalAiCogvideox5bVideoToVideoImageSize
    | 'square_hd'
    | 'square'
    | 'portrait_4_3'
    | 'portrait_16_9'
    | 'landscape_4_3'
    | 'landscape_16_9'
  /**
   * Strength
   *
   * The strength to use for Video to Video.  1.0 completely remakes the video while 0.0 preserves the original.
   */
  strength?: number
  /**
   * Guidance scale (CFG)
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related video to show you.
   *
   */
  guidance_scale?: number
  /**
   * Num Inference Steps
   *
   * The number of inference steps to perform.
   */
  num_inference_steps?: number
  /**
   * Export Fps
   *
   * The target FPS of the video
   */
  export_fps?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to generate video from
   */
  negative_prompt?: string
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number
}

/**
 * LoraWeight
 */
export type FalAiCogvideox5bVideoToVideoLoraWeight = {
  /**
   * Path
   *
   * URL or the path to the LoRA weights.
   */
  path: string
  /**
   * Scale
   *
   *
   * The scale of the LoRA weight. This is used to scale the LoRA weight
   * before merging it with the base model.
   *
   */
  scale?: number
}

/**
 * ImageSize
 */
export type FalAiCogvideox5bVideoToVideoImageSize = {
  /**
   * Height
   *
   * The height of the generated image.
   */
  height?: number
  /**
   * Width
   *
   * The width of the generated image.
   */
  width?: number
}

/**
 * Output
 */
export type Cogvideox5bVideoToVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for generating the video.
   */
  prompt: string
  /**
   * Timings
   */
  timings: {
    [key: string]: number
  }
  /**
   * Seed
   *
   *
   * Seed of the generated video. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   *
   */
  seed: number
  /**
   * Video
   *
   * The URL to the generated video
   */
  video: FalAiCogvideox5bVideoToVideoFile
}

/**
 * File
 */
export type FalAiCogvideox5bVideoToVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * ControlNeXtInput
 */
export type ControlnextInput = {
  /**
   * Controlnext Cond Scale
   *
   * Condition scale for ControlNeXt.
   */
  controlnext_cond_scale?: number
  /**
   * Video Url
   *
   * URL of the input video.
   */
  video_url: string
  /**
   * Fps
   *
   * Frames per second for the output video.
   */
  fps?: number
  /**
   * Max Frame Num
   *
   * Maximum number of frames to process.
   */
  max_frame_num?: number
  /**
   * Width
   *
   * Width of the output video.
   */
  width?: number
  /**
   * Overlap
   *
   * Number of overlapping frames between batches.
   */
  overlap?: number
  /**
   * Guidance Scale
   *
   * Guidance scale for the diffusion process.
   */
  guidance_scale?: number
  /**
   * Batch Frames
   *
   * Number of frames to process in each batch.
   */
  batch_frames?: number
  /**
   * Height
   *
   * Height of the output video.
   */
  height?: number
  /**
   * Sample Stride
   *
   * Stride for sampling frames from the input video.
   */
  sample_stride?: number
  /**
   * Image Url
   *
   * URL of the reference image.
   */
  image_url: string
  /**
   * Decode Chunk Size
   *
   * Chunk size for decoding frames.
   */
  decode_chunk_size?: number
  /**
   * Motion Bucket Id
   *
   * Motion bucket ID for the pipeline.
   */
  motion_bucket_id?: number
  /**
   * Num Inference Steps
   *
   * Number of inference steps.
   */
  num_inference_steps?: number
}

/**
 * ControlNeXtOutput
 */
export type ControlnextOutput = {
  /**
   * The generated video.
   */
  video: FalAiControlnextFile
}

/**
 * File
 */
export type FalAiControlnextFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * SAM2VideoRLEInput
 */
export type Sam2VideoInput = {
  /**
   * Video Url
   *
   * The URL of the video to be segmented.
   */
  video_url: string
  /**
   * Prompts
   *
   * List of prompts to segment the video
   */
  prompts?: Array<FalAiSam2VideoPointPrompt>
  /**
   * Boundingbox Zip
   *
   * Return per-frame bounding box overlays as a zip archive.
   */
  boundingbox_zip?: boolean
  /**
   * Mask Url
   *
   * The URL of the mask to be applied initially.
   */
  mask_url?: string
  /**
   * Apply Mask
   *
   * Apply the mask on the video.
   */
  apply_mask?: boolean
  /**
   * Box Prompts
   *
   * Coordinates for boxes
   */
  box_prompts?: Array<FalAiSam2VideoBoxPrompt>
}

/**
 * PointPrompt
 */
export type FalAiSam2VideoPointPrompt = {
  /**
   * Y
   *
   * Y Coordinate of the prompt
   */
  y?: number
  /**
   * Label
   *
   * Label of the prompt. 1 for foreground, 0 for background
   */
  label?: 0 | 1
  /**
   * Frame Index
   *
   * The frame index to interact with.
   */
  frame_index?: number
  /**
   * X
   *
   * X Coordinate of the prompt
   */
  x?: number
}

/**
 * BoxPrompt
 */
export type FalAiSam2VideoBoxPrompt = {
  /**
   * Y Min
   *
   * Y Min Coordinate of the box
   */
  y_min?: number
  /**
   * Frame Index
   *
   * The frame index to interact with.
   */
  frame_index?: number
  /**
   * X Max
   *
   * X Max Coordinate of the prompt
   */
  x_max?: number
  /**
   * X Min
   *
   * X Min Coordinate of the box
   */
  x_min?: number
  /**
   * Y Max
   *
   * Y Max Coordinate of the prompt
   */
  y_max?: number
}

/**
 * SAM2VideoOutput
 */
export type Sam2VideoOutput = {
  /**
   * Boundingbox Frames Zip
   *
   * Zip file containing per-frame bounding box overlays.
   */
  boundingbox_frames_zip?: FalAiSam2VideoFile
  /**
   * Video
   *
   * The segmented video.
   */
  video: FalAiSam2VideoFile
}

/**
 * File
 */
export type FalAiSam2VideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * AMTInterpolationInput
 */
export type AmtInterpolationInput = {
  /**
   * Video URL
   *
   * URL of the video to be processed
   */
  video_url: string
  /**
   * Recursive Interpolation Passes
   *
   * Number of recursive interpolation passes
   */
  recursive_interpolation_passes?: number
  /**
   * Output FPS
   *
   * Output frames per second
   */
  output_fps?: number
}

/**
 * AMTInterpolationOutput
 */
export type AmtInterpolationOutput = {
  /**
   * Video
   *
   * Generated video
   */
  video: FalAiAmtInterpolationFile
}

/**
 * File
 */
export type FalAiAmtInterpolationFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * AnimateDiffV2VTurboInput
 */
export type FastAnimatediffTurboVideoToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to use for generating the image. Be as descriptive as possible for best results.
   */
  prompt: string
  /**
   * Video Url
   *
   * URL of the video.
   */
  video_url: string
  /**
   * First N Seconds
   *
   * The first N number of seconds of video to animate.
   */
  first_n_seconds?: number
  /**
   * Fps
   *
   * Number of frames per second to extract from the video.
   */
  fps?: number
  /**
   * Strength
   *
   * The strength of the input video in the final output.
   */
  strength?: number
  /**
   * Guidance Scale
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
   */
  guidance_scale?: number
  /**
   * Num Inference Steps
   *
   * The number of inference steps to perform. 4-12 is recommended for turbo mode.
   */
  num_inference_steps?: number
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of Stable Diffusion
   * will output the same image every time.
   *
   */
  seed?: number
  /**
   * Negative Prompt
   *
   *
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution).
   *
   */
  negative_prompt?: string
  /**
   * Motions
   *
   * The motions to apply to the video.
   */
  motions?: Array<
    'zoom-out' | 'zoom-in' | 'pan-left' | 'pan-right' | 'tilt-up' | 'tilt-down'
  >
}

/**
 * AnimateDiffV2VOutput
 */
export type FastAnimatediffTurboVideoToVideoOutput = {
  /**
   * Seed
   *
   * Seed used for generating the video.
   */
  seed: number
  /**
   * Video
   *
   * Generated video file.
   */
  video: FalAiFastAnimatediffTurboVideoToVideoFile
}

/**
 * File
 */
export type FalAiFastAnimatediffTurboVideoToVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * AnimateDiffV2VInput
 */
export type FastAnimatediffVideoToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to use for generating the image. Be as descriptive as possible for best results.
   */
  prompt: string
  /**
   * Video Url
   *
   * URL of the video.
   */
  video_url: string
  /**
   * First N Seconds
   *
   * The first N number of seconds of video to animate.
   */
  first_n_seconds?: number
  /**
   * Fps
   *
   * Number of frames per second to extract from the video.
   */
  fps?: number
  /**
   * Strength
   *
   * The strength of the input video in the final output.
   */
  strength?: number
  /**
   * Guidance scale (CFG)
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you.
   *
   */
  guidance_scale?: number
  /**
   * Num Inference Steps
   *
   * The number of inference steps to perform.
   */
  num_inference_steps?: number
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of Stable Diffusion
   * will output the same image every time.
   *
   */
  seed?: number
  /**
   * Negative Prompt
   *
   *
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution).
   *
   */
  negative_prompt?: string
  /**
   * Motions
   *
   * The motions to apply to the video.
   */
  motions?: Array<
    'zoom-out' | 'zoom-in' | 'pan-left' | 'pan-right' | 'tilt-up' | 'tilt-down'
  >
}

/**
 * AnimateDiffV2VOutput
 */
export type FastAnimatediffVideoToVideoOutput = {
  /**
   * Seed
   *
   * Seed used for generating the video.
   */
  seed: number
  /**
   * Video
   *
   * Generated video file.
   */
  video: FalAiFastAnimatediffVideoToVideoFile
}

/**
 * File
 */
export type FalAiFastAnimatediffVideoToVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}
