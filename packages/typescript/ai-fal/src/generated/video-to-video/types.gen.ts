// This file is auto-generated by @hey-api/openapi-ts

export type ClientOptions = {
  baseUrl: 'https://queue.fal.run' | (string & {})
}

/**
 * AnimateDiffV2VOutput
 */
export type SchemaFastAnimatediffVideoToVideoOutput = {
  /**
   * Seed
   *
   * Seed used for generating the video.
   */
  seed: number
  /**
   * Video
   *
   * Generated video file.
   */
  video: SchemaFile
}

/**
 * File
 */
export type SchemaFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * AnimateDiffV2VInput
 */
export type SchemaFastAnimatediffVideoToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to use for generating the image. Be as descriptive as possible for best results.
   */
  prompt: string
  /**
   * Video Url
   *
   * URL of the video.
   */
  video_url: string
  /**
   * First N Seconds
   *
   * The first N number of seconds of video to animate.
   */
  first_n_seconds?: number
  /**
   * Fps
   *
   * Number of frames per second to extract from the video.
   */
  fps?: number
  /**
   * Strength
   *
   * The strength of the input video in the final output.
   */
  strength?: number
  /**
   * Guidance scale (CFG)
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you.
   *
   */
  guidance_scale?: number
  /**
   * Num Inference Steps
   *
   * The number of inference steps to perform.
   */
  num_inference_steps?: number
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of Stable Diffusion
   * will output the same image every time.
   *
   */
  seed?: number
  /**
   * Negative Prompt
   *
   *
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution).
   *
   */
  negative_prompt?: string
  /**
   * Motions
   *
   * The motions to apply to the video.
   */
  motions?: Array<
    'zoom-out' | 'zoom-in' | 'pan-left' | 'pan-right' | 'tilt-up' | 'tilt-down'
  >
}

/**
 * AnimateDiffV2VOutput
 */
export type SchemaFastAnimatediffTurboVideoToVideoOutput = {
  /**
   * Seed
   *
   * Seed used for generating the video.
   */
  seed: number
  /**
   * Video
   *
   * Generated video file.
   */
  video: SchemaFile
}

/**
 * AnimateDiffV2VTurboInput
 */
export type SchemaFastAnimatediffTurboVideoToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to use for generating the image. Be as descriptive as possible for best results.
   */
  prompt: string
  /**
   * Video Url
   *
   * URL of the video.
   */
  video_url: string
  /**
   * First N Seconds
   *
   * The first N number of seconds of video to animate.
   */
  first_n_seconds?: number
  /**
   * Fps
   *
   * Number of frames per second to extract from the video.
   */
  fps?: number
  /**
   * Strength
   *
   * The strength of the input video in the final output.
   */
  strength?: number
  /**
   * Guidance Scale
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
   */
  guidance_scale?: number
  /**
   * Num Inference Steps
   *
   * The number of inference steps to perform. 4-12 is recommended for turbo mode.
   */
  num_inference_steps?: number
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of Stable Diffusion
   * will output the same image every time.
   *
   */
  seed?: number
  /**
   * Negative Prompt
   *
   *
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution).
   *
   */
  negative_prompt?: string
  /**
   * Motions
   *
   * The motions to apply to the video.
   */
  motions?: Array<
    'zoom-out' | 'zoom-in' | 'pan-left' | 'pan-right' | 'tilt-up' | 'tilt-down'
  >
}

/**
 * AMTInterpolationOutput
 */
export type SchemaAmtInterpolationOutput = {
  /**
   * Video
   *
   * Generated video
   */
  video: SchemaFile
}

/**
 * AMTInterpolationInput
 */
export type SchemaAmtInterpolationInput = {
  /**
   * Video URL
   *
   * URL of the video to be processed
   */
  video_url: string
  /**
   * Recursive Interpolation Passes
   *
   * Number of recursive interpolation passes
   */
  recursive_interpolation_passes?: number
  /**
   * Output FPS
   *
   * Output frames per second
   */
  output_fps?: number
}

/**
 * SAM2VideoOutput
 */
export type SchemaSam2VideoOutput = {
  /**
   * Boundingbox Frames Zip
   *
   * Zip file containing per-frame bounding box overlays.
   */
  boundingbox_frames_zip?: SchemaFile
  /**
   * Video
   *
   * The segmented video.
   */
  video: SchemaFile
}

/**
 * SAM2VideoRLEInput
 */
export type SchemaSam2VideoInput = {
  /**
   * Video Url
   *
   * The URL of the video to be segmented.
   */
  video_url: string
  /**
   * Prompts
   *
   * List of prompts to segment the video
   */
  prompts?: Array<SchemaPointPrompt>
  /**
   * Boundingbox Zip
   *
   * Return per-frame bounding box overlays as a zip archive.
   */
  boundingbox_zip?: boolean
  /**
   * Mask Url
   *
   * The URL of the mask to be applied initially.
   */
  mask_url?: string
  /**
   * Apply Mask
   *
   * Apply the mask on the video.
   */
  apply_mask?: boolean
  /**
   * Box Prompts
   *
   * Coordinates for boxes
   */
  box_prompts?: Array<SchemaBoxPrompt>
}

/**
 * BoxPrompt
 */
export type SchemaBoxPrompt = {
  /**
   * Y Min
   *
   * Y Min Coordinate of the box
   */
  y_min?: number
  /**
   * Frame Index
   *
   * The frame index to interact with.
   */
  frame_index?: number
  /**
   * X Max
   *
   * X Max Coordinate of the prompt
   */
  x_max?: number
  /**
   * X Min
   *
   * X Min Coordinate of the box
   */
  x_min?: number
  /**
   * Y Max
   *
   * Y Max Coordinate of the prompt
   */
  y_max?: number
}

/**
 * PointPrompt
 */
export type SchemaPointPrompt = {
  /**
   * Y
   *
   * Y Coordinate of the prompt
   */
  y?: number
  /**
   * Label
   *
   * Label of the prompt. 1 for foreground, 0 for background
   */
  label?: 0 | 1
  /**
   * Frame Index
   *
   * The frame index to interact with.
   */
  frame_index?: number
  /**
   * X
   *
   * X Coordinate of the prompt
   */
  x?: number
}

/**
 * ControlNeXtOutput
 */
export type SchemaControlnextOutput = {
  /**
   * The generated video.
   */
  video: SchemaFile
}

/**
 * ControlNeXtInput
 */
export type SchemaControlnextInput = {
  /**
   * Controlnext Cond Scale
   *
   * Condition scale for ControlNeXt.
   */
  controlnext_cond_scale?: number
  /**
   * Video Url
   *
   * URL of the input video.
   */
  video_url: string
  /**
   * Fps
   *
   * Frames per second for the output video.
   */
  fps?: number
  /**
   * Max Frame Num
   *
   * Maximum number of frames to process.
   */
  max_frame_num?: number
  /**
   * Width
   *
   * Width of the output video.
   */
  width?: number
  /**
   * Overlap
   *
   * Number of overlapping frames between batches.
   */
  overlap?: number
  /**
   * Guidance Scale
   *
   * Guidance scale for the diffusion process.
   */
  guidance_scale?: number
  /**
   * Batch Frames
   *
   * Number of frames to process in each batch.
   */
  batch_frames?: number
  /**
   * Height
   *
   * Height of the output video.
   */
  height?: number
  /**
   * Sample Stride
   *
   * Stride for sampling frames from the input video.
   */
  sample_stride?: number
  /**
   * Image Url
   *
   * URL of the reference image.
   */
  image_url: string
  /**
   * Decode Chunk Size
   *
   * Chunk size for decoding frames.
   */
  decode_chunk_size?: number
  /**
   * Motion Bucket Id
   *
   * Motion bucket ID for the pipeline.
   */
  motion_bucket_id?: number
  /**
   * Num Inference Steps
   *
   * Number of inference steps.
   */
  num_inference_steps?: number
}

/**
 * Output
 */
export type SchemaCogvideox5bVideoToVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for generating the video.
   */
  prompt: string
  /**
   * Timings
   */
  timings: {
    [key: string]: number
  }
  /**
   * Seed
   *
   *
   * Seed of the generated video. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   *
   */
  seed: number
  /**
   * Video
   *
   * The URL to the generated video
   */
  video: SchemaFile
}

/**
 * VideoToVideoInput
 */
export type SchemaCogvideox5bVideoToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * Input Video Url
   *
   * The video to generate the video from.
   */
  video_url: string
  /**
   * Use Rife
   *
   * Use RIFE for video interpolation
   */
  use_rife?: boolean
  /**
   * Loras
   *
   *
   * The LoRAs to use for the image generation. We currently support one lora.
   *
   */
  loras?: Array<SchemaLoraWeight>
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | SchemaImageSize
    | 'square_hd'
    | 'square'
    | 'portrait_4_3'
    | 'portrait_16_9'
    | 'landscape_4_3'
    | 'landscape_16_9'
  /**
   * Strength
   *
   * The strength to use for Video to Video.  1.0 completely remakes the video while 0.0 preserves the original.
   */
  strength?: number
  /**
   * Guidance scale (CFG)
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related video to show you.
   *
   */
  guidance_scale?: number
  /**
   * Num Inference Steps
   *
   * The number of inference steps to perform.
   */
  num_inference_steps?: number
  /**
   * Export Fps
   *
   * The target FPS of the video
   */
  export_fps?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to generate video from
   */
  negative_prompt?: string
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number
}

/**
 * ImageSize
 */
export type SchemaImageSize = {
  /**
   * Height
   *
   * The height of the generated image.
   */
  height?: number
  /**
   * Width
   *
   * The width of the generated image.
   */
  width?: number
}

/**
 * LoraWeight
 */
export type SchemaLoraWeight = {
  /**
   * Path
   *
   * URL or the path to the LoRA weights.
   */
  path: string
  /**
   * Scale
   *
   *
   * The scale of the LoRA weight. This is used to scale the LoRA weight
   * before merging it with the base model.
   *
   */
  scale?: number
}

/**
 * Output
 */
export type SchemaVideoUpscalerOutput = {
  /**
   * Video
   *
   * The stitched video
   */
  video: SchemaFile
}

/**
 * Input
 */
export type SchemaVideoUpscalerInput = {
  /**
   * Video Url
   *
   * The URL of the video to upscale
   */
  video_url: string
  /**
   * Scale
   *
   * The scale factor
   */
  scale?: number
}

/**
 * OutputModel
 */
export type SchemaDubbingOutput = {
  /**
   * Video
   *
   * The generated video with the lip sync.
   */
  video: SchemaFile
}

/**
 * InputModel
 */
export type SchemaDubbingInput = {
  /**
   * Do Lipsync
   *
   * Whether to lip sync the audio to the video
   */
  do_lipsync?: boolean
  /**
   * Video Url
   *
   * Input video URL to be dubbed.
   */
  video_url: string
  /**
   * Target Language
   *
   * Target language to dub the video to
   */
  target_language?: 'hindi' | 'turkish' | 'english'
}

/**
 * Output
 */
export type SchemaAutoCaptionOutput = {
  /**
   * Video Url
   *
   * URL to the caption .mp4 video.
   */
  video_url: string
}

/**
 * CaptionInput
 */
export type SchemaAutoCaptionInput = {
  /**
   * Txt Font
   *
   * Font for generated captions. Choose one in 'Arial','Standard','Garamond', 'Times New Roman','Georgia', or pass a url to a .ttf file
   */
  txt_font?: string
  /**
   * Video Url
   *
   * URL to the .mp4 video with audio. Only videos of size <100MB are allowed.
   */
  video_url: string
  /**
   * Top Align
   *
   * Top-to-bottom alignment of the text. Can be a string ('top', 'center', 'bottom') or a float (0.0-1.0)
   */
  top_align?: string | number
  /**
   * Txt Color
   *
   * Colour of the text. Can be a RGB tuple, a color name, or an hexadecimal notation.
   */
  txt_color?: string
  /**
   * Stroke Width
   *
   * Width of the text strokes in pixels
   */
  stroke_width?: number
  /**
   * Refresh Interval
   *
   * Number of seconds the captions should stay on screen. A higher number will also result in more text being displayed at once.
   */
  refresh_interval?: number
  /**
   * Font Size
   *
   * Size of text in generated captions.
   */
  font_size?: number
  /**
   * Left Align
   *
   * Left-to-right alignment of the text. Can be a string ('left', 'center', 'right') or a float (0.0-1.0)
   */
  left_align?: string | number
}

/**
 * LipSyncOutput
 */
export type SchemaSyncLipsyncOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: SchemaFile
}

/**
 * LipSyncInput
 */
export type SchemaSyncLipsyncInput = {
  /**
   * Model
   *
   * The model to use for lipsyncing
   */
  model?: 'lipsync-1.8.0' | 'lipsync-1.7.1' | 'lipsync-1.9.0-beta'
  /**
   * Video Url
   *
   * URL of the input video
   */
  video_url: string
  /**
   * Sync Mode
   *
   * Lipsync mode when audio and video durations are out of sync.
   */
  sync_mode?: 'cut_off' | 'loop' | 'bounce' | 'silence' | 'remap'
  /**
   * Audio Url
   *
   * URL of the input audio
   */
  audio_url: string
}

/**
 * Keyframe
 */
export type SchemaKeyframe = {
  /**
   * Duration
   *
   * The duration in milliseconds of this keyframe
   */
  duration: number
  /**
   * Timestamp
   *
   * The timestamp in milliseconds where this keyframe starts
   */
  timestamp: number
  /**
   * Url
   *
   * The URL where this keyframe's media file can be accessed
   */
  url: string
}

/**
 * Track
 */
export type SchemaTrack = {
  /**
   * Type
   *
   * Type of track ('video' or 'audio')
   */
  type: string
  /**
   * Id
   *
   * Unique identifier for the track
   */
  id: string
  /**
   * Keyframes
   *
   * List of keyframes that make up this track
   */
  keyframes: Array<SchemaKeyframe>
}

/**
 * ComposeOutput
 */
export type SchemaFfmpegApiComposeOutput = {
  /**
   * Video Url
   *
   * URL of the processed video file
   */
  video_url: string
  /**
   * Thumbnail Url
   *
   * URL of the video's thumbnail image
   */
  thumbnail_url: string
}

/**
 * Input
 */
export type SchemaFfmpegApiComposeInput = {
  /**
   * Tracks
   *
   * List of tracks to be combined into the final media
   */
  tracks: Array<SchemaTrack>
}

/**
 * HunyuanV2VResponse
 */
export type SchemaHunyuanVideoLoraVideoToVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generating the video.
   */
  seed: number
  /**
   * Video
   */
  video: SchemaFile
}

/**
 * HunyuanV2VRequest
 */
export type SchemaHunyuanVideoLoraVideoToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * Aspect Ratio (W:H)
   *
   * The aspect ratio of the video to generate.
   */
  aspect_ratio?: '16:9' | '9:16'
  /**
   * Resolution
   *
   * The resolution of the video to generate.
   */
  resolution?: '480p' | '580p' | '720p'
  /**
   * Video Url
   *
   * URL of the video
   */
  video_url: string
  /**
   * Loras
   *
   *
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   *
   */
  loras?: Array<SchemaLoraWeight>
  /**
   * Strength
   *
   * Strength of video-to-video
   */
  strength?: number
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Seed
   *
   * The seed to use for generating the video.
   */
  seed?: number
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: '129' | '85'
  /**
   * Pro Mode
   *
   * By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.
   */
  pro_mode?: boolean
}

/**
 * HunyuanT2VResponse
 */
export type SchemaHunyuanVideoVideoToVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generating the video.
   */
  seed: number
  /**
   * Video
   */
  video: SchemaFile
}

/**
 * HunyuanV2VRequest
 */
export type SchemaHunyuanVideoVideoToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * Aspect Ratio (W:H)
   *
   * The aspect ratio of the video to generate.
   */
  aspect_ratio?: '16:9' | '9:16'
  /**
   * Resolution
   *
   * The resolution of the video to generate.
   */
  resolution?: '480p' | '580p' | '720p'
  /**
   * Video Url
   *
   * URL of the video input.
   */
  video_url: string
  /**
   * Strength
   *
   * Strength for Video-to-Video
   */
  strength?: number
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Num Inference Steps
   *
   * The number of inference steps to run. Lower gets faster results, higher gets better results.
   */
  num_inference_steps?: number
  /**
   * Seed
   *
   * The seed to use for generating the video.
   */
  seed?: number
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: '129' | '85'
  /**
   * Pro Mode
   *
   * By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.
   */
  pro_mode?: boolean
}

/**
 * Ben2OutputVideo
 */
export type SchemaBenV2VideoOutput = {
  /**
   * Seed
   *
   *
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   *
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * Ben2InputVideo
 */
export type SchemaBenV2VideoInput = {
  /**
   * Video Url
   *
   * URL of video to be used for background removal.
   */
  video_url: string
  /**
   * Seed
   *
   * Random seed for reproducible generation.
   */
  seed?: number
  /**
   * Background Color
   *
   * Optional RGB values (0-255) for the background color. If not provided, the background will be transparent. For ex: [0, 0, 0]
   */
  background_color?: [unknown, unknown, unknown]
}

/**
 * VideoUpscaleOutput
 */
export type SchemaTopazUpscaleVideoOutput = {
  /**
   * Video
   *
   * The upscaled video file
   */
  video: SchemaFile
}

/**
 * VideoUpscaleRequest
 */
export type SchemaTopazUpscaleVideoInput = {
  /**
   * H264 Output
   *
   * Whether to use H264 codec for output video. Default is H265.
   */
  H264_output?: boolean
  /**
   * Video Url
   *
   * URL of the video to upscale
   */
  video_url: string
  /**
   * Upscale Factor
   *
   * Factor to upscale the video by (e.g. 2.0 doubles width and height)
   */
  upscale_factor?: number
  /**
   * Target Fps
   *
   * Target FPS for frame interpolation. If set, frame interpolation will be enabled.
   */
  target_fps?: number
}

/**
 * ExtendVideoOutput
 */
export type SchemaLtxVideoV095ExtendOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * ExtendVideoInput
 */
export type SchemaLtxVideoV095ExtendInput = {
  /**
   * Prompt
   *
   * Text prompt to guide generation
   */
  prompt: string
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p).
   */
  resolution?: '480p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video (16:9 or 9:16).
   */
  aspect_ratio?: '9:16' | '16:9'
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using the model's own capabilities.
   */
  expand_prompt?: boolean
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number
  /**
   * Num Inference Steps
   *
   * Number of inference steps
   */
  num_inference_steps?: number
  /**
   * Negative Prompt
   *
   * Negative prompt for generation
   */
  negative_prompt?: string
  /**
   * Video
   *
   * Video to be extended.
   */
  video: SchemaVideoConditioningInput
}

/**
 * VideoConditioningInput
 */
export type SchemaVideoConditioningInput = {
  /**
   * Video Url
   *
   * URL of video to be extended
   */
  video_url: string
  /**
   * Start Frame Num
   *
   * Frame number of the video from which the conditioning starts. Must be a multiple of 8.
   */
  start_frame_num: number
}

/**
 * MulticonditioningVideoOutput
 */
export type SchemaLtxVideoV095MulticonditioningOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * MultiConditioningVideoInput
 */
export type SchemaLtxVideoV095MulticonditioningInput = {
  /**
   * Prompt
   *
   * Text prompt to guide generation
   */
  prompt: string
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p).
   */
  resolution?: '480p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video (16:9 or 9:16).
   */
  aspect_ratio?: '9:16' | '16:9'
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using the model's own capabilities.
   */
  expand_prompt?: boolean
  /**
   * Images
   *
   * URL of images to use as conditioning
   */
  images?: Array<SchemaImageConditioningInput>
  /**
   * Videos
   *
   * Videos to use as conditioning
   */
  videos?: Array<SchemaVideoConditioningInput>
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number
  /**
   * Num Inference Steps
   *
   * Number of inference steps
   */
  num_inference_steps?: number
  /**
   * Negative Prompt
   *
   * Negative prompt for generation
   */
  negative_prompt?: string
}

/**
 * ImageConditioningInput
 */
export type SchemaImageConditioningInput = {
  /**
   * Start Frame Num
   *
   * Frame number of the image from which the conditioning starts. Must be a multiple of 8.
   */
  start_frame_num: number
  /**
   * Image Url
   *
   * URL of image to use as conditioning
   */
  image_url: string
}

/**
 * PikadditionsOutput
 *
 * Output from Pikadditions generation
 */
export type SchemaPikaV2PikadditionsOutput = {
  /**
   * Video
   *
   * The generated video with added objects/images
   */
  video: SchemaFile
}

/**
 * PikadditionsRequest
 *
 * Request model for Pikadditions endpoint
 */
export type SchemaPikaV2PikadditionsInput = {
  /**
   * Prompt
   *
   * Text prompt describing what to add
   */
  prompt?: string
  /**
   * Video Url
   *
   * URL of the input video
   */
  video_url: string
  /**
   * Seed
   *
   * The seed for the random number generator
   */
  seed?: number
  /**
   * Negative Prompt
   *
   * Negative prompt to guide the model
   */
  negative_prompt?: string
  /**
   * Image Url
   *
   * URL of the image to add
   */
  image_url: string
}

/**
 * Output
 */
export type SchemaLatentsyncOutput = {
  /**
   * Video
   *
   * The generated video with the lip sync.
   */
  video: SchemaFile
}

/**
 * Input
 */
export type SchemaLatentsyncInput = {
  /**
   * Video Url
   *
   * The URL of the video to generate the lip sync for.
   */
  video_url: string
  /**
   * Guidance Scale
   *
   * Guidance scale for the model inference
   */
  guidance_scale?: number
  /**
   * Seed
   *
   * Random seed for generation. If None, a random seed will be used.
   */
  seed?: number
  /**
   * Audio Url
   *
   * The URL of the audio to generate the lip sync for.
   */
  audio_url: string
  /**
   * Loop Mode
   *
   * Video loop mode when audio is longer than video. Options: pingpong, loop
   */
  loop_mode?: 'pingpong' | 'loop'
}

/**
 * LipSyncV2Output
 */
export type SchemaSyncLipsyncV2Output = {
  /**
   * Video
   *
   * The generated video
   */
  video: SchemaFile
}

/**
 * LipSyncV2Input
 */
export type SchemaSyncLipsyncV2Input = {
  /**
   * Model
   *
   * The model to use for lipsyncing. `lipsync-2-pro` will cost roughly 1.67 times as much as `lipsync-2` for the same duration.
   */
  model?: 'lipsync-2' | 'lipsync-2-pro'
  /**
   * Video Url
   *
   * URL of the input video
   */
  video_url: string
  /**
   * Sync Mode
   *
   * Lipsync mode when audio and video durations are out of sync.
   */
  sync_mode?: 'cut_off' | 'loop' | 'bounce' | 'silence' | 'remap'
  /**
   * Audio Url
   *
   * URL of the input audio
   */
  audio_url: string
}

/**
 * VideoOutput
 *
 * Pydantic model for returning the re-sounded video back to the client.
 */
export type SchemaVideoSoundEffectsGeneratorOutput = {
  video: SchemaFile
}

/**
 * VideoInput
 *
 * Pydantic model for receiving a video file to analyze and re-sound.
 */
export type SchemaVideoSoundEffectsGeneratorInput = {
  video_url: SchemaVideo
}

/**
 * Video
 *
 * Represents a video file.
 */
export type SchemaVideo = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File | unknown
}

/**
 * WanT2VResponse
 */
export type SchemaWanVaceOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * WanT2VRequest
 */
export type SchemaWanVaceInput = {
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Video Url
   *
   * URL to the source video file. If provided, the model will use this video as a reference.
   */
  video_url?: string
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Ref Image Urls
   *
   * Urls to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>
  /**
   * Task
   *
   * Task type for the model.
   */
  task?: 'depth' | 'inpainting'
  /**
   * Frames Per Second
   *
   * Frames per second of the generated video. Must be between 5 to 24.
   */
  frames_per_second?: number
  /**
   * Mask Image Url
   *
   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored.
   */
  mask_image_url?: string
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Num Frames
   *
   * Number of frames to generate. Must be between 81 to 100 (inclusive). Works only with only reference images as input if source video or mask video is provided output len would be same as source up to 241 frames
   */
  num_frames?: number
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video (16:9 or 9:16).
   */
  aspect_ratio?: 'auto' | '9:16' | '16:9'
  /**
   * Resolution
   *
   * Resolution of the generated video (480p,580p, or 720p).
   */
  resolution?: '480p' | '580p' | '720p'
  /**
   * Mask Video Url
   *
   * URL to the source mask file. If provided, the model will use this mask as a reference.
   */
  mask_video_url?: string
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
}

/**
 * MagiVideoExtensionResponse
 */
export type SchemaMagiDistilledExtendVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * MagiVideoExtensionRequest
 */
export type SchemaMagiDistilledExtendVideoInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.
   */
  resolution?: '480p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
   */
  aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1'
  /**
   * Video Url
   *
   * URL of the input video to represent the beginning of the video. If the input video does not match the chosen aspect ratio, it is resized and center cropped.
   */
  video_url: string
  /**
   * Start Frame
   *
   * The frame to begin the generation from, with the remaining frames will be treated as the prefix video. The final video will contain the frames up until this number unchanged, followed by the generated frames. The default start frame is 32 frames before the end of the video, which gives optimal results.
   */
  start_frame?: number
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: 4 | 8 | 16 | 32
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
  /**
   * Num Frames
   *
   * Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit.
   */
  num_frames?: number
}

/**
 * MagiVideoExtensionResponse
 */
export type SchemaMagiExtendVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * MagiVideoExtensionRequest
 */
export type SchemaMagiExtendVideoInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.
   */
  resolution?: '480p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
   */
  aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1'
  /**
   * Video Url
   *
   * URL of the input video to represent the beginning of the video. If the input video does not match the chosen aspect ratio, it is resized and center cropped.
   */
  video_url: string
  /**
   * Start Frame
   *
   * The frame to begin the generation from, with the remaining frames will be treated as the prefix video. The final video will contain the frames up until this number unchanged, followed by the generated frames. The default start frame is 32 frames before the end of the video, which gives optimal results.
   */
  start_frame?: number
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: 4 | 8 | 16 | 32 | 64
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
  /**
   * Num Frames
   *
   * Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit.
   */
  num_frames?: number
}

/**
 * VideoCondition
 *
 * Video condition to use for generation.
 */
export type SchemaVideoCondition = {
  /**
   * Strength
   *
   * The strength of the condition.
   */
  strength?: number
  /**
   * Start Frame Number
   *
   * The frame number to start the condition on.
   */
  start_frame_number?: number
  /**
   * Video Url
   *
   * The URL of the video to use as input.
   */
  video_url: string
}

/**
 * ImageCondition
 *
 * Image condition to use for generation.
 */
export type SchemaImageCondition = {
  /**
   * Strength
   *
   * The strength of the condition.
   */
  strength?: number
  /**
   * Start Frame Number
   *
   * The frame number to start the condition on.
   */
  start_frame_number?: number
  /**
   * Image Url
   *
   * The URL of the image to use as input.
   */
  image_url: string
}

/**
 * MulticonditioningVideoOutput
 */
export type SchemaLtxVideoLoraMulticonditioningOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video.
   */
  video: SchemaFile
}

/**
 * MulticonditioningVideoInput
 *
 * Request model for text-to-video generation with multiple conditions.
 */
export type SchemaLtxVideoLoraMulticonditioningInput = {
  /**
   * Number Of Steps
   *
   * The number of inference steps to use.
   */
  number_of_steps?: number
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * Reverse Video
   *
   * Whether to reverse the video.
   */
  reverse_video?: boolean
  /**
   * Frame Rate
   *
   * The frame rate of the video.
   */
  frame_rate?: number
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using the LLM.
   */
  expand_prompt?: boolean
  /**
   * Number Of Frames
   *
   * The number of frames in the video.
   */
  number_of_frames?: number
  /**
   * Loras
   *
   * The LoRA weights to use for generation.
   */
  loras?: Array<SchemaLoRaWeight>
  /**
   * Images
   *
   * The image conditions to use for generation.
   */
  images?: Array<SchemaImageCondition>
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * The negative prompt to use.
   */
  negative_prompt?: string
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the video.
   */
  aspect_ratio?: '16:9' | '1:1' | '9:16' | 'auto'
  /**
   * Resolution
   *
   * The resolution of the video.
   */
  resolution?: '480p' | '720p'
  /**
   * Videos
   *
   * The video conditions to use for generation.
   */
  videos?: Array<SchemaVideoCondition>
  /**
   * Seed
   *
   * The seed to use for generation.
   */
  seed?: number
}

/**
 * LoRAWeight
 *
 * LoRA weight to use for generation.
 */
export type SchemaLoRaWeight = {
  /**
   * Path
   *
   * URL or path to the LoRA weights.
   */
  path: string
  /**
   * Scale
   *
   * Scale of the LoRA weight. This is a multiplier applied to the LoRA weight when loading it.
   */
  scale?: number
  /**
   * Weight Name
   *
   * Name of the LoRA weight. Only used if `path` is a HuggingFace repository, and is only required when the repository contains multiple LoRA weights.
   */
  weight_name?: string
}

/**
 * ExtendVideoOutput
 */
export type SchemaLtxVideo13bDevExtendOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * ExtendVideoInput
 */
export type SchemaLtxVideo13bDevExtendInput = {
  /**
   * Second Pass Skip Initial Steps
   *
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
   */
  second_pass_skip_initial_steps?: number
  /**
   * First Pass Num Inference Steps
   *
   * Number of inference steps during the first pass.
   */
  first_pass_num_inference_steps?: number
  /**
   * Frame Rate
   *
   * The frame rate of the video.
   */
  frame_rate?: number
  /**
   * Prompt
   *
   * Text prompt to guide generation
   */
  prompt: string
  /**
   * Reverse Video
   *
   * Whether to reverse the video.
   */
  reverse_video?: boolean
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean
  /**
   * Loras
   *
   * LoRA weights to use for generation
   */
  loras?: Array<SchemaLoRaWeight>
  /**
   * Second Pass Num Inference Steps
   *
   * Number of inference steps during the second pass.
   */
  second_pass_num_inference_steps?: number
  /**
   * Num Frames
   *
   * The number of frames in the video.
   */
  num_frames?: number
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Video
   *
   * Video to be extended.
   */
  video: SchemaVideoConditioningInput
  /**
   * Negative Prompt
   *
   * Negative prompt for generation
   */
  negative_prompt?: string
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p).
   */
  resolution?: '480p' | '720p'
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the video.
   */
  aspect_ratio?: '9:16' | '1:1' | '16:9' | 'auto'
  /**
   * Constant Rate Factor
   *
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
   */
  constant_rate_factor?: number
  /**
   * First Pass Skip Final Steps
   *
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.
   */
  first_pass_skip_final_steps?: number
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number
}

/**
 * MultiConditioningVideoOutput
 */
export type SchemaLtxVideo13bDevMulticonditioningOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * MultiConditioningVideoInput
 */
export type SchemaLtxVideo13bDevMulticonditioningInput = {
  /**
   * Second Pass Skip Initial Steps
   *
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
   */
  second_pass_skip_initial_steps?: number
  /**
   * First Pass Num Inference Steps
   *
   * Number of inference steps during the first pass.
   */
  first_pass_num_inference_steps?: number
  /**
   * Frame Rate
   *
   * The frame rate of the video.
   */
  frame_rate?: number
  /**
   * Prompt
   *
   * Text prompt to guide generation
   */
  prompt: string
  /**
   * Reverse Video
   *
   * Whether to reverse the video.
   */
  reverse_video?: boolean
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean
  /**
   * Loras
   *
   * LoRA weights to use for generation
   */
  loras?: Array<SchemaLoRaWeight>
  /**
   * Images
   *
   * URL of images to use as conditioning
   */
  images?: Array<SchemaImageConditioningInput>
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Num Frames
   *
   * The number of frames in the video.
   */
  num_frames?: number
  /**
   * Second Pass Num Inference Steps
   *
   * Number of inference steps during the second pass.
   */
  second_pass_num_inference_steps?: number
  /**
   * Negative Prompt
   *
   * Negative prompt for generation
   */
  negative_prompt?: string
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p).
   */
  resolution?: '480p' | '720p'
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the video.
   */
  aspect_ratio?: '9:16' | '1:1' | '16:9' | 'auto'
  /**
   * Videos
   *
   * Videos to use as conditioning
   */
  videos?: Array<SchemaVideoConditioningInput>
  /**
   * Constant Rate Factor
   *
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
   */
  constant_rate_factor?: number
  /**
   * First Pass Skip Final Steps
   *
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.
   */
  first_pass_skip_final_steps?: number
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number
}

/**
 * MultiConditioningVideoOutput
 */
export type SchemaLtxVideo13bDistilledMulticonditioningOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * DistilledMultiConditioningVideoInput
 *
 * Distilled model input
 */
export type SchemaLtxVideo13bDistilledMulticonditioningInput = {
  /**
   * Second Pass Skip Initial Steps
   *
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
   */
  second_pass_skip_initial_steps?: number
  /**
   * First Pass Num Inference Steps
   *
   * Number of inference steps during the first pass.
   */
  first_pass_num_inference_steps?: number
  /**
   * Frame Rate
   *
   * The frame rate of the video.
   */
  frame_rate?: number
  /**
   * Reverse Video
   *
   * Whether to reverse the video.
   */
  reverse_video?: boolean
  /**
   * Prompt
   *
   * Text prompt to guide generation
   */
  prompt: string
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean
  /**
   * Loras
   *
   * LoRA weights to use for generation
   */
  loras?: Array<SchemaLoRaWeight>
  /**
   * Images
   *
   * URL of images to use as conditioning
   */
  images?: Array<SchemaImageConditioningInput>
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Num Frames
   *
   * The number of frames in the video.
   */
  num_frames?: number
  /**
   * Second Pass Num Inference Steps
   *
   * Number of inference steps during the second pass.
   */
  second_pass_num_inference_steps?: number
  /**
   * Negative Prompt
   *
   * Negative prompt for generation
   */
  negative_prompt?: string
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p).
   */
  resolution?: '480p' | '720p'
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the video.
   */
  aspect_ratio?: '9:16' | '1:1' | '16:9' | 'auto'
  /**
   * Constant Rate Factor
   *
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
   */
  constant_rate_factor?: number
  /**
   * Videos
   *
   * Videos to use as conditioning
   */
  videos?: Array<SchemaVideoConditioningInput>
  /**
   * First Pass Skip Final Steps
   *
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.
   */
  first_pass_skip_final_steps?: number
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number
}

/**
 * ExtendVideoOutput
 */
export type SchemaLtxVideo13bDistilledExtendOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * DistilledExtendVideoInput
 *
 * Distilled model input
 */
export type SchemaLtxVideo13bDistilledExtendInput = {
  /**
   * Second Pass Skip Initial Steps
   *
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
   */
  second_pass_skip_initial_steps?: number
  /**
   * First Pass Num Inference Steps
   *
   * Number of inference steps during the first pass.
   */
  first_pass_num_inference_steps?: number
  /**
   * Frame Rate
   *
   * The frame rate of the video.
   */
  frame_rate?: number
  /**
   * Reverse Video
   *
   * Whether to reverse the video.
   */
  reverse_video?: boolean
  /**
   * Prompt
   *
   * Text prompt to guide generation
   */
  prompt: string
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean
  /**
   * Loras
   *
   * LoRA weights to use for generation
   */
  loras?: Array<SchemaLoRaWeight>
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Num Frames
   *
   * The number of frames in the video.
   */
  num_frames?: number
  /**
   * Second Pass Num Inference Steps
   *
   * Number of inference steps during the second pass.
   */
  second_pass_num_inference_steps?: number
  /**
   * Video
   *
   * Video to be extended.
   */
  video: SchemaVideoConditioningInput
  /**
   * Negative Prompt
   *
   * Negative prompt for generation
   */
  negative_prompt?: string
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p).
   */
  resolution?: '480p' | '720p'
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the video.
   */
  aspect_ratio?: '9:16' | '1:1' | '16:9' | 'auto'
  /**
   * Constant Rate Factor
   *
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
   */
  constant_rate_factor?: number
  /**
   * First Pass Skip Final Steps
   *
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.
   */
  first_pass_skip_final_steps?: number
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number
}

/**
 * WanVACEResponse
 */
export type SchemaWanVace14bOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: SchemaFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * VideoFile
 */
export type SchemaVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number | unknown
  /**
   * Height
   *
   * The height of the video
   */
  height?: number | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Width
   *
   * The width of the video
   */
  width?: number | unknown
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
}

/**
 * WanVACERequest
 */
export type SchemaWanVace14bInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL to the source video file. If provided, the model will use this video as a reference.
   */
  video_url?: string | unknown
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown
  /**
   * Reference Image URLs
   *
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Mask Video URL
   *
   * URL to the source mask file. If provided, the model will use this mask as a reference.
   */
  mask_video_url?: string | unknown
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Mask Image URL
   *
   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored.
   */
  mask_image_url?: string | unknown
  /**
   * Task
   *
   * Task type for the model.
   */
  task?: 'depth' | 'pose' | 'inpainting' | 'outpainting' | 'reframe'
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown
}

/**
 * LipsyncAppOutput
 */
export type SchemaLipsyncOutput = {
  video: SchemaFile
}

/**
 * LipsyncInput
 */
export type SchemaLipsyncInput = {
  /**
   * Video Url
   */
  video_url: string
  /**
   * Audio Url
   */
  audio_url: string
}

/**
 * ReframeOutput
 */
export type SchemaLumaDreamMachineRay2ReframeOutput = {
  /**
   * Video
   *
   * URL of the reframed video
   */
  video: SchemaFile
}

/**
 * ReframeVideoRequest
 */
export type SchemaLumaDreamMachineRay2ReframeInput = {
  /**
   * Prompt
   *
   * Optional prompt for reframing
   */
  prompt?: string
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the reframed video
   */
  aspect_ratio: '1:1' | '16:9' | '9:16' | '4:3' | '3:4' | '21:9' | '9:21'
  /**
   * Y Start
   *
   * Start Y coordinate for reframing
   */
  y_start?: number
  /**
   * X End
   *
   * End X coordinate for reframing
   */
  x_end?: number
  /**
   * Video Url
   *
   * URL of the input video to reframe
   */
  video_url: string
  /**
   * Y End
   *
   * End Y coordinate for reframing
   */
  y_end?: number
  /**
   * X Start
   *
   * Start X coordinate for reframing
   */
  x_start?: number
  /**
   * Grid Position Y
   *
   * Y position of the grid for reframing
   */
  grid_position_y?: number
  /**
   * Grid Position X
   *
   * X position of the grid for reframing
   */
  grid_position_x?: number
  /**
   * Image Url
   *
   * Optional URL of the first frame image for reframing
   */
  image_url?: string
}

/**
 * ReframeOutput
 */
export type SchemaLumaDreamMachineRay2FlashReframeOutput = {
  /**
   * Video
   *
   * URL of the reframed video
   */
  video: SchemaFile
}

/**
 * ReframeVideoRequest
 */
export type SchemaLumaDreamMachineRay2FlashReframeInput = {
  /**
   * Prompt
   *
   * Optional prompt for reframing
   */
  prompt?: string
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the reframed video
   */
  aspect_ratio: '1:1' | '16:9' | '9:16' | '4:3' | '3:4' | '21:9' | '9:21'
  /**
   * Y Start
   *
   * Start Y coordinate for reframing
   */
  y_start?: number
  /**
   * X End
   *
   * End X coordinate for reframing
   */
  x_end?: number
  /**
   * Video Url
   *
   * URL of the input video to reframe
   */
  video_url: string
  /**
   * Y End
   *
   * End Y coordinate for reframing
   */
  y_end?: number
  /**
   * X Start
   *
   * Start X coordinate for reframing
   */
  x_start?: number
  /**
   * Grid Position Y
   *
   * Y position of the grid for reframing
   */
  grid_position_y?: number
  /**
   * Grid Position X
   *
   * X position of the grid for reframing
   */
  grid_position_x?: number
  /**
   * Image Url
   *
   * Optional URL of the first frame image for reframing
   */
  image_url?: string
}

/**
 * WanT2VResponse
 */
export type SchemaWanVace13bOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * WanT2VRequest
 */
export type SchemaWanVace13bInput = {
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Video Url
   *
   * URL to the source video file. If provided, the model will use this video as a reference.
   */
  video_url?: string
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Mask Image Url
   *
   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored.
   */
  mask_image_url?: string
  /**
   * Task
   *
   * Task type for the model.
   */
  task?: 'depth' | 'inpainting' | 'pose'
  /**
   * Frames Per Second
   *
   * Frames per second of the generated video. Must be between 5 to 24.
   */
  frames_per_second?: number
  /**
   * Ref Image Urls
   *
   * Urls to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Num Frames
   *
   * Number of frames to generate. Must be between 81 to 100 (inclusive). Works only with only reference images as input if source video or mask video is provided output len would be same as source up to 241 frames
   */
  num_frames?: number
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Resolution
   *
   * Resolution of the generated video (480p,580p, or 720p).
   */
  resolution?: '480p' | '580p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video (16:9 or 9:16).
   */
  aspect_ratio?: 'auto' | '9:16' | '16:9'
  /**
   * Mask Video Url
   *
   * URL to the source mask file. If provided, the model will use this mask as a reference.
   */
  mask_video_url?: string
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
}

/**
 * CombineOutput
 */
export type SchemaFfmpegApiMergeAudioVideoOutput = {
  video: SchemaFile
}

/**
 * CombineInput
 */
export type SchemaFfmpegApiMergeAudioVideoInput = {
  /**
   * Video Url
   *
   * URL of the video file to use as the video track
   */
  video_url: string
  /**
   * Start Offset
   *
   * Offset in seconds for when the audio should start relative to the video
   */
  start_offset?: number
  /**
   * Audio Url
   *
   * URL of the audio file to use as the audio track
   */
  audio_url: string
}

/**
 * DWPoseVideoOutput
 */
export type SchemaDwposeVideoOutput = {
  /**
   * Video
   *
   * The output video with pose estimation.
   */
  video: SchemaFile
}

/**
 * DWPoseVideoInput
 */
export type SchemaDwposeVideoInput = {
  /**
   * Video Url
   *
   * URL of video to be used for pose estimation
   */
  video_url: string
  /**
   * Draw Mode
   *
   * Mode of drawing the pose on the video. Options are: 'full-pose', 'body-pose', 'face-pose', 'hand-pose', 'face-hand-mask', 'face-mask', 'hand-mask'.
   */
  draw_mode?:
    | 'full-pose'
    | 'body-pose'
    | 'face-pose'
    | 'hand-pose'
    | 'face-hand-mask'
    | 'face-mask'
    | 'hand-mask'
}

/**
 * WanVACEDepthResponse
 */
export type SchemaWanVace14bDepthOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: SchemaFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * WanVACEDepthRequest
 */
export type SchemaWanVace14bDepthInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL to the source video file. Required for depth task.
   */
  video_url: string
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown
  /**
   * Reference Image URLs
   *
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown
}

/**
 * WanVACEPoseResponse
 */
export type SchemaWanVace14bPoseOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: SchemaFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * WanVACEPoseRequest
 */
export type SchemaWanVace14bPoseInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation. For pose task, the prompt should describe the desired pose and action of the subject in the video.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL to the source video file. Required for pose task.
   */
  video_url: string
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown
  /**
   * Reference Image URLs
   *
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown
}

/**
 * WanVACEInpaintingResponse
 */
export type SchemaWanVace14bInpaintingOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: SchemaFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * WanVACEInpaintingRequest
 */
export type SchemaWanVace14bInpaintingInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL to the source video file. Required for inpainting.
   */
  video_url: string
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown
  /**
   * Reference Image URLs
   *
   * Urls to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Mask Video URL
   *
   * URL to the source mask file. Required for inpainting.
   */
  mask_video_url: string | unknown
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Mask Image URL
   *
   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video using salient mask tracking. Will be ignored if mask_video_url is provided.
   */
  mask_image_url?: string | unknown
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown
}

/**
 * WanVACEOutpaintingResponse
 */
export type SchemaWanVace14bOutpaintingOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: SchemaFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * WanVACEOutpaintingRequest
 */
export type SchemaWanVace14bOutpaintingInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL to the source video file. Required for outpainting.
   */
  video_url: string
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown
  /**
   * Reference Image URLs
   *
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>
  /**
   * Expand Ratio
   *
   * Amount of expansion. This is a float value between 0 and 1, where 0.25 adds 25% to the original video size on the specified sides.
   */
  expand_ratio?: number
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number
  /**
   * Expand Bottom
   *
   * Whether to expand the video to the bottom.
   */
  expand_bottom?: boolean
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
  /**
   * Expand Top
   *
   * Whether to expand the video to the top.
   */
  expand_top?: boolean
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Expand Left
   *
   * Whether to expand the video to the left.
   */
  expand_left?: boolean
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Expand Right
   *
   * Whether to expand the video to the right.
   */
  expand_right?: boolean
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown
}

/**
 * WanVACEReframeResponse
 */
export type SchemaWanVace14bReframeOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: SchemaFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * WanVACEReframeRequest
 */
export type SchemaWanVace14bReframeInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation. Optional for reframing.
   */
  prompt?: string
  /**
   * Video URL
   *
   * URL to the source video file. This video will be used as a reference for the reframe task.
   */
  video_url: string
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Trim Borders
   *
   * Whether to trim borders from the video.
   */
  trim_borders?: boolean
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Zoom Factor
   *
   * Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom.
   */
  zoom_factor?: number
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown
}

/**
 * ModifyOutput
 */
export type SchemaLumaDreamMachineRay2ModifyOutput = {
  /**
   * Video
   *
   * URL of the modified video
   */
  video: SchemaFile
}

/**
 * ModifyVideoRequest
 */
export type SchemaLumaDreamMachineRay2ModifyInput = {
  /**
   * Prompt
   *
   * Instruction for modifying the video
   */
  prompt?: string
  /**
   * Video Url
   *
   * URL of the input video to modify
   */
  video_url: string
  /**
   * Mode
   *
   * Amount of modification to apply to the video, adhere_1 is the least amount of modification, reimagine_3 is the most
   */
  mode?:
    | 'adhere_1'
    | 'adhere_2'
    | 'adhere_3'
    | 'flex_1'
    | 'flex_2'
    | 'flex_3'
    | 'reimagine_1'
    | 'reimagine_2'
    | 'reimagine_3'
  /**
   * Image Url
   *
   * Optional URL of the first frame image for modification
   */
  image_url?: string
}

/**
 * LipsyncOutput
 */
export type SchemaPixverseLipsyncOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: SchemaFile
}

/**
 * LipsyncRequest
 */
export type SchemaPixverseLipsyncInput = {
  /**
   * Text
   *
   * Text content for TTS when audio_url is not provided
   */
  text?: string
  /**
   * Video Url
   *
   * URL of the input video
   */
  video_url: string
  /**
   * Audio Url
   *
   * URL of the input audio. If not provided, TTS will be used.
   */
  audio_url?: string
  /**
   * Voice Id
   *
   * Voice to use for TTS when audio_url is not provided
   */
  voice_id?:
    | 'Emily'
    | 'James'
    | 'Isabella'
    | 'Liam'
    | 'Chloe'
    | 'Adrian'
    | 'Harper'
    | 'Ava'
    | 'Sophia'
    | 'Julia'
    | 'Mason'
    | 'Jack'
    | 'Oliver'
    | 'Ethan'
    | 'Auto'
}

/**
 * ExtendOutput
 */
export type SchemaPixverseExtendOutput = {
  /**
   * Video
   *
   * The extended video
   */
  video: SchemaFile
}

/**
 * ExtendRequest
 */
export type SchemaPixverseExtendInput = {
  /**
   * Prompt
   *
   * Prompt describing how to extend the video
   */
  prompt: string
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: '360p' | '540p' | '720p' | '1080p'
  /**
   * Duration
   *
   * The duration of the generated video in seconds. 1080p videos are limited to 5 seconds
   */
  duration?: '5' | '8'
  /**
   * Style
   *
   * The style of the extended video
   */
  style?: 'anime' | '3d_animation' | 'clay' | 'comic' | 'cyberpunk'
  /**
   * Video Url
   *
   * URL of the input video to extend
   */
  video_url: string
  /**
   * Model
   *
   * The model version to use for generation
   */
  model?: 'v3.5' | 'v4' | 'v4.5' | 'v5' | 'v5.5' | 'v5.6'
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string
}

/**
 * ExtendOutput
 */
export type SchemaPixverseExtendFastOutput = {
  /**
   * Video
   *
   * The extended video
   */
  video: SchemaFile
}

/**
 * FastExtendRequest
 */
export type SchemaPixverseExtendFastInput = {
  /**
   * Prompt
   *
   * Prompt describing how to extend the video
   */
  prompt: string
  /**
   * Resolution
   *
   * The resolution of the generated video. Fast mode doesn't support 1080p
   */
  resolution?: '360p' | '540p' | '720p'
  /**
   * Video Url
   *
   * URL of the input video to extend
   */
  video_url: string
  /**
   * Style
   *
   * The style of the extended video
   */
  style?: 'anime' | '3d_animation' | 'clay' | 'comic' | 'cyberpunk'
  /**
   * Model
   *
   * The model version to use for generation
   */
  model?: 'v3.5' | 'v4' | 'v4.5' | 'v5' | 'v5.5' | 'v5.6'
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string
}

/**
 * Output
 */
export type SchemaThinksoundOutput = {
  /**
   * Prompt
   *
   * The prompt used to generate the audio.
   */
  prompt: string
  /**
   * Video
   *
   * The generated video with audio.
   */
  video: SchemaFile
}

/**
 * Input
 */
export type SchemaThinksoundInput = {
  /**
   * Prompt
   *
   * A prompt to guide the audio generation. If not provided, it will be extracted from the video.
   */
  prompt?: string
  /**
   * Video Url
   *
   * The URL of the video to generate the audio for.
   */
  video_url: string
  /**
   * Seed
   *
   * The seed for the random number generator
   */
  seed?: number
  /**
   * Number of Inference Steps
   *
   * The number of inference steps for audio generation.
   */
  num_inference_steps?: number
  /**
   * CFG Scale
   *
   * The classifier-free guidance scale for audio generation.
   */
  cfg_scale?: number
}

/**
 * AudioOutput
 */
export type SchemaThinksoundAudioOutput = {
  /**
   * Prompt
   *
   * The prompt used to generate the audio.
   */
  prompt: string
  /**
   * Audio
   *
   * The generated audio file.
   */
  audio: SchemaFile
}

/**
 * Input
 */
export type SchemaThinksoundAudioInput = {
  /**
   * Prompt
   *
   * A prompt to guide the audio generation. If not provided, it will be extracted from the video.
   */
  prompt?: string
  /**
   * Video Url
   *
   * The URL of the video to generate the audio for.
   */
  video_url: string
  /**
   * Seed
   *
   * The seed for the random number generator
   */
  seed?: number
  /**
   * Number of Inference Steps
   *
   * The number of inference steps for audio generation.
   */
  num_inference_steps?: number
  /**
   * CFG Scale
   *
   * The classifier-free guidance scale for audio generation.
   */
  cfg_scale?: number
}

/**
 * SoundEffectOutput
 */
export type SchemaPixverseSoundEffectsOutput = {
  /**
   * Video
   *
   * The video with added sound effects
   */
  video: SchemaFile
}

/**
 * SoundEffectRequest
 */
export type SchemaPixverseSoundEffectsInput = {
  /**
   * Prompt
   *
   * Description of the sound effect to generate. If empty, a random sound effect will be generated
   */
  prompt?: string
  /**
   * Video Url
   *
   * URL of the input video to add sound effects to
   */
  video_url: string
  /**
   * Original Sound Switch
   *
   * Whether to keep the original audio from the video
   */
  original_sound_switch?: boolean
}

/**
 * MultiConditioningVideoOutput
 */
export type SchemaLtxv13B098DistilledMulticonditioningOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * DistilledMultiConditioningVideoInput
 *
 * Distilled model input
 */
export type SchemaLtxv13B098DistilledMulticonditioningInput = {
  /**
   * Second Pass Skip Initial Steps
   *
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
   */
  second_pass_skip_initial_steps?: number
  /**
   * Number of Inference Steps
   *
   * Number of inference steps during the first pass.
   */
  first_pass_num_inference_steps?: number
  /**
   * Frame Rate
   *
   * The frame rate of the video.
   */
  frame_rate?: number
  /**
   * Reverse Video
   *
   * Whether to reverse the video.
   */
  reverse_video?: boolean
  /**
   * Prompt
   *
   * Text prompt to guide generation
   */
  prompt: string
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean
  /**
   * Temporal AdaIN Factor
   *
   * The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution.
   */
  temporal_adain_factor?: number
  /**
   * Loras
   *
   * LoRA weights to use for generation
   */
  loras?: Array<SchemaLoRaWeight>
  /**
   * Images
   *
   * URL of images to use as conditioning
   */
  images?: Array<SchemaImageConditioningInput>
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Number of Frames
   *
   * The number of frames in the video.
   */
  num_frames?: number
  /**
   * Second Pass Number of Inference Steps
   *
   * Number of inference steps during the second pass.
   */
  second_pass_num_inference_steps?: number
  /**
   * Negative Prompt
   *
   * Negative prompt for generation
   */
  negative_prompt?: string
  /**
   * Enable Detail Pass
   *
   * Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.
   */
  enable_detail_pass?: boolean
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: '480p' | '720p'
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the video.
   */
  aspect_ratio?: '9:16' | '1:1' | '16:9' | 'auto'
  /**
   * Tone Map Compression Ratio
   *
   * The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.
   */
  tone_map_compression_ratio?: number
  /**
   * Videos
   *
   * Videos to use as conditioning
   */
  videos?: Array<SchemaVideoConditioningInput>
  /**
   * Constant Rate Factor
   *
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
   */
  constant_rate_factor?: number
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number
}

/**
 * ModifyOutput
 */
export type SchemaLumaDreamMachineRay2FlashModifyOutput = {
  /**
   * Video
   *
   * URL of the modified video
   */
  video: SchemaFile
}

/**
 * ModifyVideoRequest
 */
export type SchemaLumaDreamMachineRay2FlashModifyInput = {
  /**
   * Prompt
   *
   * Instruction for modifying the video
   */
  prompt?: string
  /**
   * Video Url
   *
   * URL of the input video to modify
   */
  video_url: string
  /**
   * Mode
   *
   * Amount of modification to apply to the video, adhere_1 is the least amount of modification, reimagine_3 is the most
   */
  mode?:
    | 'adhere_1'
    | 'adhere_2'
    | 'adhere_3'
    | 'flex_1'
    | 'flex_2'
    | 'flex_3'
    | 'reimagine_1'
    | 'reimagine_2'
    | 'reimagine_3'
  /**
   * Image Url
   *
   * Optional URL of the first frame image for modification
   */
  image_url?: string
}

/**
 * FILMVideoOutput
 */
export type SchemaFilmVideoOutput = {
  /**
   * Video
   *
   * The generated video file with interpolated frames.
   */
  video: SchemaVideoFile
}

/**
 * FILMVideoInput
 */
export type SchemaFilmVideoInput = {
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Only applicable if output_type is 'video'.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Video URL
   *
   * The URL of the video to use for interpolation.
   */
  video_url: string
  /**
   * Use Calculated FPS
   *
   * If True, the function will use the calculated FPS of the input video multiplied by the number of frames to determine the output FPS. If False, the passed FPS will be used.
   */
  use_calculated_fps?: boolean
  /**
   * Loop
   *
   * If True, the final frame will be looped back to the first frame to create a seamless loop. If False, the final frame will not loop back.
   */
  loop?: boolean
  /**
   * Frames Per Second
   *
   * Frames per second for the output video. Only applicable if use_calculated_fps is False.
   */
  fps?: number
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Video Quality
   *
   * The quality of the output video. Only applicable if output_type is 'video'.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Use Scene Detection
   *
   * If True, the input video will be split into scenes before interpolation. This removes smear frames between scenes, but can result in false positives if the scene detection is not accurate. If False, the entire video will be treated as a single scene.
   */
  use_scene_detection?: boolean
  /**
   * Number of Frames
   *
   * The number of frames to generate between the input video frames.
   */
  num_frames?: number
}

/**
 * RIFEVideoOutput
 */
export type SchemaRifeVideoOutput = {
  /**
   * Video
   *
   * The generated video file with interpolated frames.
   */
  video: SchemaFile
}

/**
 * RIFEVideoInput
 */
export type SchemaRifeVideoInput = {
  /**
   * Video URL
   *
   * The URL of the video to use for interpolation.
   */
  video_url: string
  /**
   * Use Scene Detection
   *
   * If True, the input video will be split into scenes before interpolation. This removes smear frames between scenes, but can result in false positives if the scene detection is not accurate. If False, the entire video will be treated as a single scene.
   */
  use_scene_detection?: boolean
  /**
   * Loop
   *
   * If True, the final frame will be looped back to the first frame to create a seamless loop. If False, the final frame will not loop back.
   */
  loop?: boolean
  /**
   * Number of Frames
   *
   * The number of frames to generate between the input video frames.
   */
  num_frames?: number
  /**
   * Use Calculated FPS
   *
   * If True, the function will use the calculated FPS of the input video multiplied by the number of frames to determine the output FPS. If False, the passed FPS will be used.
   */
  use_calculated_fps?: boolean
  /**
   * Frames Per Second
   *
   * Frames per second for the output video. Only applicable if use_calculated_fps is False.
   */
  fps?: number
}

/**
 * ExtendVideoConditioningInput
 */
export type SchemaExtendVideoConditioningInput = {
  /**
   * Video URL
   *
   * URL of video to use as conditioning
   */
  video_url: string
  /**
   * Start Frame Number
   *
   * Frame number of the video from which the conditioning starts. Must be a multiple of 8.
   */
  start_frame_num?: number
  /**
   * Reverse Video
   *
   * Whether to reverse the video. This is useful for tasks where the video conditioning should be applied in reverse order.
   */
  reverse_video?: boolean
  /**
   * Limit Number of Frames
   *
   * Whether to limit the number of frames used from the video. If True, the `max_num_frames` parameter will be used to limit the number of frames.
   */
  limit_num_frames?: boolean
  /**
   * Resample FPS
   *
   * Whether to resample the video to a specific FPS. If True, the `target_fps` parameter will be used to resample the video.
   */
  resample_fps?: boolean
  /**
   * Strength
   *
   * Strength of the conditioning. 0.0 means no conditioning, 1.0 means full conditioning.
   */
  strength?: number
  /**
   * Target FPS
   *
   * Target FPS to resample the video to. Only relevant if `resample_fps` is True.
   */
  target_fps?: number
  /**
   * Maximum Number of Frames
   *
   * Maximum number of frames to use from the video. If None, all frames will be used.
   */
  max_num_frames?: number
}

/**
 * ExtendVideoOutput
 */
export type SchemaLtxv13B098DistilledExtendOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * DistilledExtendVideoInput
 *
 * Distilled model input
 */
export type SchemaLtxv13B098DistilledExtendInput = {
  /**
   * Second Pass Skip Initial Steps
   *
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
   */
  second_pass_skip_initial_steps?: number
  /**
   * Number of Inference Steps
   *
   * Number of inference steps during the first pass.
   */
  first_pass_num_inference_steps?: number
  /**
   * Frame Rate
   *
   * The frame rate of the video.
   */
  frame_rate?: number
  /**
   * Reverse Video
   *
   * Whether to reverse the video.
   */
  reverse_video?: boolean
  /**
   * Prompt
   *
   * Text prompt to guide generation
   */
  prompt: string
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean
  /**
   * Temporal AdaIN Factor
   *
   * The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution.
   */
  temporal_adain_factor?: number
  /**
   * Loras
   *
   * LoRA weights to use for generation
   */
  loras?: Array<SchemaLoRaWeight>
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Number of Frames
   *
   * The number of frames in the video.
   */
  num_frames?: number
  /**
   * Second Pass Number of Inference Steps
   *
   * Number of inference steps during the second pass.
   */
  second_pass_num_inference_steps?: number
  /**
   * Negative Prompt
   *
   * Negative prompt for generation
   */
  negative_prompt?: string
  /**
   * Video
   *
   * Video to be extended.
   */
  video: SchemaExtendVideoConditioningInput
  /**
   * Enable Detail Pass
   *
   * Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.
   */
  enable_detail_pass?: boolean
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: '480p' | '720p'
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the video.
   */
  aspect_ratio?: '9:16' | '1:1' | '16:9' | 'auto'
  /**
   * Tone Map Compression Ratio
   *
   * The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.
   */
  tone_map_compression_ratio?: number
  /**
   * Constant Rate Factor
   *
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
   */
  constant_rate_factor?: number
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number
}

/**
 * WanV2VResponse
 */
export type SchemaWanV22A14bVideoToVideoOutput = {
  /**
   * Prompt
   *
   * The text prompt used for video generation.
   */
  prompt?: string
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * WanV2VRequest
 */
export type SchemaWanV22A14bVideoToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL of the input video.
   */
  video_url: string
  /**
   * Acceleration
   *
   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.
   */
  acceleration?: 'none' | 'regular'
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
   */
  num_interpolated_frames?: number
  /**
   * Shift
   *
   * Shift value for the video. Must be between 1.0 and 10.0.
   */
  shift?: number
  /**
   * Resample Video Frame Rate
   *
   * If true, the video will be resampled to the passed frames per second. If false, the video will not be resampled.
   */
  resample_fps?: boolean
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.
   */
  frames_per_second?: number
  /**
   * Enable Safety Checker
   *
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 17 to 161 (inclusive).
   */
  num_frames?: number
  /**
   * Guidance Scale (1st Stage)
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guidance_scale?: number
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Resolution
   *
   * Resolution of the generated video (480p, 580p, or 720p).
   */
  resolution?: '480p' | '580p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input video.
   */
  aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1'
  /**
   * Enable Output Safety Checker
   *
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean
  /**
   * Guidance Scale (2nd Stage)
   *
   * Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model.
   */
  guidance_scale_2?: number
  /**
   * Video Quality
   *
   * The quality of the output video. Higher quality means better visual quality but larger file size.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Strength
   *
   * Strength of the video transformation. A value of 1.0 means the output will be completely based on the prompt, while a value of 0.0 means the output will be identical to the input video.
   */
  strength?: number
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. If None, no interpolation is applied.
   */
  interpolator_model?: 'none' | 'film' | 'rife'
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
  /**
   * Adjust FPS for Interpolation
   *
   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.
   */
  adjust_fps_for_interpolation?: boolean
}

/**
 * MergeVideosOutput
 */
export type SchemaFfmpegApiMergeVideosOutput = {
  /**
   * Metadata
   *
   * Metadata about the merged video including original video info
   */
  metadata: {
    [key: string]: unknown
  }
  video: SchemaFile
}

/**
 * MergeVideosInput
 */
export type SchemaFfmpegApiMergeVideosInput = {
  /**
   * Target Fps
   *
   * Target FPS for the output video. If not provided, uses the lowest FPS from input videos.
   */
  target_fps?: number | unknown
  /**
   * Video Urls
   *
   * List of video URLs to merge in order
   */
  video_urls: Array<string>
  /**
   * Resolution
   *
   * Resolution of the final video. Width and height must be between 512 and 2048.
   */
  resolution?:
    | SchemaImageSize
    | 'square_hd'
    | 'square'
    | 'portrait_4_3'
    | 'portrait_16_9'
    | 'landscape_4_3'
    | 'landscape_16_9'
    | unknown
}

/**
 * MareyOutput
 */
export type SchemaMareyMotionTransferOutput = {
  video: SchemaFile
}

/**
 * MareyInputMotionTransfer
 */
export type SchemaMareyMotionTransferInput = {
  /**
   * Prompt
   *
   * The prompt to generate a video from
   */
  prompt: string
  /**
   * Video Url
   *
   * The URL of the video to use as the control video.
   */
  video_url: string
  /**
   * Seed
   *
   * Seed for random number generation. Use -1 for random seed each run.
   */
  seed?: number | unknown
  /**
   * Reference Image Url
   *
   * Optional reference image URL to use for pose control or as a starting frame
   */
  reference_image_url?: string | unknown
  /**
   * Negative Prompt
   *
   * Negative prompt used to guide the model away from undesirable features.
   */
  negative_prompt?: string | unknown
  /**
   * First Frame Image Url
   *
   * Optional first frame image URL to use as the first frame of the generated video
   */
  first_frame_image_url?: string | unknown
}

/**
 * MareyOutput
 */
export type SchemaMareyPoseTransferOutput = {
  video: SchemaFile
}

/**
 * MareyInputPoseTransfer
 */
export type SchemaMareyPoseTransferInput = {
  /**
   * Prompt
   *
   * The prompt to generate a video from
   */
  prompt: string
  /**
   * Video Url
   *
   * The URL of the video to use as the control video.
   */
  video_url: string
  /**
   * Seed
   *
   * Seed for random number generation. Use -1 for random seed each run.
   */
  seed?: number | unknown
  /**
   * Reference Image Url
   *
   * Optional reference image URL to use for pose control or as a starting frame
   */
  reference_image_url?: string | unknown
  /**
   * Negative Prompt
   *
   * Negative prompt used to guide the model away from undesirable features.
   */
  negative_prompt?: string | unknown
  /**
   * First Frame Image Url
   *
   * Optional first frame image URL to use as the first frame of the generated video
   */
  first_frame_image_url?: string | unknown
}

/**
 * VideoOutput
 */
export type SchemaSfxV1VideoToVideoOutput = {
  /**
   * Video
   *
   * The processed video with sound effects
   */
  video: Array<SchemaVideo>
}

/**
 * Input
 */
export type SchemaSfxV1VideoToVideoInput = {
  /**
   * Num Samples
   *
   * The number of samples to generate from the model
   */
  num_samples?: number | unknown
  /**
   * Video Url
   *
   * A video url that can accessed from the API to process and add sound effects
   */
  video_url: string
  /**
   * Duration
   *
   * The duration of the generated audio in seconds
   */
  duration?: number | unknown
  /**
   * Seed
   *
   * The seed to use for the generation. If not provided, a random seed will be used
   */
  seed?: number | unknown
  /**
   * Text Prompt
   *
   * Additional description to guide the model
   */
  text_prompt?: string | unknown
}

/**
 * AvatarSingleAudioResponse
 */
export type SchemaInfinitalkOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * InfiniTalkSingleAudioRequest
 */
export type SchemaInfinitalkInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Resolution
   *
   * Resolution of the video to generate. Must be either 480p or 720p.
   */
  resolution?: '480p' | '720p'
  /**
   * Acceleration
   *
   * The acceleration level to use for generation.
   */
  acceleration?: 'none' | 'regular' | 'high'
  /**
   * Image URL
   *
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string
  /**
   * Audio URL
   *
   * The URL of the audio file.
   */
  audio_url: string
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 41 to 721.
   */
  num_frames?: number
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
}

/**
 * OutputIncreaseResolutionModel
 */
export type SchemaVideoIncreaseResolutionOutput = {
  /**
   * Video
   *
   * Video with removed background and audio.
   */
  video: SchemaVideo | SchemaFile
}

/**
 * InputIncreaseResolutionModel
 */
export type SchemaVideoIncreaseResolutionInput = {
  /**
   * Video Url
   *
   * Input video to increase resolution. Size should be less than 14142x14142 and duration less than 30s.
   */
  video_url: string
  /**
   * Output Container And Codec
   *
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, mov_h265, mov_proresks, mkv_h265, mkv_h264, mkv_vp9, gif.
   */
  output_container_and_codec?:
    | 'mp4_h265'
    | 'mp4_h264'
    | 'webm_vp9'
    | 'mov_h265'
    | 'mov_proresks'
    | 'mkv_h265'
    | 'mkv_h264'
    | 'mkv_vp9'
    | 'gif'
  /**
   * Desired Increase
   *
   * desired_increase factor. Options: 2x, 4x.
   */
  desired_increase?: '2' | '4'
}

/**
 * WanFunControlResponse
 */
export type SchemaWanFunControlOutput = {
  /**
   * Video
   *
   * The video generated by the model.
   */
  video: SchemaFile
}

/**
 * WanFunControlRequest
 */
export type SchemaWanFunControlInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video.
   */
  prompt: string
  /**
   * Shift
   *
   * The shift for the scheduler.
   */
  shift?: number
  /**
   * Preprocess Video
   *
   * Whether to preprocess the video. If True, the video will be preprocessed to depth or pose.
   */
  preprocess_video?: boolean
  /**
   * Reference Image URL
   *
   * The URL of the reference image to use as a reference for the video generation.
   */
  reference_image_url?: string
  /**
   * FPS
   *
   * The fps to generate. Only used when match_input_fps is False.
   */
  fps?: number
  /**
   * Match Input Number of Frames
   *
   * Whether to match the number of frames in the input video.
   */
  match_input_num_frames?: boolean
  /**
   * Guidance Scale
   *
   * The guidance scale.
   */
  guidance_scale?: number
  /**
   * Preprocess Type
   *
   * The type of preprocess to apply to the video. Only used when preprocess_video is True.
   */
  preprocess_type?: 'depth' | 'pose'
  /**
   * Control Video URL
   *
   * The URL of the control video to use as a reference for the video generation.
   */
  control_video_url: string
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video.
   */
  negative_prompt?: string
  /**
   * Number of Frames
   *
   * The number of frames to generate. Only used when match_input_num_frames is False.
   */
  num_frames?: number
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number
  /**
   * Number of Inference Steps
   *
   * The number of inference steps.
   */
  num_inference_steps?: number
  /**
   * Match Input FPS
   *
   * Whether to match the fps in the input video.
   */
  match_input_fps?: boolean
}

/**
 * LipSyncV2ProOutput
 */
export type SchemaSyncLipsyncV2ProOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: SchemaFile
}

/**
 * LipSyncV2ProInput
 */
export type SchemaSyncLipsyncV2ProInput = {
  /**
   * Sync Mode
   *
   * Lipsync mode when audio and video durations are out of sync.
   */
  sync_mode?: 'cut_off' | 'loop' | 'bounce' | 'silence' | 'remap'
  /**
   * Video Url
   *
   * URL of the input video
   */
  video_url: string
  /**
   * Audio Url
   *
   * URL of the input audio
   */
  audio_url: string
}

/**
 * HunyuanFoleyResponse
 */
export type SchemaHunyuanVideoFoleyOutput = {
  /**
   * Video
   *
   * List of generated video files with audio.
   */
  video: SchemaFile
}

/**
 * HunyuanFoleyRequest
 */
export type SchemaHunyuanVideoFoleyInput = {
  /**
   * Video Url
   *
   * The URL of the video to generate audio for.
   */
  video_url: string
  /**
   * Guidance Scale
   *
   * Guidance scale for audio generation.
   */
  guidance_scale?: number
  /**
   * Num Inference Steps
   *
   * Number of inference steps for generation.
   */
  num_inference_steps?: number
  /**
   * Seed
   *
   * Random seed for reproducible generation.
   */
  seed?: number
  /**
   * Negative Prompt
   *
   * Negative prompt to avoid certain audio characteristics.
   */
  negative_prompt?: string
  /**
   * Text Prompt
   *
   * Text description of the desired audio (optional).
   */
  text_prompt: string
}

/**
 * WanVACEPoseResponse
 */
export type SchemaWan22VaceFunA14bPoseOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: SchemaFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * WanVACEPoseRequest
 */
export type SchemaWan22VaceFunA14bPoseInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation. For pose task, the prompt should describe the desired pose and action of the subject in the video.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL to the source video file. Required for pose task.
   */
  video_url: string
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown
  /**
   * Reference Image URLs
   *
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown
}

/**
 * WanVACEDepthResponse
 */
export type SchemaWan22VaceFunA14bDepthOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: SchemaFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * WanVACEDepthRequest
 */
export type SchemaWan22VaceFunA14bDepthInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL to the source video file. Required for depth task.
   */
  video_url: string
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown
  /**
   * Reference Image URLs
   *
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown
}

/**
 * WanVACEInpaintingResponse
 */
export type SchemaWan22VaceFunA14bInpaintingOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: SchemaFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * WanVACEInpaintingRequest
 */
export type SchemaWan22VaceFunA14bInpaintingInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL to the source video file. Required for inpainting.
   */
  video_url: string
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown
  /**
   * Reference Image URLs
   *
   * Urls to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Mask Video URL
   *
   * URL to the source mask file. Required for inpainting.
   */
  mask_video_url: string | unknown
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Mask Image URL
   *
   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video using salient mask tracking. Will be ignored if mask_video_url is provided.
   */
  mask_image_url?: string | unknown
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown
}

/**
 * WanVACEOutpaintingResponse
 */
export type SchemaWan22VaceFunA14bOutpaintingOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: SchemaFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * WanVACEOutpaintingRequest
 */
export type SchemaWan22VaceFunA14bOutpaintingInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL to the source video file. Required for outpainting.
   */
  video_url: string
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown
  /**
   * Reference Image URLs
   *
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>
  /**
   * Expand Ratio
   *
   * Amount of expansion. This is a float value between 0 and 1, where 0.25 adds 25% to the original video size on the specified sides.
   */
  expand_ratio?: number
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Expand Bottom
   *
   * Whether to expand the video to the bottom.
   */
  expand_bottom?: boolean
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Expand Left
   *
   * Whether to expand the video to the left.
   */
  expand_left?: boolean
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
  /**
   * Expand Top
   *
   * Whether to expand the video to the top.
   */
  expand_top?: boolean
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Expand Right
   *
   * Whether to expand the video to the right.
   */
  expand_right?: boolean
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown
}

/**
 * WanVACEReframeResponse
 */
export type SchemaWan22VaceFunA14bReframeOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: SchemaFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * WanVACEReframeRequest
 */
export type SchemaWan22VaceFunA14bReframeInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation. Optional for reframing.
   */
  prompt?: string
  /**
   * Video URL
   *
   * URL to the source video file. This video will be used as a reference for the reframe task.
   */
  video_url: string
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number
  /**
   * Trim Borders
   *
   * Whether to trim borders from the video.
   */
  trim_borders?: boolean
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Zoom Factor
   *
   * Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom.
   */
  zoom_factor?: number
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown
}

/**
 * LucyEditDevOutput
 */
export type SchemaLucyEditDevOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: SchemaFile
}

/**
 * LucyEditDevInput
 */
export type SchemaLucyEditDevInput = {
  /**
   * Sync Mode
   *
   *
   * If set to true, the function will wait for the video to be generated
   * and uploaded before returning the response. This will increase the
   * latency of the function but it allows you to get the video directly
   * in the response without going through the CDN.
   *
   */
  sync_mode?: boolean
  /**
   * Video Url
   *
   * URL of the video to edit
   */
  video_url: string
  /**
   * Prompt
   *
   * Text description of the desired video content
   */
  prompt: string
  /**
   * Enhance Prompt
   *
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean
}

/**
 * LucyEditProOutput
 */
export type SchemaLucyEditProOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: SchemaFile
}

/**
 * LucyEditProInput
 */
export type SchemaLucyEditProInput = {
  /**
   * Sync Mode
   *
   *
   * If set to true, the function will wait for the video to be generated
   * and uploaded before returning the response. This will increase the
   * latency of the function but it allows you to get the video directly
   * in the response without going through the CDN.
   *
   */
  sync_mode?: boolean
  /**
   * Video Url
   *
   * URL of the video to edit
   */
  video_url: string
  /**
   * Prompt
   *
   * Text description of the desired video content
   */
  prompt: string
  /**
   * Resolution
   *
   * Resolution of the generated video
   */
  resolution?: '720p'
  /**
   * Enhance Prompt
   *
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean
}

/**
 * WanAnimateMoveResponse
 */
export type SchemaWanV2214bAnimateMoveOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation (auto-generated by the model)
   */
  prompt: string
  /**
   * Frames Zip
   *
   * ZIP archive of generated frames (if requested).
   */
  frames_zip?: SchemaFile
  /**
   * Seed
   *
   * The seed used for generation
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * WanAnimateMoveRequest
 */
export type SchemaWanV2214bAnimateMoveInput = {
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Video URL
   *
   * URL of the input video.
   */
  video_url: string
  /**
   * Resolution
   *
   * Resolution of the generated video (480p, 580p, or 720p).
   */
  resolution?: '480p' | '580p' | '720p'
  /**
   * Return Frames ZIP
   *
   * If true, also return a ZIP archive containing per-frame images generated on GPU (lossless).
   */
  return_frames_zip?: boolean
  /**
   * Shift
   *
   * Shift value for the video. Must be between 1.0 and 10.0.
   */
  shift?: number
  /**
   * Enable Output Safety Checker
   *
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean
  /**
   * Image URL
   *
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string
  /**
   * Video Quality
   *
   * The quality of the output video. Higher quality means better visual quality but larger file size.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Enable Safety Checker
   *
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Use Turbo
   *
   * If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized for best results.
   */
  use_turbo?: boolean
  /**
   * Guidance Scale
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guidance_scale?: number
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
}

/**
 * WanAnimateReplaceResponse
 */
export type SchemaWanV2214bAnimateReplaceOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation (auto-generated by the model)
   */
  prompt: string
  /**
   * Frames Zip
   *
   * ZIP archive of generated frames (if requested).
   */
  frames_zip?: SchemaFile
  /**
   * Seed
   *
   * The seed used for generation
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * WanAnimateMoveRequest
 */
export type SchemaWanV2214bAnimateReplaceInput = {
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Video URL
   *
   * URL of the input video.
   */
  video_url: string
  /**
   * Resolution
   *
   * Resolution of the generated video (480p, 580p, or 720p).
   */
  resolution?: '480p' | '580p' | '720p'
  /**
   * Return Frames ZIP
   *
   * If true, also return a ZIP archive containing per-frame images generated on GPU (lossless).
   */
  return_frames_zip?: boolean
  /**
   * Shift
   *
   * Shift value for the video. Must be between 1.0 and 10.0.
   */
  shift?: number
  /**
   * Enable Output Safety Checker
   *
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean
  /**
   * Image URL
   *
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string
  /**
   * Video Quality
   *
   * The quality of the output video. Higher quality means better visual quality but larger file size.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Enable Safety Checker
   *
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Use Turbo
   *
   * If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized for best results.
   */
  use_turbo?: boolean
  /**
   * Guidance Scale
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guidance_scale?: number
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
}

/**
 * WanVACEVideoEditResponse
 */
export type SchemaWanVaceAppsVideoEditOutput = {
  /**
   * Frames Zip
   *
   * ZIP archive of generated frames if requested.
   */
  frames_zip?: SchemaFile
  /**
   * Video
   *
   * The edited video.
   */
  video: SchemaVideoFile
}

/**
 * WanVACEVideoEditRequest
 */
export type SchemaWanVaceAppsVideoEditInput = {
  /**
   * Prompt
   *
   * Prompt to edit the video.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL of the input video.
   */
  video_url: string
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular'
  /**
   * Resolution
   *
   * Resolution of the edited video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the edited video.
   */
  aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1'
  /**
   * Return Frames ZIP
   *
   * Whether to include a ZIP archive containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Video Type
   *
   * The type of video you're editing. Use 'general' for most videos, and 'human' for videos emphasizing human subjects and motions. The default value 'auto' means the model will guess based on the first frame of the video.
   */
  video_type?: 'auto' | 'general' | 'human'
  /**
   * Image URLs
   *
   * URLs of the input images to use as a reference for the generation.
   */
  image_urls?: Array<string>
  /**
   * Enable Auto Downsampling
   *
   * Whether to enable automatic downsampling. If your video has a high frame rate or is long, enabling longer sequences to be generated. The video will be interpolated back to the original frame rate after generation.
   */
  enable_auto_downsample?: boolean
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to.
   */
  auto_downsample_min_fps?: number
}

/**
 * SeedVRVideoOutput
 */
export type SchemaSeedvrUpscaleVideoOutput = {
  /**
   * Seed
   *
   * The random seed used for the generation process.
   */
  seed: number
  video: SchemaFile
}

/**
 * SeedVRVideoInput
 */
export type SchemaSeedvrUpscaleVideoInput = {
  /**
   * Upscale Mode
   *
   * The mode to use for the upscale. If 'target', the upscale factor will be calculated based on the target resolution. If 'factor', the upscale factor will be used directly.
   */
  upscale_mode?: 'target' | 'factor'
  /**
   * Video Url
   *
   * The input video to be processed
   */
  video_url: string
  /**
   * Noise Scale
   *
   * The noise scale to use for the generation process.
   */
  noise_scale?: number
  /**
   * Output Format
   *
   * The format of the output video.
   */
  output_format?:
    | 'X264 (.mp4)'
    | 'VP9 (.webm)'
    | 'PRORES4444 (.mov)'
    | 'GIF (.gif)'
  /**
   * Output Write Mode
   *
   * The write mode of the output video.
   */
  output_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Target Resolution
   *
   * The target resolution to upscale to when `upscale_mode` is `target`.
   */
  target_resolution?: '720p' | '1080p' | '1440p' | '2160p'
  /**
   * Output Quality
   *
   * The quality of the output video.
   */
  output_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Upscale Factor
   *
   * Upscaling factor to be used. Will multiply the dimensions with this factor when `upscale_mode` is `factor`.
   */
  upscale_factor?: number
  /**
   * Seed
   *
   * The random seed used for the generation process.
   */
  seed?: number | unknown
}

/**
 * InfinitalkVid2VidResponse
 */
export type SchemaInfinitalkVideoToVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * InfiniTalkVid2VidAudioRequest
 */
export type SchemaInfinitalkVideoToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Resolution
   *
   * Resolution of the video to generate. Must be either 480p or 720p.
   */
  resolution?: '480p' | '720p'
  /**
   * Acceleration
   *
   * The acceleration level to use for generation.
   */
  acceleration?: 'none' | 'regular' | 'high'
  /**
   * Video Url
   *
   * URL of the input video.
   */
  video_url: string
  /**
   * Audio URL
   *
   * The URL of the audio file.
   */
  audio_url: string
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.
   */
  num_frames?: number
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
}

/**
 * LongWanVACEReframeResponse
 */
export type SchemaWanVaceAppsLongReframeOutput = {
  /**
   * Video
   *
   * The output video file.
   */
  video: SchemaVideoFile
}

/**
 * LongWanVACEReframeRequest
 */
export type SchemaWanVaceAppsLongReframeInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation. Optional for reframing.
   */
  prompt?: string
  /**
   * Video URL
   *
   * URL to the source video file. This video will be used as a reference for the reframe task.
   */
  video_url: string
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular'
  /**
   * Paste Back
   *
   * Whether to paste back the reframed scene to the original video.
   */
  paste_back?: boolean
  /**
   * Zoom Factor
   *
   * Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom.
   */
  zoom_factor?: number
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Scene Threshold
   *
   * Threshold for scene detection sensitivity (0-100). Lower values detect more scenes.
   */
  scene_threshold?: number
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Auto Downsample Min Fps
   *
   * Minimum FPS for auto downsample.
   */
  auto_downsample_min_fps?: number
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Trim Borders
   *
   * Whether to trim borders from the video.
   */
  trim_borders?: boolean
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: 'content_aware' | 'white' | 'black'
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: 'rife' | 'film'
  /**
   * Enable Auto Downsample
   *
   * Whether to enable auto downsample.
   */
  enable_auto_downsample?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
}

/**
 * ImageFile
 */
export type SchemaImageFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * Height
   *
   * The height of the image
   */
  height?: number
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Width
   *
   * The width of the image
   */
  width?: number
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
}

/**
 * RemixOutput
 */
export type SchemaSora2VideoToVideoRemixOutput = {
  /**
   * Spritesheet
   *
   * Spritesheet image for the video
   */
  spritesheet?: SchemaImageFile
  /**
   * Thumbnail
   *
   * Thumbnail image for the video
   */
  thumbnail?: SchemaImageFile
  /**
   * Video ID
   *
   * The ID of the generated video
   */
  video_id: string
  /**
   * Video
   *
   * The generated video
   */
  video: SchemaVideoFile
}

/**
 * RemixInput
 */
export type SchemaSora2VideoToVideoRemixInput = {
  /**
   * Prompt
   *
   * Updated text prompt that directs the remix generation
   */
  prompt: string
  /**
   * Video ID
   *
   * The video_id from a previous Sora 2 generation. Note: You can only remix videos that were generated by Sora (via text-to-video or image-to-video endpoints), not arbitrary uploaded videos.
   */
  video_id: string
  /**
   * Delete Video
   *
   * Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted.
   */
  delete_video?: boolean
}

/**
 * VideoToVideoOutput
 */
export type SchemaKreaWan14bVideoToVideoOutput = {
  video: SchemaFile
}

/**
 * VideoToVideoInput
 */
export type SchemaKreaWan14bVideoToVideoInput = {
  /**
   * Prompt
   *
   * Prompt for the video-to-video generation.
   */
  prompt: string
  /**
   * Video Url
   *
   * URL of the input video. Currently, only outputs of 16:9 aspect ratio and 480p resolution are supported. Video duration should be less than 1000 frames at 16fps, and output frames will be 6 plus a multiple of 12, for example 18, 30, 42, etc.
   */
  video_url: string
  /**
   * Strength
   *
   * Denoising strength for the video-to-video generation. 0.0 preserves the original, 1.0 completely remakes the video.
   */
  strength?: number
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * Seed for the video-to-video generation.
   */
  seed?: number | unknown
}

/**
 * Video
 */
export type SchemaVideoOutput = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * VideoOutput
 */
export type SchemaSfxV15VideoToVideoOutput = {
  /**
   * Video
   *
   * The processed video with sound effects
   */
  video: Array<SchemaVideoOutput>
}

/**
 * Input
 */
export type SchemaSfxV15VideoToVideoInput = {
  /**
   * Num Samples
   *
   * The number of samples to generate from the model
   */
  num_samples?: number | unknown
  /**
   * Duration
   *
   * The duration of the generated audio in seconds
   */
  duration?: number | unknown
  /**
   * Start Offset
   *
   * The start offset in seconds to start the audio generation from
   */
  start_offset?: number | unknown
  /**
   * Video Url
   *
   * A video url that can accessed from the API to process and add sound effects
   */
  video_url: string
  /**
   * Seed
   *
   * The seed to use for the generation. If not provided, a random seed will be used
   */
  seed?: number | unknown
  /**
   * Text Prompt
   *
   * Additional description to guide the model
   */
  text_prompt?: string | unknown
}

/**
 * Q2VideoExtensionOutput
 */
export type SchemaViduQ2VideoExtensionProOutput = {
  /**
   * Video
   *
   * The extended video using the Q2 model
   */
  video: SchemaFile
}

/**
 * Q2VideoExtensionRequest
 */
export type SchemaViduQ2VideoExtensionProInput = {
  /**
   * Prompt
   *
   * text prompt to guide the video extension
   */
  prompt?: string
  /**
   * Duration
   *
   * Duration of the extension in seconds
   */
  duration?: 2 | 3 | 4 | 5 | 6 | 7
  /**
   * Video Url
   *
   * URL of the video to extend
   */
  video_url: string
  /**
   * Resolution
   *
   * Output video resolution
   */
  resolution?: '720p' | '1080p'
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
}

/**
 * VideoOutput
 */
export type SchemaBirefnetV2VideoOutput = {
  /**
   * Video
   *
   * Video with background removed
   */
  video: SchemaVideoFile
  /**
   * Mask Video
   *
   * Mask used to remove the background
   */
  mask_video?: SchemaVideoFile
}

/**
 * VideoInputV2
 */
export type SchemaBirefnetV2VideoInput = {
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | 'X264 (.mp4)'
    | 'VP9 (.webm)'
    | 'PRORES4444 (.mov)'
    | 'GIF (.gif)'
  /**
   * Operating Resolution
   *
   * The resolution to operate on. The higher the resolution, the more accurate the output will be for high res input images. The '2304x2304' option is only available for the 'General Use (Dynamic)' model.
   */
  operating_resolution?: '1024x1024' | '2048x2048' | '2304x2304'
  /**
   * Video Url
   *
   * URL of the video to remove background from
   */
  video_url: string
  /**
   * Model
   *
   *
   * Model to use for background removal.
   * The 'General Use (Light)' model is the original model used in the BiRefNet repository.
   * The 'General Use (Light 2K)' model is the original model used in the BiRefNet repository but trained with 2K images.
   * The 'General Use (Heavy)' model is a slower but more accurate model.
   * The 'Matting' model is a model trained specifically for matting images.
   * The 'Portrait' model is a model trained specifically for portrait images.
   * The 'General Use (Dynamic)' model supports dynamic resolutions from 256x256 to 2304x2304.
   * The 'General Use (Light)' model is recommended for most use cases.
   *
   * The corresponding models are as follows:
   * - 'General Use (Light)': BiRefNet
   * - 'General Use (Light 2K)': BiRefNet_lite-2K
   * - 'General Use (Heavy)': BiRefNet_lite
   * - 'Matting': BiRefNet-matting
   * - 'Portrait': BiRefNet-portrait
   * - 'General Use (Dynamic)': BiRefNet_dynamic
   *
   */
  model?:
    | 'General Use (Light)'
    | 'General Use (Light 2K)'
    | 'General Use (Heavy)'
    | 'Matting'
    | 'Portrait'
    | 'General Use (Dynamic)'
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Output Mask
   *
   * Whether to output the mask used to remove the background
   */
  output_mask?: boolean
  /**
   * Refine Foreground
   *
   * Whether to refine the foreground using the estimated mask
   */
  refine_foreground?: boolean
}

/**
 * VideoEffectOutput
 */
export type SchemaVideoAsPromptOutput = {
  video: SchemaFile
}

/**
 * VideoEffectInputWan
 */
export type SchemaVideoAsPromptInput = {
  /**
   * Prompt
   *
   * The prompt to generate an image from.
   */
  prompt: string
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: '16:9' | '9:16'
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: '480p' | '580p' | '720p'
  /**
   * Video Url
   *
   * reference video to generate effect video from.
   */
  video_url: string
  /**
   * Image Url
   *
   * Input image to generate the effect video for.
   */
  image_url: string
  /**
   * Frames Per Second
   *
   * Frames per second for the output video. Only applicable if output_type is 'video'.
   */
  fps?: number
  /**
   * Video Description
   *
   * A brief description of the input video content.
   */
  video_description: string
  /**
   * Seed
   *
   * Random seed for reproducible generation. If set none, a random seed will be used.
   */
  seed?: number | unknown
  /**
   * Guidance Scale
   *
   * Guidance scale for generation.
   */
  guidance_scale?: number
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Num Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number
}

/**
 * UpscaleOutput
 */
export type SchemaBytedanceUpscalerUpscaleVideoOutput = {
  /**
   * Duration
   *
   * Duration of audio input/video output as used for billing.
   */
  duration: number
  /**
   * Video
   *
   * Generated video file
   */
  video: SchemaFile
}

/**
 * UpscaleInput
 */
export type SchemaBytedanceUpscalerUpscaleVideoInput = {
  /**
   * Target Fps
   *
   * The target FPS of the video to upscale.
   */
  target_fps?: '30fps' | '60fps'
  /**
   * Video Url
   *
   * The URL of the video to upscale.
   */
  video_url: string
  /**
   * Target Resolution
   *
   * The target resolution of the video to upscale.
   */
  target_resolution?: '1080p' | '2k' | '4k'
}

/**
 * AutoSubtitleOutput
 *
 * Output model for video with automatic subtitles
 */
export type SchemaWorkflowUtilitiesAutoSubtitleOutput = {
  /**
   * Transcription
   *
   * Full transcription text
   */
  transcription: string
  /**
   * Subtitle Count
   *
   * Number of subtitle segments generated
   */
  subtitle_count: number
  /**
   * Transcription Metadata
   *
   * Additional transcription metadata from ElevenLabs (language, segments, etc.)
   */
  transcription_metadata?: {
    [key: string]: unknown
  }
  /**
   * Words
   *
   * Word-level timing information from transcription service
   */
  words?: Array<{
    [key: string]: unknown
  }>
  /**
   * Video
   *
   * The video with automatic subtitles
   */
  video: SchemaFile
}

/**
 * AutoSubtitleInput
 *
 * Input model for automatic subtitle generation and styling
 */
export type SchemaWorkflowUtilitiesAutoSubtitleInput = {
  /**
   * Font Weight
   *
   * Font weight (TikTok style typically uses bold or black)
   */
  font_weight?: 'normal' | 'bold' | 'black'
  /**
   * Video Url
   *
   * URL of the video file to add automatic subtitles to
   *
   * Max file size: 95.4MB, Timeout: 30.0s
   */
  video_url: string
  /**
   * Stroke Width
   *
   * Text stroke/outline width in pixels (0 for no stroke)
   */
  stroke_width?: number
  /**
   * Font Color
   *
   * Subtitle text color for non-active words
   */
  font_color?:
    | 'white'
    | 'black'
    | 'red'
    | 'green'
    | 'blue'
    | 'yellow'
    | 'orange'
    | 'purple'
    | 'pink'
    | 'brown'
    | 'gray'
    | 'cyan'
    | 'magenta'
  /**
   * Font Size
   *
   * Font size for subtitles (TikTok style uses larger text)
   */
  font_size?: number
  /**
   * Language
   *
   * Language code for transcription (e.g., 'en', 'es', 'fr', 'de', 'it', 'pt', 'nl', 'ja', 'zh', 'ko') or 3-letter ISO code (e.g., 'eng', 'spa', 'fra')
   */
  language?: string
  /**
   * Y Offset
   *
   * Vertical offset in pixels (positive = move down, negative = move up)
   */
  y_offset?: number
  /**
   * Background Opacity
   *
   * Background opacity (0.0 = fully transparent, 1.0 = fully opaque)
   */
  background_opacity?: number
  /**
   * Stroke Color
   *
   * Text stroke/outline color
   */
  stroke_color?:
    | 'black'
    | 'white'
    | 'red'
    | 'green'
    | 'blue'
    | 'yellow'
    | 'orange'
    | 'purple'
    | 'pink'
    | 'brown'
    | 'gray'
    | 'cyan'
    | 'magenta'
  /**
   * Highlight Color
   *
   * Color for the currently speaking word (karaoke-style highlight)
   */
  highlight_color?:
    | 'white'
    | 'black'
    | 'red'
    | 'green'
    | 'blue'
    | 'yellow'
    | 'orange'
    | 'purple'
    | 'pink'
    | 'brown'
    | 'gray'
    | 'cyan'
    | 'magenta'
  /**
   * Enable Animation
   *
   * Enable animation effects for subtitles (bounce style entrance)
   */
  enable_animation?: boolean
  /**
   * Font Name
   *
   * Any Google Font name from fonts.google.com (e.g., 'Montserrat', 'Poppins', 'BBH Sans Hegarty')
   */
  font_name?: string
  /**
   * Position
   *
   * Vertical position of subtitles
   */
  position?: 'top' | 'center' | 'bottom'
  /**
   * Words Per Subtitle
   *
   * Maximum number of words per subtitle segment. Use 1 for single-word display, 2-3 for short phrases, or 8-12 for full sentences.
   */
  words_per_subtitle?: number
  /**
   * Background Color
   *
   * Background color behind text ('none' or 'transparent' for no background)
   */
  background_color?:
    | 'black'
    | 'white'
    | 'red'
    | 'green'
    | 'blue'
    | 'yellow'
    | 'orange'
    | 'purple'
    | 'pink'
    | 'brown'
    | 'gray'
    | 'cyan'
    | 'magenta'
    | 'none'
    | 'transparent'
}

/**
 * FlashVSRPlusVideoOutput
 */
export type SchemaFlashvsrUpscaleVideoOutput = {
  /**
   * Seed
   *
   * The random seed used for the generation process.
   */
  seed: number
  /**
   * Video
   *
   * Upscaled video file after processing
   */
  video: SchemaFile
}

/**
 * FlashVSRPlusVideoInput
 *
 * Input fields common to FlashVSR+ image/video endpoints.
 */
export type SchemaFlashvsrUpscaleVideoInput = {
  /**
   * Video Url
   *
   * The input video to be upscaled
   */
  video_url: string
  /**
   * Acceleration
   *
   * Acceleration mode for VAE decoding. Options: regular (best quality), high (balanced), full (fastest). More accerleation means longer duration videos can be processed too.
   */
  acceleration?: 'regular' | 'high' | 'full'
  /**
   * Quality
   *
   * Quality level for tile blending (0-100). Controls overlap between tiles to prevent grid artifacts. Higher values provide better quality with more overlap. Recommended: 70-85 for high-res videos, 50-70 for faster processing.
   */
  quality?: number
  /**
   * Output Format
   *
   * The format of the output video.
   */
  output_format?:
    | 'X264 (.mp4)'
    | 'VP9 (.webm)'
    | 'PRORES4444 (.mov)'
    | 'GIF (.gif)'
  /**
   * Color Fix
   *
   * Color correction enabled.
   */
  color_fix?: boolean
  /**
   * Output Write Mode
   *
   * The write mode of the output video.
   */
  output_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned inline and not stored in history.
   */
  sync_mode?: boolean
  /**
   * Output Quality
   *
   * The quality of the output video.
   */
  output_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Upscale Factor
   *
   * Upscaling factor to be used.
   */
  upscale_factor?: number
  /**
   * Preserve Audio
   *
   * Copy the original audio tracks into the upscaled video using FFmpeg when possible.
   */
  preserve_audio?: boolean
  /**
   * Seed
   *
   * The random seed used for the generation process.
   */
  seed?: number
}

/**
 * EdittoOutput
 */
export type SchemaEdittoOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: SchemaFile | unknown
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * EdittoInput
 */
export type SchemaEdittoInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string
  /**
   * Video URL
   *
   * URL to the source video file. Required for inpainting.
   */
  video_url: string
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: 'none' | 'low' | 'regular' | unknown
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: 'unipc' | 'dpm++' | 'euler'
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: 'auto' | '240p' | '360p' | '480p' | '580p' | '720p'
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '1:1' | '9:16'
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean
}

/**
 * PointPromptBase
 */
export type SchemaPointPromptBase = {
  /**
   * Y
   *
   * Y Coordinate of the prompt
   */
  y?: number
  /**
   * X
   *
   * X Coordinate of the prompt
   */
  x?: number
  /**
   * Object Id
   *
   * Optional object identifier. Prompts sharing an object id refine the same object.
   */
  object_id?: number
  /**
   * Label
   *
   * 1 for foreground, 0 for background
   */
  label?: 0 | 1
}

/**
 * BoxPromptBase
 */
export type SchemaBoxPromptBase = {
  /**
   * Y Min
   *
   * Y Min Coordinate of the box
   */
  y_min?: number
  /**
   * Object Id
   *
   * Optional object identifier. Boxes sharing an object id refine the same object.
   */
  object_id?: number
  /**
   * X Max
   *
   * X Max Coordinate of the box
   */
  x_max?: number
  /**
   * X Min
   *
   * X Min Coordinate of the box
   */
  x_min?: number
  /**
   * Y Max
   *
   * Y Max Coordinate of the box
   */
  y_max?: number
}

/**
 * SAM3VideoOutput
 */
export type SchemaSam3VideoOutput = {
  /**
   * Boundingbox Frames Zip
   *
   * Zip file containing per-frame bounding box overlays.
   */
  boundingbox_frames_zip?: SchemaFile
  /**
   * Video
   *
   * The segmented video.
   */
  video: SchemaFile
}

/**
 * SAM3VideoInput
 */
export type SchemaSam3VideoInput = {
  /**
   * Prompt
   *
   * Text prompt for segmentation. Use commas to track multiple objects (e.g., 'person, cloth').
   */
  prompt?: string
  /**
   * Video Url
   *
   * The URL of the video to be segmented.
   */
  video_url: string
  /**
   * Detection Threshold
   *
   * Detection confidence threshold (0.0-1.0). Lower = more detections but less precise.
   */
  detection_threshold?: number
  /**
   * Box Prompts
   *
   * List of box prompt coordinates (x_min, y_min, x_max, y_max).
   */
  box_prompts?: Array<SchemaBoxPromptBase>
  /**
   * Point Prompts
   *
   * List of point prompts
   */
  point_prompts?: Array<SchemaPointPromptBase>
  /**
   * Apply Mask
   *
   * Apply the mask on the video.
   */
  apply_mask?: boolean
  /**
   * Text Prompt
   *
   * [DEPRECATED] Use 'prompt' instead. Kept for backward compatibility.
   *
   * @deprecated
   */
  text_prompt?: string
}

/**
 * SAM3VideoOutput
 */
export type SchemaSam3VideoRleOutput = {
  /**
   * Boundingbox Frames Zip
   *
   * Zip file containing per-frame bounding box overlays.
   */
  boundingbox_frames_zip?: SchemaFile
  /**
   * Video
   *
   * The segmented video.
   */
  video: SchemaFile
}

/**
 * SAM3VideoRLEInput
 */
export type SchemaSam3VideoRleInput = {
  /**
   * Prompt
   *
   * Text prompt for segmentation. Use commas to track multiple objects (e.g., 'person, cloth').
   */
  prompt?: string
  /**
   * Video Url
   *
   * The URL of the video to be segmented.
   */
  video_url: string
  /**
   * Detection Threshold
   *
   * Detection confidence threshold (0.0-1.0). Lower = more detections but less precise. Defaults: 0.5 for existing, 0.7 for new objects. Try 0.2-0.3 if text prompts fail.
   */
  detection_threshold?: number
  /**
   * Box Prompts
   *
   * List of box prompts with optional frame_index.
   */
  box_prompts?: Array<SchemaBoxPrompt>
  /**
   * Boundingbox Zip
   *
   * Return per-frame bounding box overlays as a zip archive.
   */
  boundingbox_zip?: boolean
  /**
   * Point Prompts
   *
   * List of point prompts with frame indices.
   */
  point_prompts?: Array<SchemaPointPrompt>
  /**
   * Frame Index
   *
   * Frame index used for initial interaction when mask_url is provided.
   */
  frame_index?: number
  /**
   * Mask Url
   *
   * The URL of the mask to be applied initially.
   */
  mask_url?: string
  /**
   * Apply Mask
   *
   * Apply the mask on the video.
   */
  apply_mask?: boolean
}

/**
 * LucyEditFastOutput
 */
export type SchemaLucyEditFastOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: SchemaFile
}

/**
 * LucyEditFastInput
 */
export type SchemaLucyEditFastInput = {
  /**
   * Sync Mode
   *
   *
   * If set to true, the function will wait for the video to be generated
   * and uploaded before returning the response. This will increase the
   * latency of the function but it allows you to get the video directly
   * in the response without going through the CDN.
   *
   */
  sync_mode?: boolean
  /**
   * Video Url
   *
   * URL of the video to edit
   */
  video_url: string
  /**
   * Prompt
   *
   * Text description of the desired video content
   */
  prompt: string
  /**
   * Enhance Prompt
   *
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean
}

/**
 * LTXRetakeVideoResponse
 */
export type SchemaLtx2RetakeVideoOutput = {
  /**
   * Video
   *
   * The generated video file
   */
  video: SchemaVideoFile
}

/**
 * LTXRetakeVideoRequest
 */
export type SchemaLtx2RetakeVideoInput = {
  /**
   * Prompt
   *
   * The prompt to retake the video with
   */
  prompt: string
  /**
   * Duration
   *
   * The duration of the video to retake in seconds
   */
  duration?: number
  /**
   * Video URL
   *
   * The URL of the video to retake
   */
  video_url: string
  /**
   * Start Time
   *
   * The start time of the video to retake in seconds
   */
  start_time?: number
  /**
   * Retake Mode
   *
   * The retake mode to use for the retake
   */
  retake_mode?: 'replace_audio' | 'replace_video' | 'replace_audio_and_video'
}

/**
 * GreenScreenRembgOutput
 */
export type SchemaVideoBackgroundRemovalGreenScreenOutput = {
  /**
   * Video
   */
  video: Array<SchemaFile>
}

/**
 * GreenScreenRembgInput
 */
export type SchemaVideoBackgroundRemovalGreenScreenInput = {
  /**
   * Video Url
   */
  video_url: string
  /**
   * Output Codec
   *
   * Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality.
   */
  output_codec?: 'vp9' | 'h264'
  /**
   * Spill Suppression Strength
   *
   * Increase the value if green spots remain in the video, decrease if color changes are noticed on the extracted subject.
   */
  spill_suppression_strength?: number | unknown
}

/**
 * OmniV2VReferenceOutput
 */
export type SchemaKlingVideoO1VideoToVideoReferenceOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: SchemaFile
}

/**
 * OmniV2VReferenceInput
 *
 * Input for video editing or video-as-reference generation.
 */
export type SchemaKlingVideoO1VideoToVideoReferenceInput = {
  /**
   * Prompt
   *
   * Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.
   */
  prompt: string
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video frame. If 'auto', the aspect ratio will be determined automatically based on the input video, and the closest aspect ratio to the input video will be used.
   */
  aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1'
  /**
   * Duration
   *
   * Video duration in seconds.
   */
  duration?: '3' | '4' | '5' | '6' | '7' | '8' | '9' | '10'
  /**
   * Video Url
   *
   * Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB.
   *
   * Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s
   */
  video_url: string
  /**
   * Keep Audio
   *
   * Whether to keep the original audio from the video.
   */
  keep_audio?: boolean
  /**
   * Elements
   *
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  elements?: Array<SchemaOmniVideoElementInput>
  /**
   * Image Urls
   *
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>
}

/**
 * OmniVideoElementInput
 */
export type SchemaOmniVideoElementInput = {
  /**
   * Reference Image Urls
   *
   * Additional reference images from different angles. 1-4 images supported. At least one image is required.
   */
  reference_image_urls?: Array<string>
  /**
   * Frontal Image Url
   *
   * The frontal image of the element (main view).
   *
   * Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s
   */
  frontal_image_url: string
}

/**
 * OmniV2VEditOutput
 */
export type SchemaKlingVideoO1VideoToVideoEditOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: SchemaFile
}

/**
 * OmniV2VEditInput
 *
 * Input for video editing or video-as-reference generation.
 */
export type SchemaKlingVideoO1VideoToVideoEditInput = {
  /**
   * Prompt
   *
   * Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.
   */
  prompt: string
  /**
   * Video Url
   *
   * Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB.
   *
   * Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s
   */
  video_url: string
  /**
   * Elements
   *
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  elements?: Array<SchemaOmniVideoElementInput>
  /**
   * Image Urls
   *
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>
  /**
   * Keep Audio
   *
   * Whether to keep the original audio from the video.
   */
  keep_audio?: boolean
}

/**
 * FastGeneralRembgOutput
 */
export type SchemaVideoBackgroundRemovalFastOutput = {
  /**
   * Video
   */
  video: Array<SchemaFile>
}

/**
 * FastGeneralRembgInput
 */
export type SchemaVideoBackgroundRemovalFastInput = {
  /**
   * Video Url
   */
  video_url: string
  /**
   * Subject Is Person
   *
   * Set to False if the subject is not a person.
   */
  subject_is_person?: boolean
  /**
   * Output Codec
   *
   * Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality.
   */
  output_codec?: 'vp9' | 'h264'
  /**
   * Refine Foreground Edges
   *
   * Improves the quality of the extracted object's edges.
   */
  refine_foreground_edges?: boolean
}

/**
 * React1Output
 */
export type SchemaSyncLipsyncReact1Output = {
  /**
   * Video
   *
   * The generated video with synchronized lip and facial movements.
   */
  video: SchemaVideoFile
}

/**
 * React1Input
 */
export type SchemaSyncLipsyncReact1Input = {
  /**
   * Emotion
   *
   * Emotion prompt for the generation. Currently supports single-word emotions only.
   */
  emotion: 'happy' | 'angry' | 'sad' | 'neutral' | 'disgusted' | 'surprised'
  /**
   * Video Url
   *
   * URL to the input video. Must be **15 seconds or shorter**.
   */
  video_url: string
  /**
   * Lipsync Mode
   *
   * Lipsync mode when audio and video durations are out of sync.
   */
  lipsync_mode?: 'cut_off' | 'loop' | 'bounce' | 'silence' | 'remap'
  /**
   * Audio Url
   *
   * URL to the input audio. Must be **15 seconds or shorter**.
   */
  audio_url: string
  /**
   * Temperature
   *
   * Controls the expresiveness of the lipsync.
   */
  temperature?: number
  /**
   * Model Mode
   *
   * Controls the edit region and movement scope for the model. Available options:
   * - `lips`: Only lipsync using react-1 (minimal facial changes).
   * - `face`: Lipsync + facial expressions without head movements.
   * - `head`: Lipsync + facial expressions + natural talking head movements.
   */
  model_mode?: 'lips' | 'face' | 'head'
}

/**
 * Output
 *
 * Output from Wan Vision Enhancer
 */
export type SchemaWanVisionEnhancerOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Timings
   *
   * The timings of the different steps in the workflow.
   */
  timings: {
    [key: string]: number
  }
  /**
   * The enhanced video file.
   */
  video: SchemaFile
}

/**
 * Input
 *
 * Input parameters for Wan Vision Enhancer (Video-to-Video)
 */
export type SchemaWanVisionEnhancerInput = {
  /**
   * Prompt
   *
   * Optional prompt to prepend to the VLM-generated description. Leave empty to use only the auto-generated description from the video.
   */
  prompt?: string | unknown
  /**
   * Video Url
   *
   * The URL of the video to enhance with Wan Video. Maximum 200MB file size. Videos longer than 500 frames will have only the first 500 frames processed (~8-21 seconds depending on fps).
   */
  video_url: string
  /**
   * Seed
   *
   * Random seed for reproducibility. If not provided, a random seed will be used.
   */
  seed?: number | unknown
  /**
   * Output Resolution
   *
   * Target output resolution for the enhanced video. 720p (native, fast) or 1080p (upscaled, slower). Processing is always done at 720p, then upscaled if 1080p selected.
   */
  target_resolution?: '720p' | '1080p'
  /**
   * Negative Prompt
   *
   * Negative prompt to avoid unwanted features.
   */
  negative_prompt?: string | unknown
  /**
   * Creativity
   *
   * Controls how much the model enhances/changes the video. 0 = Minimal change (preserves original), 1 = Subtle enhancement (default), 2 = Medium enhancement, 3 = Strong enhancement, 4 = Maximum enhancement.
   */
  creativity?: number
}

/**
 * OneToALLAnimationResponse
 */
export type SchemaOneToAllAnimation14bOutput = {
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * OneToALLAnimationRequest
 */
export type SchemaOneToAllAnimation14bInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * Resolution
   *
   * The resolution of the video to generate.
   */
  resolution?: '480p' | '580p' | '720p'
  /**
   * Image Guidance Scale
   *
   * The image guidance scale to use for the video generation.
   */
  image_guidance_scale?: number
  /**
   * Pose Guidance Scale
   *
   * The pose guidance scale to use for the video generation.
   */
  pose_guidance_scale?: number
  /**
   * Video Url
   *
   * The URL of the video to use as a reference for the video generation.
   */
  video_url: string
  /**
   * Image Url
   *
   * The URL of the image to use as a reference for the video generation.
   */
  image_url: string
  /**
   * Num Inference Steps
   *
   * The number of inference steps to use for the video generation.
   */
  num_inference_steps?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt: string
}

/**
 * OneToALLAnimationResponse
 */
export type SchemaOneToAllAnimation13bOutput = {
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * OneToALLAnimationRequest
 */
export type SchemaOneToAllAnimation13bInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * Resolution
   *
   * The resolution of the video to generate.
   */
  resolution?: '480p' | '580p' | '720p'
  /**
   * Image Guidance Scale
   *
   * The image guidance scale to use for the video generation.
   */
  image_guidance_scale?: number
  /**
   * Pose Guidance Scale
   *
   * The pose guidance scale to use for the video generation.
   */
  pose_guidance_scale?: number
  /**
   * Video Url
   *
   * The URL of the video to use as a reference for the video generation.
   */
  video_url: string
  /**
   * Image Url
   *
   * The URL of the image to use as a reference for the video generation.
   */
  image_url: string
  /**
   * Num Inference Steps
   *
   * The number of inference steps to use for the video generation.
   */
  num_inference_steps?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt: string
}

/**
 * SteadyDancerResponse
 *
 * Response model for SteadyDancer.
 */
export type SchemaSteadyDancerOutput = {
  /**
   * Num Frames
   *
   * The actual number of frames generated (aligned to 4k+1 pattern).
   */
  num_frames: number
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated dance animation video.
   */
  video: SchemaFile
}

/**
 * SteadyDancerRequest
 *
 * Request model for SteadyDancer human animation.
 */
export type SchemaSteadyDancerInput = {
  /**
   * Prompt
   *
   * Text prompt describing the desired animation.
   */
  prompt?: string
  /**
   * Video Url
   *
   * URL of the driving pose video. The motion from this video will be transferred to the reference image.
   */
  video_url?: string
  /**
   * Acceleration
   *
   * Acceleration levels.
   */
  acceleration?: 'light' | 'moderate' | 'aggressive'
  /**
   * Pose Guidance Scale
   *
   * Pose guidance scale for pose control strength.
   */
  pose_guidance_scale?: number
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number
  /**
   * Pose Guidance End
   *
   * End ratio for pose guidance. Controls when pose guidance ends.
   */
  pose_guidance_end?: number
  /**
   * Frames Per Second
   *
   * Frames per second of the generated video. Must be between 5 to 24. If not specified, uses the FPS from the input video.
   */
  frames_per_second?: number
  /**
   * Guidance Scale
   *
   * Classifier-free guidance scale for prompt adherence.
   */
  guidance_scale?: number
  /**
   * Num Frames
   *
   * Number of frames to generate. If not specified, uses the frame count from the input video (capped at 241). Will be adjusted to nearest valid value (must satisfy 4k+1 pattern).
   */
  num_frames?: number
  /**
   * Use Turbo
   *
   * If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized (num_inference_steps=6, guidance_scale=1.0) and uses the LightX2V distillation LoRA.
   */
  use_turbo?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. If 'auto', will be determined from the reference image.
   */
  aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1'
  /**
   * Pose Guidance Start
   *
   * Start ratio for pose guidance. Controls when pose guidance begins.
   */
  pose_guidance_start?: number
  /**
   * Resolution
   *
   * Resolution of the generated video. 576p is default, 720p for higher quality. 480p is lower quality.
   */
  resolution?: '480p' | '576p' | '720p'
  /**
   * Image Url
   *
   * URL of the reference image to animate. This is the person/character whose appearance will be preserved.
   */
  image_url?: string
  /**
   * Preserve Audio
   *
   * If enabled, copies audio from the input driving video to the output video.
   */
  preserve_audio?: boolean
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
}

/**
 * OmniV2VEditOutput
 */
export type SchemaKlingVideoO1StandardVideoToVideoEditOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: SchemaFile
}

/**
 * OmniV2VEditInput
 *
 * Input for video editing or video-as-reference generation.
 */
export type SchemaKlingVideoO1StandardVideoToVideoEditInput = {
  /**
   * Prompt
   *
   * Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.
   */
  prompt: string
  /**
   * Video Url
   *
   * Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB.
   *
   * Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s
   */
  video_url: string
  /**
   * Elements
   *
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  elements?: Array<SchemaOmniVideoElementInput>
  /**
   * Image Urls
   *
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>
  /**
   * Keep Audio
   *
   * Whether to keep the original audio from the video.
   */
  keep_audio?: boolean
}

/**
 * OmniV2VReferenceOutput
 */
export type SchemaKlingVideoO1StandardVideoToVideoReferenceOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: SchemaFile
}

/**
 * OmniV2VReferenceInput
 *
 * Input for video editing or video-as-reference generation.
 */
export type SchemaKlingVideoO1StandardVideoToVideoReferenceInput = {
  /**
   * Prompt
   *
   * Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.
   */
  prompt: string
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video frame. If 'auto', the aspect ratio will be determined automatically based on the input video, and the closest aspect ratio to the input video will be used.
   */
  aspect_ratio?: 'auto' | '16:9' | '9:16' | '1:1'
  /**
   * Duration
   *
   * Video duration in seconds.
   */
  duration?: '3' | '4' | '5' | '6' | '7' | '8' | '9' | '10'
  /**
   * Video Url
   *
   * Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB.
   *
   * Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s
   */
  video_url: string
  /**
   * Keep Audio
   *
   * Whether to keep the original audio from the video.
   */
  keep_audio?: boolean
  /**
   * Elements
   *
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  elements?: Array<SchemaOmniVideoElementInput>
  /**
   * Image Urls
   *
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>
}

/**
 * Veo31VideoToVideoOutput
 */
export type SchemaVeo31ExtendVideoOutput = {
  /**
   * Video
   *
   * The extended video.
   */
  video: SchemaFile
}

/**
 * Veo31VideoToVideoInput
 *
 * Input for video extension/video-to-video generation.
 */
export type SchemaVeo31ExtendVideoInput = {
  /**
   * Prompt
   *
   * The text prompt describing how the video should be extended
   */
  prompt: string
  /**
   * Duration
   *
   * The duration of the generated video.
   */
  duration?: '7s'
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '9:16'
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean
  /**
   * Auto Fix
   *
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean
  /**
   * Video URL
   *
   * URL of the video to extend. The video should be 720p or 1080p resolution in 16:9 or 9:16 aspect ratio.
   */
  video_url: string
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: '720p'
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number
  /**
   * Negative Prompt
   *
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string
}

/**
 * Veo31VideoToVideoOutput
 */
export type SchemaVeo31FastExtendVideoOutput = {
  /**
   * Video
   *
   * The extended video.
   */
  video: SchemaFile
}

/**
 * Veo31VideoToVideoInput
 *
 * Input for video extension/video-to-video generation.
 */
export type SchemaVeo31FastExtendVideoInput = {
  /**
   * Prompt
   *
   * The text prompt describing how the video should be extended
   */
  prompt: string
  /**
   * Duration
   *
   * The duration of the generated video.
   */
  duration?: '7s'
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video.
   */
  aspect_ratio?: 'auto' | '16:9' | '9:16'
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean
  /**
   * Auto Fix
   *
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean
  /**
   * Video URL
   *
   * URL of the video to extend. The video should be 720p or 1080p resolution in 16:9 or 9:16 aspect ratio.
   */
  video_url: string
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: '720p'
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number
  /**
   * Negative Prompt
   *
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string
}

/**
 * ReferenceToVideoOutput
 *
 * Output for reference-to-video generation
 */
export type SchemaV26ReferenceToVideoOutput = {
  /**
   * Actual Prompt
   *
   * The actual prompt used if prompt rewriting was enabled
   */
  actual_prompt?: string
  /**
   * Seed
   *
   * The seed used for generation
   */
  seed: number
  /**
   * Video
   *
   * The generated video file
   */
  video: SchemaVideoFile
}

/**
 * ReferenceToVideoInput
 *
 * Input for Wan 2.6 reference-to-video generation (R2V)
 */
export type SchemaV26ReferenceToVideoInput = {
  /**
   * Prompt
   *
   * Use @Video1, @Video2, @Video3 to reference subjects from your videos. Works for people, animals, or objects. For multi-shot prompts: '[0-3s] Shot 1. [3-6s] Shot 2.' Max 800 characters.
   */
  prompt: string
  /**
   * Resolution
   *
   * Video resolution tier. R2V only supports 720p and 1080p (no 480p).
   */
  resolution?: '720p' | '1080p'
  /**
   * Video Urls
   *
   * Reference videos for subject consistency (1-3 videos). Videos' FPS must be at least 16 FPS.Reference in prompt as @Video1, @Video2, @Video3. Works for people, animals, or objects.
   */
  video_urls: Array<string>
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video.
   */
  aspect_ratio?: '16:9' | '9:16' | '1:1' | '4:3' | '3:4'
  /**
   * Duration
   *
   * Duration of the generated video in seconds. R2V supports only 5 or 10 seconds (no 15s).
   */
  duration?: '5' | '10'
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt rewriting using LLM.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
  /**
   * Multi Shots
   *
   * When true (default), enables intelligent multi-shot segmentation for coherent narrative videos with multiple shots. When false, generates single continuous shot. Only active when enable_prompt_expansion is True.
   */
  multi_shots?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt to describe content to avoid. Max 500 characters.
   */
  negative_prompt?: string
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean
}

/**
 * VideoOutput
 */
export type SchemaBriaVideoEraserErasePromptOutput = {
  /**
   * Video
   *
   * Final video.
   */
  video: SchemaVideo | SchemaFile
}

/**
 * EraseByPromptInputModel
 */
export type SchemaBriaVideoEraserErasePromptInput = {
  /**
   * Preserve Audio
   *
   * If true, audio will be preserved in the output video.
   */
  preserve_audio?: boolean
  /**
   * Video Url
   *
   * Input video to erase object from. duration must be less than 5s.
   */
  video_url: string
  /**
   * Prompt
   *
   * Input prompt to detect object to erase
   */
  prompt: string
  /**
   * Output Container And Codec
   *
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.
   */
  output_container_and_codec?:
    | 'mp4_h265'
    | 'mp4_h264'
    | 'webm_vp9'
    | 'gif'
    | 'mov_h264'
    | 'mov_h265'
    | 'mov_proresks'
    | 'mkv_h264'
    | 'mkv_h265'
    | 'mkv_vp9'
    | 'mkv_mpeg4'
  /**
   * Auto Trim
   *
   * auto trim the video, to working duration ( 5s )
   */
  auto_trim?: boolean
}

/**
 * VideoOutput
 */
export type SchemaBriaVideoEraserEraseKeypointsOutput = {
  /**
   * Video
   *
   * Final video.
   */
  video: SchemaVideo | SchemaFile
}

/**
 * EraseByKeyPointsInputModel
 */
export type SchemaBriaVideoEraserEraseKeypointsInput = {
  /**
   * Preserve Audio
   *
   * If true, audio will be preserved in the output video.
   */
  preserve_audio?: boolean
  /**
   * Video Url
   *
   * Input video to erase object from. duration must be less than 5s.
   */
  video_url: string
  /**
   * Output Container And Codec
   *
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.
   */
  output_container_and_codec?:
    | 'mp4_h265'
    | 'mp4_h264'
    | 'webm_vp9'
    | 'gif'
    | 'mov_h264'
    | 'mov_h265'
    | 'mov_proresks'
    | 'mkv_h264'
    | 'mkv_h265'
    | 'mkv_vp9'
    | 'mkv_mpeg4'
  /**
   * Keypoints
   *
   * Input keypoints [x,y] to erase or keep from the video. Format like so: {'x':100, 'y':100, 'type':'positive/negative'}
   */
  keypoints: Array<string>
  /**
   * Auto Trim
   *
   * auto trim the video, to working duration ( 5s )
   */
  auto_trim?: boolean
}

/**
 * VideoOutput
 */
export type SchemaBriaVideoEraserEraseMaskOutput = {
  /**
   * Video
   *
   * Final video.
   */
  video: SchemaVideo | SchemaFile
}

/**
 * EraseInputModel
 */
export type SchemaBriaVideoEraserEraseMaskInput = {
  /**
   * Preserve Audio
   *
   * If true, audio will be preserved in the output video.
   */
  preserve_audio?: boolean
  /**
   * Video Url
   *
   * Input video to erase object from. duration must be less than 5s.
   */
  video_url: string
  /**
   * Output Container And Codec
   *
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.
   */
  output_container_and_codec?:
    | 'mp4_h265'
    | 'mp4_h264'
    | 'webm_vp9'
    | 'gif'
    | 'mov_h264'
    | 'mov_h265'
    | 'mov_proresks'
    | 'mkv_h264'
    | 'mkv_h265'
    | 'mkv_vp9'
    | 'mkv_mpeg4'
  /**
   * Mask Video Url
   *
   * Input video to mask erase object from. duration must be less than 5s.
   */
  mask_video_url: string
  /**
   * Auto Trim
   *
   * auto trim the video, to working duration ( 5s )
   */
  auto_trim?: boolean
}

/**
 * CrystalVideoUpscaleOutput
 */
export type SchemaCrystalVideoUpscalerOutput = {
  /**
   * Video
   *
   * URL to the upscaled video
   */
  video: SchemaVideoFile
}

/**
 * CrystalVideoUpscaleInput
 */
export type SchemaCrystalVideoUpscalerInput = {
  /**
   * Video Url
   *
   * URL to the input video.
   */
  video_url: string
  /**
   * Scale Factor
   *
   * Scale factor. The scale factor must be chosen such that the upscaled video does not exceed 5K resolution.
   */
  scale_factor?: number
}

/**
 * ScailResponse
 */
export type SchemaScailOutput = {
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * ScailRequest
 */
export type SchemaScailInput = {
  /**
   * Prompt
   *
   * The prompt to guide video generation.
   */
  prompt: string
  /**
   * Video Url
   *
   * The URL of the video to use as a reference for the video generation.
   */
  video_url: string
  /**
   * Resolution
   *
   * Output resolution. Outputs 896x512 (landscape) or 512x896 (portrait) based on the input image aspect ratio.
   */
  resolution?: '512p'
  /**
   * Num Inference Steps
   *
   * The number of inference steps to use for the video generation.
   */
  num_inference_steps?: number
  /**
   * Multi Character
   *
   * Enable multi-character mode. Use when driving video has multiple people.
   */
  multi_character?: boolean
  /**
   * Image Url
   *
   * The URL of the image to use as a reference for the video generation.
   */
  image_url: string
}

/**
 * LucyRestyleOutput
 */
export type SchemaLucyRestyleOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: SchemaFile
}

/**
 * LucyRestyleInput
 */
export type SchemaLucyRestyleInput = {
  /**
   * Sync Mode
   *
   *
   * If set to true, the function will wait for the video to be generated
   * and uploaded before returning the response. This will increase the
   * latency of the function but it allows you to get the video directly
   * in the response without going through the CDN.
   *
   */
  sync_mode?: boolean
  /**
   * Video Url
   *
   * URL of the video to edit
   */
  video_url: string
  /**
   * Resolution
   *
   * Resolution of the generated video
   */
  resolution?: '720p'
  /**
   * Prompt
   *
   * Text description of the desired video content
   */
  prompt: string
  /**
   * Seed
   *
   * Seed for video generation
   */
  seed?: number
  /**
   * Enhance Prompt
   *
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean
}

/**
 * MotionControlOutput
 *
 * Output model for motion control video generation.
 */
export type SchemaKlingVideoV26ProMotionControlOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: SchemaFile
}

/**
 * MotionControlRequest
 *
 * Request model for motion control video generation.
 */
export type SchemaKlingVideoV26ProMotionControlInput = {
  /**
   * Prompt
   */
  prompt?: string
  /**
   * Video Url
   *
   * Reference video URL. The character actions in the generated video will be consistent with this reference video. Should contain a realistic style character with entire body or upper body visible, including head, without obstruction. Duration limit depends on character_orientation: 10s max for 'image', 30s max for 'video'.
   */
  video_url: string
  /**
   * Character Orientation
   *
   * Controls whether the output character's orientation matches the reference image or video. 'video': orientation matches reference video - better for complex motions (max 30s). 'image': orientation matches reference image - better for following camera movements (max 10s).
   */
  character_orientation: 'image' | 'video'
  /**
   * Keep Original Sound
   *
   * Whether to keep the original sound from the reference video.
   */
  keep_original_sound?: boolean
  /**
   * Image Url
   *
   * Reference image URL. The characters, backgrounds, and other elements in the generated video are based on this reference image. Characters should have clear body proportions, avoid occlusion, and occupy more than 5% of the image area.
   */
  image_url: string
}

/**
 * MotionControlOutput
 *
 * Output model for motion control video generation.
 */
export type SchemaKlingVideoV26StandardMotionControlOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: SchemaFile
}

/**
 * MotionControlRequest
 *
 * Request model for motion control video generation.
 */
export type SchemaKlingVideoV26StandardMotionControlInput = {
  /**
   * Prompt
   */
  prompt?: string
  /**
   * Video Url
   *
   * Reference video URL. The character actions in the generated video will be consistent with this reference video. Should contain a realistic style character with entire body or upper body visible, including head, without obstruction. Duration limit depends on character_orientation: 10s max for 'image', 30s max for 'video'.
   */
  video_url: string
  /**
   * Character Orientation
   *
   * Controls whether the output character's orientation matches the reference image or video. 'video': orientation matches reference video - better for complex motions (max 30s). 'image': orientation matches reference image - better for following camera movements (max 10s).
   */
  character_orientation: 'image' | 'video'
  /**
   * Keep Original Sound
   *
   * Whether to keep the original sound from the reference video.
   */
  keep_original_sound?: boolean
  /**
   * Image Url
   *
   * Reference image URL. The characters, backgrounds, and other elements in the generated video are based on this reference image. Characters should have clear body proportions, avoid occlusion, and occupy more than 5% of the image area.
   */
  image_url: string
}

/**
 * TrajectoryParameters
 *
 * Camera trajectory parameters for re-camera operations.
 *
 * Each list represents interpolation values across frames:
 * - theta: Horizontal rotation angles (degrees)
 * - phi: Vertical rotation angles (degrees)
 * - radius: Camera distance scaling factors
 */
export type SchemaTrajectoryParameters = {
  /**
   * Theta
   *
   * Horizontal rotation angles (degrees) for each keyframe.
   */
  theta: Array<number>
  /**
   * Radius
   *
   * Camera distance scaling factors for each keyframe.
   */
  radius: Array<number>
  /**
   * Phi
   *
   * Vertical rotation angles (degrees) for each keyframe.
   */
  phi: Array<number>
}

/**
 * LightXOutput
 */
export type SchemaLightxRecameraOutput = {
  /**
   * Viz Video
   *
   * Optional: visualization/debug video (if produced by the pipeline).
   */
  viz_video?: SchemaFile
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Input Video
   *
   * Optional: normalized/processed input video (if produced by the pipeline).
   */
  input_video?: SchemaFile
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * LightXRecameraRequest
 *
 * Re-camera-only request (minimal schema).
 */
export type SchemaLightxRecameraInput = {
  /**
   * Prompt
   *
   * Optional text prompt. If omitted, Light-X will auto-caption the video.
   */
  prompt?: string
  /**
   * Trajectory
   *
   * Camera trajectory parameters (required for recamera mode).
   */
  trajectory?: SchemaTrajectoryParameters
  /**
   * Video Url
   *
   * URL of the input video.
   */
  video_url: string
  /**
   * Camera
   *
   * Camera control mode.
   */
  camera?: 'traj' | 'target'
  /**
   * Target Pose
   *
   * Target camera pose [theta, phi, radius, x, y] (required when camera='target').
   */
  target_pose?: Array<number>
  /**
   * Mode
   *
   * Camera motion mode.
   */
  mode?: 'gradual' | 'bullet' | 'direct' | 'dolly-zoom'
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
}

/**
 * RelightParameters
 *
 * Relighting parameters for video relighting operations.
 *
 * Used with relight_condition_type 'ic' (intrinsic conditioning).
 */
export type SchemaRelightParameters = {
  /**
   * Relight Prompt
   *
   * Text prompt describing the desired lighting condition.
   */
  relight_prompt: string
  /**
   * Bg Source
   *
   * Direction of the light source (used for IC-light).
   */
  bg_source?: 'Left' | 'Right' | 'Top' | 'Bottom'
  /**
   * Use Sky Mask
   *
   * Whether to use sky masking for outdoor scenes.
   */
  use_sky_mask?: boolean
  /**
   * Cfg
   *
   * Classifier-free guidance scale for relighting.
   */
  cfg?: number
}

/**
 * LightXOutput
 */
export type SchemaLightxRelightOutput = {
  /**
   * Viz Video
   *
   * Optional: visualization/debug video (if produced by the pipeline).
   */
  viz_video?: SchemaFile
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Input Video
   *
   * Optional: normalized/processed input video (if produced by the pipeline).
   */
  input_video?: SchemaFile
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * LightXRelightRequest
 *
 * Relighting-only request (minimal schema).
 */
export type SchemaLightxRelightInput = {
  /**
   * Prompt
   *
   * Optional text prompt. If omitted, Light-X will auto-caption the video.
   */
  prompt?: string
  /**
   * Video Url
   *
   * URL of the input video.
   */
  video_url: string
  /**
   * Relight Parameters
   *
   * Relighting parameters (required for relight_condition_type='ic'). Not used for 'bg' (which expects a background image URL instead).
   */
  relight_parameters?: SchemaRelightParameters
  /**
   * Ref Id
   *
   * Frame index to use as referencen to relight the video with reference.
   */
  ref_id?: number
  /**
   * Relit Cond Img Url
   *
   * URL of conditioning image. Required for relight_condition_type='ref'/'hdr'. Also required for relight_condition_type='bg' (background image).
   */
  relit_cond_img_url?: string
  /**
   * Relit Cond Type
   *
   * Relight condition type.
   */
  relit_cond_type?: 'ic' | 'ref' | 'hdr' | 'bg'
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
}

/**
 * VideoOutput
 */
export type SchemaVideoEraseMaskOutput = {
  /**
   * Video
   *
   * Final video.
   */
  video: SchemaVideo | SchemaFile
}

/**
 * EraseInputModel
 */
export type SchemaVideoEraseMaskInput = {
  /**
   * Preserve Audio
   *
   * If true, audio will be preserved in the output video.
   */
  preserve_audio?: boolean
  /**
   * Video Url
   *
   * Input video to erase object from. duration must be less than 5s.
   */
  video_url: string
  /**
   * Output Container And Codec
   *
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.
   */
  output_container_and_codec?:
    | 'mp4_h265'
    | 'mp4_h264'
    | 'webm_vp9'
    | 'gif'
    | 'mov_h264'
    | 'mov_h265'
    | 'mov_proresks'
    | 'mkv_h264'
    | 'mkv_h265'
    | 'mkv_vp9'
    | 'mkv_mpeg4'
  /**
   * Mask Video Url
   *
   * Input video to mask erase object from. duration must be less than 5s.
   */
  mask_video_url: string
  /**
   * Auto Trim
   *
   * auto trim the video, to working duration ( 5s )
   */
  auto_trim?: boolean
}

/**
 * VideoOutput
 */
export type SchemaVideoErasePromptOutput = {
  /**
   * Video
   *
   * Final video.
   */
  video: SchemaVideo | SchemaFile
}

/**
 * EraseByPromptInputModel
 */
export type SchemaVideoErasePromptInput = {
  /**
   * Preserve Audio
   *
   * If true, audio will be preserved in the output video.
   */
  preserve_audio?: boolean
  /**
   * Video Url
   *
   * Input video to erase object from. duration must be less than 5s.
   */
  video_url: string
  /**
   * Prompt
   *
   * Input prompt to detect object to erase
   */
  prompt: string
  /**
   * Output Container And Codec
   *
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.
   */
  output_container_and_codec?:
    | 'mp4_h265'
    | 'mp4_h264'
    | 'webm_vp9'
    | 'gif'
    | 'mov_h264'
    | 'mov_h265'
    | 'mov_proresks'
    | 'mkv_h264'
    | 'mkv_h265'
    | 'mkv_vp9'
    | 'mkv_mpeg4'
  /**
   * Auto Trim
   *
   * auto trim the video, to working duration ( 5s )
   */
  auto_trim?: boolean
}

/**
 * VideoOutput
 */
export type SchemaVideoEraseKeypointsOutput = {
  /**
   * Video
   *
   * Final video.
   */
  video: SchemaVideo | SchemaFile
}

/**
 * EraseByKeyPointsInputModel
 */
export type SchemaVideoEraseKeypointsInput = {
  /**
   * Preserve Audio
   *
   * If true, audio will be preserved in the output video.
   */
  preserve_audio?: boolean
  /**
   * Video Url
   *
   * Input video to erase object from. duration must be less than 5s.
   */
  video_url: string
  /**
   * Output Container And Codec
   *
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.
   */
  output_container_and_codec?:
    | 'mp4_h265'
    | 'mp4_h264'
    | 'webm_vp9'
    | 'gif'
    | 'mov_h264'
    | 'mov_h265'
    | 'mov_proresks'
    | 'mkv_h264'
    | 'mkv_h265'
    | 'mkv_vp9'
    | 'mkv_mpeg4'
  /**
   * Keypoints
   *
   * Input keypoints [x,y] to erase or keep from the video. Format like so: {'x':100, 'y':100, 'type':'positive/negative'}
   */
  keypoints: Array<string>
  /**
   * Auto Trim
   *
   * auto trim the video, to working duration ( 5s )
   */
  auto_trim?: boolean
}

/**
 * LTX2ExtendVideoOutput
 */
export type SchemaLtx219bExtendVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * LTX2ExtendVideoInput
 *
 * extend_direction: ExtendDirection = Field(
 * description="Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning.",
 * default="forward",
 * ui={"important": True},
 * title="Extend Direction",
 * )
 */
export type SchemaLtx219bExtendVideoInput = {
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean
  /**
   * Video URL
   *
   * The URL of the video to extend.
   */
  video_url: string
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: 'none' | 'regular' | 'high' | 'full'
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | 'dolly_in'
    | 'dolly_out'
    | 'dolly_left'
    | 'dolly_right'
    | 'jib_up'
    | 'jib_down'
    | 'static'
    | 'none'
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | SchemaImageSize
    | 'auto'
    | 'square_hd'
    | 'square'
    | 'portrait_4_3'
    | 'portrait_16_9'
    | 'landscape_4_3'
    | 'landscape_16_9'
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number
  /**
   * Guidance Scale
   *
   * The guidance scale to use.
   */
  guidance_scale?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number
  /**
   * Video Strength
   *
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
   */
  video_strength?: number
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | 'X264 (.mp4)'
    | 'VP9 (.webm)'
    | 'PRORES4444 (.mov)'
    | 'GIF (.gif)'
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Number of Context Frames
   *
   * The number of frames to use as context for the extension.
   */
  num_context_frames?: number
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number
  /**
   * Audio Strength
   *
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.
   */
  audio_strength?: number
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown
  /**
   * Match Input FPS
   *
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS.
   */
  match_input_fps?: boolean
}

/**
 * LTX2ExtendVideoOutput
 */
export type SchemaLtx219bExtendVideoLoraOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * LTX2LoRAExtendVideoInput
 */
export type SchemaLtx219bExtendVideoLoraInput = {
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean
  /**
   * Video URL
   *
   * The URL of the video to extend.
   */
  video_url: string
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: 'none' | 'regular' | 'high' | 'full'
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number
  /**
   * LoRAs
   *
   * The LoRAs to use for the generation.
   */
  loras: Array<SchemaLoRaInput>
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | 'dolly_in'
    | 'dolly_out'
    | 'dolly_left'
    | 'dolly_right'
    | 'jib_up'
    | 'jib_down'
    | 'static'
    | 'none'
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | SchemaImageSize
    | 'auto'
    | 'square_hd'
    | 'square'
    | 'portrait_4_3'
    | 'portrait_16_9'
    | 'landscape_4_3'
    | 'landscape_16_9'
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number
  /**
   * Guidance Scale
   *
   * The guidance scale to use.
   */
  guidance_scale?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number
  /**
   * Video Strength
   *
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
   */
  video_strength?: number
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | 'X264 (.mp4)'
    | 'VP9 (.webm)'
    | 'PRORES4444 (.mov)'
    | 'GIF (.gif)'
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Number of Context Frames
   *
   * The number of frames to use as context for the extension.
   */
  num_context_frames?: number
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number
  /**
   * Audio Strength
   *
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.
   */
  audio_strength?: number
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown
  /**
   * Match Input FPS
   *
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS.
   */
  match_input_fps?: boolean
}

/**
 * LoRAInput
 *
 * LoRA weight configuration.
 */
export type SchemaLoRaInput = {
  /**
   * Path
   *
   * URL, HuggingFace repo ID (owner/repo) to lora weights.
   */
  path: string
  /**
   * Scale
   *
   * Scale factor for LoRA application (0.0 to 4.0).
   */
  scale?: number
  /**
   * Weight Name
   *
   * Name of the LoRA weight. Only used if `path` is a HuggingFace repository, and is only required when the repository contains multiple LoRA weights.
   */
  weight_name?: string | unknown
}

/**
 * LTX2ExtendVideoOutput
 */
export type SchemaLtx219bDistilledExtendVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * LTX2DistilledExtendVideoInput
 */
export type SchemaLtx219bDistilledExtendVideoInput = {
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean
  /**
   * Video URL
   *
   * The URL of the video to extend.
   */
  video_url: string
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: 'none' | 'regular' | 'high' | 'full'
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | 'dolly_in'
    | 'dolly_out'
    | 'dolly_left'
    | 'dolly_right'
    | 'jib_up'
    | 'jib_down'
    | 'static'
    | 'none'
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | SchemaImageSize
    | 'auto'
    | 'square_hd'
    | 'square'
    | 'portrait_4_3'
    | 'portrait_16_9'
    | 'landscape_4_3'
    | 'landscape_16_9'
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string
  /**
   * Video Strength
   *
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
   */
  video_strength?: number
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | 'X264 (.mp4)'
    | 'VP9 (.webm)'
    | 'PRORES4444 (.mov)'
    | 'GIF (.gif)'
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Number of Context Frames
   *
   * The number of frames to use as context for the extension.
   */
  num_context_frames?: number
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Audio Strength
   *
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.
   */
  audio_strength?: number
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown
  /**
   * Match Input FPS
   *
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS.
   */
  match_input_fps?: boolean
}

/**
 * LTX2ExtendVideoOutput
 */
export type SchemaLtx219bDistilledExtendVideoLoraOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * LTX2LoRADistilledExtendVideoInput
 */
export type SchemaLtx219bDistilledExtendVideoLoraInput = {
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean
  /**
   * Video URL
   *
   * The URL of the video to extend.
   */
  video_url: string
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: 'none' | 'regular' | 'high' | 'full'
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number
  /**
   * LoRAs
   *
   * The LoRAs to use for the generation.
   */
  loras: Array<SchemaLoRaInput>
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | 'dolly_in'
    | 'dolly_out'
    | 'dolly_left'
    | 'dolly_right'
    | 'jib_up'
    | 'jib_down'
    | 'static'
    | 'none'
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | SchemaImageSize
    | 'auto'
    | 'square_hd'
    | 'square'
    | 'portrait_4_3'
    | 'portrait_16_9'
    | 'landscape_4_3'
    | 'landscape_16_9'
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string
  /**
   * Video Strength
   *
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
   */
  video_strength?: number
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | 'X264 (.mp4)'
    | 'VP9 (.webm)'
    | 'PRORES4444 (.mov)'
    | 'GIF (.gif)'
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Number of Context Frames
   *
   * The number of frames to use as context for the extension.
   */
  num_context_frames?: number
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Audio Strength
   *
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.
   */
  audio_strength?: number
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown
  /**
   * Match Input FPS
   *
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS.
   */
  match_input_fps?: boolean
}

/**
 * LTX2VideoToVideoOutput
 */
export type SchemaLtx219bVideoToVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * LTX2VideoToVideoInput
 */
export type SchemaLtx219bVideoToVideoInput = {
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean
  /**
   * Video URL
   *
   * The URL of the video to generate the video from.
   */
  video_url: string
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean
  /**
   * IC-LoRA Scale
   *
   * The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA.
   */
  ic_lora_scale?: number
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | SchemaImageSize
    | 'auto'
    | 'square_hd'
    | 'square'
    | 'portrait_4_3'
    | 'portrait_16_9'
    | 'landscape_4_3'
    | 'landscape_16_9'
  /**
   * Guidance Scale
   *
   * The guidance scale to use.
   */
  guidance_scale?: number
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number
  /**
   * Video Strength
   *
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
   */
  video_strength?: number
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | 'X264 (.mp4)'
    | 'VP9 (.webm)'
    | 'PRORES4444 (.mov)'
    | 'GIF (.gif)'
  /**
   * Image URL
   *
   * An optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | unknown
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown
  /**
   * Match Video Length
   *
   * When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames.
   */
  match_video_length?: boolean
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: 'none' | 'regular' | 'high' | 'full'
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | 'dolly_in'
    | 'dolly_out'
    | 'dolly_left'
    | 'dolly_right'
    | 'jib_up'
    | 'jib_down'
    | 'static'
    | 'none'
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Image Strength
   *
   * The strength of the image to use for the video generation.
   */
  image_strength?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string
  /**
   * Preprocessor
   *
   * The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type.
   */
  preprocessor?: 'depth' | 'canny' | 'pose' | 'none'
  /**
   * IC-LoRA
   *
   * The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)
   */
  ic_lora?:
    | 'match_preprocessor'
    | 'canny'
    | 'depth'
    | 'pose'
    | 'detailer'
    | 'none'
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number
  /**
   * Match Input FPS
   *
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS.
   */
  match_input_fps?: boolean
}

/**
 * LTX2VideoToVideoOutput
 */
export type SchemaLtx219bVideoToVideoLoraOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * LTX2LoRAVideoToVideoInput
 */
export type SchemaLtx219bVideoToVideoLoraInput = {
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean
  /**
   * Video URL
   *
   * The URL of the video to generate the video from.
   */
  video_url: string
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * IC-LoRA Scale
   *
   * The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA.
   */
  ic_lora_scale?: number
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean
  /**
   * LoRAs
   *
   * The LoRAs to use for the generation.
   */
  loras: Array<SchemaLoRaInput>
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | SchemaImageSize
    | 'auto'
    | 'square_hd'
    | 'square'
    | 'portrait_4_3'
    | 'portrait_16_9'
    | 'landscape_4_3'
    | 'landscape_16_9'
  /**
   * Guidance Scale
   *
   * The guidance scale to use.
   */
  guidance_scale?: number
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number
  /**
   * Video Strength
   *
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
   */
  video_strength?: number
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | 'X264 (.mp4)'
    | 'VP9 (.webm)'
    | 'PRORES4444 (.mov)'
    | 'GIF (.gif)'
  /**
   * Image URL
   *
   * An optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | unknown
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown
  /**
   * Match Video Length
   *
   * When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames.
   */
  match_video_length?: boolean
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: 'none' | 'regular' | 'high' | 'full'
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | 'dolly_in'
    | 'dolly_out'
    | 'dolly_left'
    | 'dolly_right'
    | 'jib_up'
    | 'jib_down'
    | 'static'
    | 'none'
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Image Strength
   *
   * The strength of the image to use for the video generation.
   */
  image_strength?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string
  /**
   * Preprocessor
   *
   * The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type.
   */
  preprocessor?: 'depth' | 'canny' | 'pose' | 'none'
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * IC-LoRA
   *
   * The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)
   */
  ic_lora?:
    | 'match_preprocessor'
    | 'canny'
    | 'depth'
    | 'pose'
    | 'detailer'
    | 'none'
  /**
   * Match Input FPS
   *
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS.
   */
  match_input_fps?: boolean
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number
}

/**
 * LTX2VideoToVideoOutput
 */
export type SchemaLtx219bDistilledVideoToVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * LTX2DistilledVideoToVideoInput
 */
export type SchemaLtx219bDistilledVideoToVideoInput = {
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean
  /**
   * Video URL
   *
   * The URL of the video to generate the video from.
   */
  video_url: string
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: 'none' | 'regular' | 'high' | 'full'
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * IC-LoRA Scale
   *
   * The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA.
   */
  ic_lora_scale?: number
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | 'dolly_in'
    | 'dolly_out'
    | 'dolly_left'
    | 'dolly_right'
    | 'jib_up'
    | 'jib_down'
    | 'static'
    | 'none'
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | SchemaImageSize
    | 'auto'
    | 'square_hd'
    | 'square'
    | 'portrait_4_3'
    | 'portrait_16_9'
    | 'landscape_4_3'
    | 'landscape_16_9'
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number
  /**
   * Image Strength
   *
   * The strength of the image to use for the video generation.
   */
  image_strength?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string
  /**
   * Preprocessor
   *
   * The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type.
   */
  preprocessor?: 'depth' | 'canny' | 'pose' | 'none'
  /**
   * Video Strength
   *
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
   */
  video_strength?: number
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | 'X264 (.mp4)'
    | 'VP9 (.webm)'
    | 'PRORES4444 (.mov)'
    | 'GIF (.gif)'
  /**
   * IC-LoRA
   *
   * The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)
   */
  ic_lora?:
    | 'match_preprocessor'
    | 'canny'
    | 'depth'
    | 'pose'
    | 'detailer'
    | 'none'
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number
  /**
   * Image URL
   *
   * An optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | unknown
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown
  /**
   * Match Input FPS
   *
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS.
   */
  match_input_fps?: boolean
  /**
   * Match Video Length
   *
   * When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames.
   */
  match_video_length?: boolean
}

/**
 * LTX2VideoToVideoOutput
 */
export type SchemaLtx219bDistilledVideoToVideoLoraOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * LTX2LoRADistilledVideoToVideoInput
 */
export type SchemaLtx219bDistilledVideoToVideoLoraInput = {
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean
  /**
   * Video URL
   *
   * The URL of the video to generate the video from.
   */
  video_url: string
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: 'none' | 'regular' | 'high' | 'full'
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * IC-LoRA Scale
   *
   * The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA.
   */
  ic_lora_scale?: number
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number
  /**
   * LoRAs
   *
   * The LoRAs to use for the generation.
   */
  loras: Array<SchemaLoRaInput>
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | 'dolly_in'
    | 'dolly_out'
    | 'dolly_left'
    | 'dolly_right'
    | 'jib_up'
    | 'jib_down'
    | 'static'
    | 'none'
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | SchemaImageSize
    | 'auto'
    | 'square_hd'
    | 'square'
    | 'portrait_4_3'
    | 'portrait_16_9'
    | 'landscape_4_3'
    | 'landscape_16_9'
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number
  /**
   * Image Strength
   *
   * The strength of the image to use for the video generation.
   */
  image_strength?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string
  /**
   * Preprocessor
   *
   * The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type.
   */
  preprocessor?: 'depth' | 'canny' | 'pose' | 'none'
  /**
   * Video Strength
   *
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
   */
  video_strength?: number
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | 'X264 (.mp4)'
    | 'VP9 (.webm)'
    | 'PRORES4444 (.mov)'
    | 'GIF (.gif)'
  /**
   * IC-LoRA
   *
   * The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)
   */
  ic_lora?:
    | 'match_preprocessor'
    | 'canny'
    | 'depth'
    | 'pose'
    | 'detailer'
    | 'none'
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number
  /**
   * Image URL
   *
   * An optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | unknown
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown
  /**
   * Match Input FPS
   *
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS.
   */
  match_input_fps?: boolean
  /**
   * Match Video Length
   *
   * When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames.
   */
  match_video_length?: boolean
}

/**
 * FaceFusionVideoOutput
 *
 * FaceFusion output payload when video content is generated
 */
export type SchemaAiFaceSwapFaceswapvideoOutput = {
  /**
   * Processing Time Ms
   *
   * Optional processing duration in milliseconds
   */
  processing_time_ms?: number | unknown
  video: SchemaVideo
}

/**
 * FaceSwapInputVideo
 *
 * Input schema for image  video face swap
 */
export type SchemaAiFaceSwapFaceswapvideoInput = {
  /**
   * Source Face Url
   *
   * Source face image
   */
  source_face_url: string
  /**
   * Target Video Url
   *
   * Target video URL
   */
  target_video_url: string
}

/**
 * Output
 */
export type SchemaMmaudioV2Output = {
  /**
   * Video
   *
   * The generated video with the lip sync.
   */
  video: SchemaFile
}

/**
 * BaseInput
 */
export type SchemaMmaudioV2Input = {
  /**
   * Prompt
   *
   * The prompt to generate the audio for.
   */
  prompt: string
  /**
   * Video Url
   *
   * The URL of the video to generate the audio for.
   */
  video_url: string
  /**
   * Num Steps
   *
   * The number of steps to generate the audio for.
   */
  num_steps?: number
  /**
   * Duration
   *
   * The duration of the audio to generate.
   */
  duration?: number
  /**
   * Cfg Strength
   *
   * The strength of Classifier Free Guidance.
   */
  cfg_strength?: number
  /**
   * Seed
   *
   * The seed for the random number generator
   */
  seed?: number
  /**
   * Mask Away Clip
   *
   * Whether to mask away the clip.
   */
  mask_away_clip?: boolean
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the audio for.
   */
  negative_prompt?: string
}

/**
 * GeneralRembgOutput
 */
export type SchemaVideoBackgroundRemovalOutput = {
  /**
   * Video
   */
  video: Array<SchemaFile>
}

/**
 * GeneralRembgInput
 */
export type SchemaVideoBackgroundRemovalInput = {
  /**
   * Video Url
   */
  video_url: string
  /**
   * Subject Is Person
   *
   * Set to False if the subject is not a person.
   */
  subject_is_person?: boolean
  /**
   * Output Codec
   *
   * Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality.
   */
  output_codec?: 'vp9' | 'h264'
  /**
   * Refine Foreground Edges
   *
   * Improves the quality of the extracted object's edges.
   */
  refine_foreground_edges?: boolean
}

export type SchemaQueueStatus = {
  status: 'IN_QUEUE' | 'IN_PROGRESS' | 'COMPLETED'
  /**
   * The request id.
   */
  request_id: string
  /**
   * The response url.
   */
  response_url?: string
  /**
   * The status url.
   */
  status_url?: string
  /**
   * The cancel url.
   */
  cancel_url?: string
  /**
   * The logs.
   */
  logs?: {
    [key: string]: unknown
  }
  /**
   * The metrics.
   */
  metrics?: {
    [key: string]: unknown
  }
  /**
   * The queue position.
   */
  queue_position?: number
}

export type GetBriaVideoBackgroundRemovalRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/bria/video/background-removal/requests/{request_id}/status'
}

export type GetBriaVideoBackgroundRemovalRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetBriaVideoBackgroundRemovalRequestsByRequestIdStatusResponse =
  GetBriaVideoBackgroundRemovalRequestsByRequestIdStatusResponses[keyof GetBriaVideoBackgroundRemovalRequestsByRequestIdStatusResponses]

export type PutBriaVideoBackgroundRemovalRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/bria/video/background-removal/requests/{request_id}/cancel'
}

export type PutBriaVideoBackgroundRemovalRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutBriaVideoBackgroundRemovalRequestsByRequestIdCancelResponse =
  PutBriaVideoBackgroundRemovalRequestsByRequestIdCancelResponses[keyof PutBriaVideoBackgroundRemovalRequestsByRequestIdCancelResponses]

export type PostBriaVideoBackgroundRemovalData = {
  body: SchemaVideoBackgroundRemovalInput
  path?: never
  query?: never
  url: '/bria/video/background-removal'
}

export type PostBriaVideoBackgroundRemovalResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostBriaVideoBackgroundRemovalResponse =
  PostBriaVideoBackgroundRemovalResponses[keyof PostBriaVideoBackgroundRemovalResponses]

export type GetBriaVideoBackgroundRemovalRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/bria/video/background-removal/requests/{request_id}'
}

export type GetBriaVideoBackgroundRemovalRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaVideoBackgroundRemovalOutput
}

export type GetBriaVideoBackgroundRemovalRequestsByRequestIdResponse =
  GetBriaVideoBackgroundRemovalRequestsByRequestIdResponses[keyof GetBriaVideoBackgroundRemovalRequestsByRequestIdResponses]

export type GetFalAiMmaudioV2RequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/mmaudio-v2/requests/{request_id}/status'
}

export type GetFalAiMmaudioV2RequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiMmaudioV2RequestsByRequestIdStatusResponse =
  GetFalAiMmaudioV2RequestsByRequestIdStatusResponses[keyof GetFalAiMmaudioV2RequestsByRequestIdStatusResponses]

export type PutFalAiMmaudioV2RequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/mmaudio-v2/requests/{request_id}/cancel'
}

export type PutFalAiMmaudioV2RequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiMmaudioV2RequestsByRequestIdCancelResponse =
  PutFalAiMmaudioV2RequestsByRequestIdCancelResponses[keyof PutFalAiMmaudioV2RequestsByRequestIdCancelResponses]

export type PostFalAiMmaudioV2Data = {
  body: SchemaMmaudioV2Input
  path?: never
  query?: never
  url: '/fal-ai/mmaudio-v2'
}

export type PostFalAiMmaudioV2Responses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiMmaudioV2Response =
  PostFalAiMmaudioV2Responses[keyof PostFalAiMmaudioV2Responses]

export type GetFalAiMmaudioV2RequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/mmaudio-v2/requests/{request_id}'
}

export type GetFalAiMmaudioV2RequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaMmaudioV2Output
}

export type GetFalAiMmaudioV2RequestsByRequestIdResponse =
  GetFalAiMmaudioV2RequestsByRequestIdResponses[keyof GetFalAiMmaudioV2RequestsByRequestIdResponses]

export type GetHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/half-moon-ai/ai-face-swap/faceswapvideo/requests/{request_id}/status'
  }

export type GetHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdStatusResponse =
  GetHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdStatusResponses[keyof GetHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdStatusResponses]

export type PutHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/half-moon-ai/ai-face-swap/faceswapvideo/requests/{request_id}/cancel'
  }

export type PutHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdCancelResponse =
  PutHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdCancelResponses[keyof PutHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdCancelResponses]

export type PostHalfMoonAiAiFaceSwapFaceswapvideoData = {
  body: SchemaAiFaceSwapFaceswapvideoInput
  path?: never
  query?: never
  url: '/half-moon-ai/ai-face-swap/faceswapvideo'
}

export type PostHalfMoonAiAiFaceSwapFaceswapvideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostHalfMoonAiAiFaceSwapFaceswapvideoResponse =
  PostHalfMoonAiAiFaceSwapFaceswapvideoResponses[keyof PostHalfMoonAiAiFaceSwapFaceswapvideoResponses]

export type GetHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/half-moon-ai/ai-face-swap/faceswapvideo/requests/{request_id}'
}

export type GetHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaAiFaceSwapFaceswapvideoOutput
}

export type GetHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdResponse =
  GetHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdResponses[keyof GetHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdResponses]

export type GetFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/ltx-2-19b/distilled/video-to-video/lora/requests/{request_id}/status'
  }

export type GetFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdStatusResponses]

export type PutFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/ltx-2-19b/distilled/video-to-video/lora/requests/{request_id}/cancel'
  }

export type PutFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdCancelResponses]

export type PostFalAiLtx219bDistilledVideoToVideoLoraData = {
  body: SchemaLtx219bDistilledVideoToVideoLoraInput
  path?: never
  query?: never
  url: '/fal-ai/ltx-2-19b/distilled/video-to-video/lora'
}

export type PostFalAiLtx219bDistilledVideoToVideoLoraResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLtx219bDistilledVideoToVideoLoraResponse =
  PostFalAiLtx219bDistilledVideoToVideoLoraResponses[keyof PostFalAiLtx219bDistilledVideoToVideoLoraResponses]

export type GetFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-2-19b/distilled/video-to-video/lora/requests/{request_id}'
}

export type GetFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaLtx219bDistilledVideoToVideoLoraOutput
  }

export type GetFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdResponse =
  GetFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdResponses[keyof GetFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdResponses]

export type GetFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/ltx-2-19b/distilled/video-to-video/requests/{request_id}/status'
  }

export type GetFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdStatusResponses]

export type PutFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/ltx-2-19b/distilled/video-to-video/requests/{request_id}/cancel'
  }

export type PutFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdCancelResponses]

export type PostFalAiLtx219bDistilledVideoToVideoData = {
  body: SchemaLtx219bDistilledVideoToVideoInput
  path?: never
  query?: never
  url: '/fal-ai/ltx-2-19b/distilled/video-to-video'
}

export type PostFalAiLtx219bDistilledVideoToVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLtx219bDistilledVideoToVideoResponse =
  PostFalAiLtx219bDistilledVideoToVideoResponses[keyof PostFalAiLtx219bDistilledVideoToVideoResponses]

export type GetFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-2-19b/distilled/video-to-video/requests/{request_id}'
}

export type GetFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLtx219bDistilledVideoToVideoOutput
}

export type GetFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdResponse =
  GetFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdResponses[keyof GetFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdResponses]

export type GetFalAiLtx219bVideoToVideoLoraRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/ltx-2-19b/video-to-video/lora/requests/{request_id}/status'
}

export type GetFalAiLtx219bVideoToVideoLoraRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiLtx219bVideoToVideoLoraRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bVideoToVideoLoraRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bVideoToVideoLoraRequestsByRequestIdStatusResponses]

export type PutFalAiLtx219bVideoToVideoLoraRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-2-19b/video-to-video/lora/requests/{request_id}/cancel'
}

export type PutFalAiLtx219bVideoToVideoLoraRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiLtx219bVideoToVideoLoraRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bVideoToVideoLoraRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bVideoToVideoLoraRequestsByRequestIdCancelResponses]

export type PostFalAiLtx219bVideoToVideoLoraData = {
  body: SchemaLtx219bVideoToVideoLoraInput
  path?: never
  query?: never
  url: '/fal-ai/ltx-2-19b/video-to-video/lora'
}

export type PostFalAiLtx219bVideoToVideoLoraResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLtx219bVideoToVideoLoraResponse =
  PostFalAiLtx219bVideoToVideoLoraResponses[keyof PostFalAiLtx219bVideoToVideoLoraResponses]

export type GetFalAiLtx219bVideoToVideoLoraRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-2-19b/video-to-video/lora/requests/{request_id}'
}

export type GetFalAiLtx219bVideoToVideoLoraRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLtx219bVideoToVideoLoraOutput
}

export type GetFalAiLtx219bVideoToVideoLoraRequestsByRequestIdResponse =
  GetFalAiLtx219bVideoToVideoLoraRequestsByRequestIdResponses[keyof GetFalAiLtx219bVideoToVideoLoraRequestsByRequestIdResponses]

export type GetFalAiLtx219bVideoToVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/ltx-2-19b/video-to-video/requests/{request_id}/status'
}

export type GetFalAiLtx219bVideoToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiLtx219bVideoToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bVideoToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bVideoToVideoRequestsByRequestIdStatusResponses]

export type PutFalAiLtx219bVideoToVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-2-19b/video-to-video/requests/{request_id}/cancel'
}

export type PutFalAiLtx219bVideoToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiLtx219bVideoToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bVideoToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bVideoToVideoRequestsByRequestIdCancelResponses]

export type PostFalAiLtx219bVideoToVideoData = {
  body: SchemaLtx219bVideoToVideoInput
  path?: never
  query?: never
  url: '/fal-ai/ltx-2-19b/video-to-video'
}

export type PostFalAiLtx219bVideoToVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLtx219bVideoToVideoResponse =
  PostFalAiLtx219bVideoToVideoResponses[keyof PostFalAiLtx219bVideoToVideoResponses]

export type GetFalAiLtx219bVideoToVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-2-19b/video-to-video/requests/{request_id}'
}

export type GetFalAiLtx219bVideoToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLtx219bVideoToVideoOutput
}

export type GetFalAiLtx219bVideoToVideoRequestsByRequestIdResponse =
  GetFalAiLtx219bVideoToVideoRequestsByRequestIdResponses[keyof GetFalAiLtx219bVideoToVideoRequestsByRequestIdResponses]

export type GetFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/ltx-2-19b/distilled/extend-video/lora/requests/{request_id}/status'
  }

export type GetFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdStatusResponses]

export type PutFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/ltx-2-19b/distilled/extend-video/lora/requests/{request_id}/cancel'
  }

export type PutFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdCancelResponses]

export type PostFalAiLtx219bDistilledExtendVideoLoraData = {
  body: SchemaLtx219bDistilledExtendVideoLoraInput
  path?: never
  query?: never
  url: '/fal-ai/ltx-2-19b/distilled/extend-video/lora'
}

export type PostFalAiLtx219bDistilledExtendVideoLoraResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLtx219bDistilledExtendVideoLoraResponse =
  PostFalAiLtx219bDistilledExtendVideoLoraResponses[keyof PostFalAiLtx219bDistilledExtendVideoLoraResponses]

export type GetFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-2-19b/distilled/extend-video/lora/requests/{request_id}'
}

export type GetFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaLtx219bDistilledExtendVideoLoraOutput
  }

export type GetFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdResponse =
  GetFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdResponses[keyof GetFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdResponses]

export type GetFalAiLtx219bDistilledExtendVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/ltx-2-19b/distilled/extend-video/requests/{request_id}/status'
}

export type GetFalAiLtx219bDistilledExtendVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiLtx219bDistilledExtendVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bDistilledExtendVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bDistilledExtendVideoRequestsByRequestIdStatusResponses]

export type PutFalAiLtx219bDistilledExtendVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-2-19b/distilled/extend-video/requests/{request_id}/cancel'
}

export type PutFalAiLtx219bDistilledExtendVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiLtx219bDistilledExtendVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bDistilledExtendVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bDistilledExtendVideoRequestsByRequestIdCancelResponses]

export type PostFalAiLtx219bDistilledExtendVideoData = {
  body: SchemaLtx219bDistilledExtendVideoInput
  path?: never
  query?: never
  url: '/fal-ai/ltx-2-19b/distilled/extend-video'
}

export type PostFalAiLtx219bDistilledExtendVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLtx219bDistilledExtendVideoResponse =
  PostFalAiLtx219bDistilledExtendVideoResponses[keyof PostFalAiLtx219bDistilledExtendVideoResponses]

export type GetFalAiLtx219bDistilledExtendVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-2-19b/distilled/extend-video/requests/{request_id}'
}

export type GetFalAiLtx219bDistilledExtendVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLtx219bDistilledExtendVideoOutput
}

export type GetFalAiLtx219bDistilledExtendVideoRequestsByRequestIdResponse =
  GetFalAiLtx219bDistilledExtendVideoRequestsByRequestIdResponses[keyof GetFalAiLtx219bDistilledExtendVideoRequestsByRequestIdResponses]

export type GetFalAiLtx219bExtendVideoLoraRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/ltx-2-19b/extend-video/lora/requests/{request_id}/status'
}

export type GetFalAiLtx219bExtendVideoLoraRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiLtx219bExtendVideoLoraRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bExtendVideoLoraRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bExtendVideoLoraRequestsByRequestIdStatusResponses]

export type PutFalAiLtx219bExtendVideoLoraRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-2-19b/extend-video/lora/requests/{request_id}/cancel'
}

export type PutFalAiLtx219bExtendVideoLoraRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiLtx219bExtendVideoLoraRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bExtendVideoLoraRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bExtendVideoLoraRequestsByRequestIdCancelResponses]

export type PostFalAiLtx219bExtendVideoLoraData = {
  body: SchemaLtx219bExtendVideoLoraInput
  path?: never
  query?: never
  url: '/fal-ai/ltx-2-19b/extend-video/lora'
}

export type PostFalAiLtx219bExtendVideoLoraResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLtx219bExtendVideoLoraResponse =
  PostFalAiLtx219bExtendVideoLoraResponses[keyof PostFalAiLtx219bExtendVideoLoraResponses]

export type GetFalAiLtx219bExtendVideoLoraRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-2-19b/extend-video/lora/requests/{request_id}'
}

export type GetFalAiLtx219bExtendVideoLoraRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLtx219bExtendVideoLoraOutput
}

export type GetFalAiLtx219bExtendVideoLoraRequestsByRequestIdResponse =
  GetFalAiLtx219bExtendVideoLoraRequestsByRequestIdResponses[keyof GetFalAiLtx219bExtendVideoLoraRequestsByRequestIdResponses]

export type GetFalAiLtx219bExtendVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/ltx-2-19b/extend-video/requests/{request_id}/status'
}

export type GetFalAiLtx219bExtendVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiLtx219bExtendVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bExtendVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bExtendVideoRequestsByRequestIdStatusResponses]

export type PutFalAiLtx219bExtendVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-2-19b/extend-video/requests/{request_id}/cancel'
}

export type PutFalAiLtx219bExtendVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiLtx219bExtendVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bExtendVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bExtendVideoRequestsByRequestIdCancelResponses]

export type PostFalAiLtx219bExtendVideoData = {
  body: SchemaLtx219bExtendVideoInput
  path?: never
  query?: never
  url: '/fal-ai/ltx-2-19b/extend-video'
}

export type PostFalAiLtx219bExtendVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLtx219bExtendVideoResponse =
  PostFalAiLtx219bExtendVideoResponses[keyof PostFalAiLtx219bExtendVideoResponses]

export type GetFalAiLtx219bExtendVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-2-19b/extend-video/requests/{request_id}'
}

export type GetFalAiLtx219bExtendVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLtx219bExtendVideoOutput
}

export type GetFalAiLtx219bExtendVideoRequestsByRequestIdResponse =
  GetFalAiLtx219bExtendVideoRequestsByRequestIdResponses[keyof GetFalAiLtx219bExtendVideoRequestsByRequestIdResponses]

export type GetBriaVideoEraseKeypointsRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/bria/video/erase/keypoints/requests/{request_id}/status'
}

export type GetBriaVideoEraseKeypointsRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetBriaVideoEraseKeypointsRequestsByRequestIdStatusResponse =
  GetBriaVideoEraseKeypointsRequestsByRequestIdStatusResponses[keyof GetBriaVideoEraseKeypointsRequestsByRequestIdStatusResponses]

export type PutBriaVideoEraseKeypointsRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/bria/video/erase/keypoints/requests/{request_id}/cancel'
}

export type PutBriaVideoEraseKeypointsRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutBriaVideoEraseKeypointsRequestsByRequestIdCancelResponse =
  PutBriaVideoEraseKeypointsRequestsByRequestIdCancelResponses[keyof PutBriaVideoEraseKeypointsRequestsByRequestIdCancelResponses]

export type PostBriaVideoEraseKeypointsData = {
  body: SchemaVideoEraseKeypointsInput
  path?: never
  query?: never
  url: '/bria/video/erase/keypoints'
}

export type PostBriaVideoEraseKeypointsResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostBriaVideoEraseKeypointsResponse =
  PostBriaVideoEraseKeypointsResponses[keyof PostBriaVideoEraseKeypointsResponses]

export type GetBriaVideoEraseKeypointsRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/bria/video/erase/keypoints/requests/{request_id}'
}

export type GetBriaVideoEraseKeypointsRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaVideoEraseKeypointsOutput
}

export type GetBriaVideoEraseKeypointsRequestsByRequestIdResponse =
  GetBriaVideoEraseKeypointsRequestsByRequestIdResponses[keyof GetBriaVideoEraseKeypointsRequestsByRequestIdResponses]

export type GetBriaVideoErasePromptRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/bria/video/erase/prompt/requests/{request_id}/status'
}

export type GetBriaVideoErasePromptRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetBriaVideoErasePromptRequestsByRequestIdStatusResponse =
  GetBriaVideoErasePromptRequestsByRequestIdStatusResponses[keyof GetBriaVideoErasePromptRequestsByRequestIdStatusResponses]

export type PutBriaVideoErasePromptRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/bria/video/erase/prompt/requests/{request_id}/cancel'
}

export type PutBriaVideoErasePromptRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutBriaVideoErasePromptRequestsByRequestIdCancelResponse =
  PutBriaVideoErasePromptRequestsByRequestIdCancelResponses[keyof PutBriaVideoErasePromptRequestsByRequestIdCancelResponses]

export type PostBriaVideoErasePromptData = {
  body: SchemaVideoErasePromptInput
  path?: never
  query?: never
  url: '/bria/video/erase/prompt'
}

export type PostBriaVideoErasePromptResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostBriaVideoErasePromptResponse =
  PostBriaVideoErasePromptResponses[keyof PostBriaVideoErasePromptResponses]

export type GetBriaVideoErasePromptRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/bria/video/erase/prompt/requests/{request_id}'
}

export type GetBriaVideoErasePromptRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaVideoErasePromptOutput
}

export type GetBriaVideoErasePromptRequestsByRequestIdResponse =
  GetBriaVideoErasePromptRequestsByRequestIdResponses[keyof GetBriaVideoErasePromptRequestsByRequestIdResponses]

export type GetBriaVideoEraseMaskRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/bria/video/erase/mask/requests/{request_id}/status'
}

export type GetBriaVideoEraseMaskRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetBriaVideoEraseMaskRequestsByRequestIdStatusResponse =
  GetBriaVideoEraseMaskRequestsByRequestIdStatusResponses[keyof GetBriaVideoEraseMaskRequestsByRequestIdStatusResponses]

export type PutBriaVideoEraseMaskRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/bria/video/erase/mask/requests/{request_id}/cancel'
}

export type PutBriaVideoEraseMaskRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutBriaVideoEraseMaskRequestsByRequestIdCancelResponse =
  PutBriaVideoEraseMaskRequestsByRequestIdCancelResponses[keyof PutBriaVideoEraseMaskRequestsByRequestIdCancelResponses]

export type PostBriaVideoEraseMaskData = {
  body: SchemaVideoEraseMaskInput
  path?: never
  query?: never
  url: '/bria/video/erase/mask'
}

export type PostBriaVideoEraseMaskResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostBriaVideoEraseMaskResponse =
  PostBriaVideoEraseMaskResponses[keyof PostBriaVideoEraseMaskResponses]

export type GetBriaVideoEraseMaskRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/bria/video/erase/mask/requests/{request_id}'
}

export type GetBriaVideoEraseMaskRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaVideoEraseMaskOutput
}

export type GetBriaVideoEraseMaskRequestsByRequestIdResponse =
  GetBriaVideoEraseMaskRequestsByRequestIdResponses[keyof GetBriaVideoEraseMaskRequestsByRequestIdResponses]

export type GetFalAiLightxRelightRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/lightx/relight/requests/{request_id}/status'
}

export type GetFalAiLightxRelightRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiLightxRelightRequestsByRequestIdStatusResponse =
  GetFalAiLightxRelightRequestsByRequestIdStatusResponses[keyof GetFalAiLightxRelightRequestsByRequestIdStatusResponses]

export type PutFalAiLightxRelightRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/lightx/relight/requests/{request_id}/cancel'
}

export type PutFalAiLightxRelightRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiLightxRelightRequestsByRequestIdCancelResponse =
  PutFalAiLightxRelightRequestsByRequestIdCancelResponses[keyof PutFalAiLightxRelightRequestsByRequestIdCancelResponses]

export type PostFalAiLightxRelightData = {
  body: SchemaLightxRelightInput
  path?: never
  query?: never
  url: '/fal-ai/lightx/relight'
}

export type PostFalAiLightxRelightResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLightxRelightResponse =
  PostFalAiLightxRelightResponses[keyof PostFalAiLightxRelightResponses]

export type GetFalAiLightxRelightRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/lightx/relight/requests/{request_id}'
}

export type GetFalAiLightxRelightRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLightxRelightOutput
}

export type GetFalAiLightxRelightRequestsByRequestIdResponse =
  GetFalAiLightxRelightRequestsByRequestIdResponses[keyof GetFalAiLightxRelightRequestsByRequestIdResponses]

export type GetFalAiLightxRecameraRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/lightx/recamera/requests/{request_id}/status'
}

export type GetFalAiLightxRecameraRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiLightxRecameraRequestsByRequestIdStatusResponse =
  GetFalAiLightxRecameraRequestsByRequestIdStatusResponses[keyof GetFalAiLightxRecameraRequestsByRequestIdStatusResponses]

export type PutFalAiLightxRecameraRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/lightx/recamera/requests/{request_id}/cancel'
}

export type PutFalAiLightxRecameraRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiLightxRecameraRequestsByRequestIdCancelResponse =
  PutFalAiLightxRecameraRequestsByRequestIdCancelResponses[keyof PutFalAiLightxRecameraRequestsByRequestIdCancelResponses]

export type PostFalAiLightxRecameraData = {
  body: SchemaLightxRecameraInput
  path?: never
  query?: never
  url: '/fal-ai/lightx/recamera'
}

export type PostFalAiLightxRecameraResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLightxRecameraResponse =
  PostFalAiLightxRecameraResponses[keyof PostFalAiLightxRecameraResponses]

export type GetFalAiLightxRecameraRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/lightx/recamera/requests/{request_id}'
}

export type GetFalAiLightxRecameraRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLightxRecameraOutput
}

export type GetFalAiLightxRecameraRequestsByRequestIdResponse =
  GetFalAiLightxRecameraRequestsByRequestIdResponses[keyof GetFalAiLightxRecameraRequestsByRequestIdResponses]

export type GetFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/kling-video/v2.6/standard/motion-control/requests/{request_id}/status'
  }

export type GetFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdStatusResponses]

export type PutFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/kling-video/v2.6/standard/motion-control/requests/{request_id}/cancel'
  }

export type PutFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdCancelResponses]

export type PostFalAiKlingVideoV26StandardMotionControlData = {
  body: SchemaKlingVideoV26StandardMotionControlInput
  path?: never
  query?: never
  url: '/fal-ai/kling-video/v2.6/standard/motion-control'
}

export type PostFalAiKlingVideoV26StandardMotionControlResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiKlingVideoV26StandardMotionControlResponse =
  PostFalAiKlingVideoV26StandardMotionControlResponses[keyof PostFalAiKlingVideoV26StandardMotionControlResponses]

export type GetFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/kling-video/v2.6/standard/motion-control/requests/{request_id}'
  }

export type GetFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaKlingVideoV26StandardMotionControlOutput
  }

export type GetFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdResponse =
  GetFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdResponses]

export type GetFalAiKlingVideoV26ProMotionControlRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/kling-video/v2.6/pro/motion-control/requests/{request_id}/status'
  }

export type GetFalAiKlingVideoV26ProMotionControlRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiKlingVideoV26ProMotionControlRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV26ProMotionControlRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV26ProMotionControlRequestsByRequestIdStatusResponses]

export type PutFalAiKlingVideoV26ProMotionControlRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/kling-video/v2.6/pro/motion-control/requests/{request_id}/cancel'
  }

export type PutFalAiKlingVideoV26ProMotionControlRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiKlingVideoV26ProMotionControlRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV26ProMotionControlRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV26ProMotionControlRequestsByRequestIdCancelResponses]

export type PostFalAiKlingVideoV26ProMotionControlData = {
  body: SchemaKlingVideoV26ProMotionControlInput
  path?: never
  query?: never
  url: '/fal-ai/kling-video/v2.6/pro/motion-control'
}

export type PostFalAiKlingVideoV26ProMotionControlResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiKlingVideoV26ProMotionControlResponse =
  PostFalAiKlingVideoV26ProMotionControlResponses[keyof PostFalAiKlingVideoV26ProMotionControlResponses]

export type GetFalAiKlingVideoV26ProMotionControlRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/kling-video/v2.6/pro/motion-control/requests/{request_id}'
}

export type GetFalAiKlingVideoV26ProMotionControlRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaKlingVideoV26ProMotionControlOutput
  }

export type GetFalAiKlingVideoV26ProMotionControlRequestsByRequestIdResponse =
  GetFalAiKlingVideoV26ProMotionControlRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV26ProMotionControlRequestsByRequestIdResponses]

export type GetDecartLucyRestyleRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/decart/lucy-restyle/requests/{request_id}/status'
}

export type GetDecartLucyRestyleRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetDecartLucyRestyleRequestsByRequestIdStatusResponse =
  GetDecartLucyRestyleRequestsByRequestIdStatusResponses[keyof GetDecartLucyRestyleRequestsByRequestIdStatusResponses]

export type PutDecartLucyRestyleRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/decart/lucy-restyle/requests/{request_id}/cancel'
}

export type PutDecartLucyRestyleRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutDecartLucyRestyleRequestsByRequestIdCancelResponse =
  PutDecartLucyRestyleRequestsByRequestIdCancelResponses[keyof PutDecartLucyRestyleRequestsByRequestIdCancelResponses]

export type PostDecartLucyRestyleData = {
  body: SchemaLucyRestyleInput
  path?: never
  query?: never
  url: '/decart/lucy-restyle'
}

export type PostDecartLucyRestyleResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostDecartLucyRestyleResponse =
  PostDecartLucyRestyleResponses[keyof PostDecartLucyRestyleResponses]

export type GetDecartLucyRestyleRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/decart/lucy-restyle/requests/{request_id}'
}

export type GetDecartLucyRestyleRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLucyRestyleOutput
}

export type GetDecartLucyRestyleRequestsByRequestIdResponse =
  GetDecartLucyRestyleRequestsByRequestIdResponses[keyof GetDecartLucyRestyleRequestsByRequestIdResponses]

export type GetFalAiScailRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/scail/requests/{request_id}/status'
}

export type GetFalAiScailRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiScailRequestsByRequestIdStatusResponse =
  GetFalAiScailRequestsByRequestIdStatusResponses[keyof GetFalAiScailRequestsByRequestIdStatusResponses]

export type PutFalAiScailRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/scail/requests/{request_id}/cancel'
}

export type PutFalAiScailRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiScailRequestsByRequestIdCancelResponse =
  PutFalAiScailRequestsByRequestIdCancelResponses[keyof PutFalAiScailRequestsByRequestIdCancelResponses]

export type PostFalAiScailData = {
  body: SchemaScailInput
  path?: never
  query?: never
  url: '/fal-ai/scail'
}

export type PostFalAiScailResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiScailResponse =
  PostFalAiScailResponses[keyof PostFalAiScailResponses]

export type GetFalAiScailRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/scail/requests/{request_id}'
}

export type GetFalAiScailRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaScailOutput
}

export type GetFalAiScailRequestsByRequestIdResponse =
  GetFalAiScailRequestsByRequestIdResponses[keyof GetFalAiScailRequestsByRequestIdResponses]

export type GetClarityaiCrystalVideoUpscalerRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/clarityai/crystal-video-upscaler/requests/{request_id}/status'
}

export type GetClarityaiCrystalVideoUpscalerRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetClarityaiCrystalVideoUpscalerRequestsByRequestIdStatusResponse =
  GetClarityaiCrystalVideoUpscalerRequestsByRequestIdStatusResponses[keyof GetClarityaiCrystalVideoUpscalerRequestsByRequestIdStatusResponses]

export type PutClarityaiCrystalVideoUpscalerRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/clarityai/crystal-video-upscaler/requests/{request_id}/cancel'
}

export type PutClarityaiCrystalVideoUpscalerRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutClarityaiCrystalVideoUpscalerRequestsByRequestIdCancelResponse =
  PutClarityaiCrystalVideoUpscalerRequestsByRequestIdCancelResponses[keyof PutClarityaiCrystalVideoUpscalerRequestsByRequestIdCancelResponses]

export type PostClarityaiCrystalVideoUpscalerData = {
  body: SchemaCrystalVideoUpscalerInput
  path?: never
  query?: never
  url: '/clarityai/crystal-video-upscaler'
}

export type PostClarityaiCrystalVideoUpscalerResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostClarityaiCrystalVideoUpscalerResponse =
  PostClarityaiCrystalVideoUpscalerResponses[keyof PostClarityaiCrystalVideoUpscalerResponses]

export type GetClarityaiCrystalVideoUpscalerRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/clarityai/crystal-video-upscaler/requests/{request_id}'
}

export type GetClarityaiCrystalVideoUpscalerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaCrystalVideoUpscalerOutput
}

export type GetClarityaiCrystalVideoUpscalerRequestsByRequestIdResponse =
  GetClarityaiCrystalVideoUpscalerRequestsByRequestIdResponses[keyof GetClarityaiCrystalVideoUpscalerRequestsByRequestIdResponses]

export type GetBriaBriaVideoEraserEraseMaskRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/bria/bria_video_eraser/erase/mask/requests/{request_id}/status'
}

export type GetBriaBriaVideoEraserEraseMaskRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetBriaBriaVideoEraserEraseMaskRequestsByRequestIdStatusResponse =
  GetBriaBriaVideoEraserEraseMaskRequestsByRequestIdStatusResponses[keyof GetBriaBriaVideoEraserEraseMaskRequestsByRequestIdStatusResponses]

export type PutBriaBriaVideoEraserEraseMaskRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/bria/bria_video_eraser/erase/mask/requests/{request_id}/cancel'
}

export type PutBriaBriaVideoEraserEraseMaskRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutBriaBriaVideoEraserEraseMaskRequestsByRequestIdCancelResponse =
  PutBriaBriaVideoEraserEraseMaskRequestsByRequestIdCancelResponses[keyof PutBriaBriaVideoEraserEraseMaskRequestsByRequestIdCancelResponses]

export type PostBriaBriaVideoEraserEraseMaskData = {
  body: SchemaBriaVideoEraserEraseMaskInput
  path?: never
  query?: never
  url: '/bria/bria_video_eraser/erase/mask'
}

export type PostBriaBriaVideoEraserEraseMaskResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostBriaBriaVideoEraserEraseMaskResponse =
  PostBriaBriaVideoEraserEraseMaskResponses[keyof PostBriaBriaVideoEraserEraseMaskResponses]

export type GetBriaBriaVideoEraserEraseMaskRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/bria/bria_video_eraser/erase/mask/requests/{request_id}'
}

export type GetBriaBriaVideoEraserEraseMaskRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaBriaVideoEraserEraseMaskOutput
}

export type GetBriaBriaVideoEraserEraseMaskRequestsByRequestIdResponse =
  GetBriaBriaVideoEraserEraseMaskRequestsByRequestIdResponses[keyof GetBriaBriaVideoEraserEraseMaskRequestsByRequestIdResponses]

export type GetBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/bria/bria_video_eraser/erase/keypoints/requests/{request_id}/status'
  }

export type GetBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdStatusResponse =
  GetBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdStatusResponses[keyof GetBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdStatusResponses]

export type PutBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/bria/bria_video_eraser/erase/keypoints/requests/{request_id}/cancel'
  }

export type PutBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdCancelResponse =
  PutBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdCancelResponses[keyof PutBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdCancelResponses]

export type PostBriaBriaVideoEraserEraseKeypointsData = {
  body: SchemaBriaVideoEraserEraseKeypointsInput
  path?: never
  query?: never
  url: '/bria/bria_video_eraser/erase/keypoints'
}

export type PostBriaBriaVideoEraserEraseKeypointsResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostBriaBriaVideoEraserEraseKeypointsResponse =
  PostBriaBriaVideoEraserEraseKeypointsResponses[keyof PostBriaBriaVideoEraserEraseKeypointsResponses]

export type GetBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/bria/bria_video_eraser/erase/keypoints/requests/{request_id}'
}

export type GetBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaBriaVideoEraserEraseKeypointsOutput
}

export type GetBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdResponse =
  GetBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdResponses[keyof GetBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdResponses]

export type GetBriaBriaVideoEraserErasePromptRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/bria/bria_video_eraser/erase/prompt/requests/{request_id}/status'
}

export type GetBriaBriaVideoEraserErasePromptRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetBriaBriaVideoEraserErasePromptRequestsByRequestIdStatusResponse =
  GetBriaBriaVideoEraserErasePromptRequestsByRequestIdStatusResponses[keyof GetBriaBriaVideoEraserErasePromptRequestsByRequestIdStatusResponses]

export type PutBriaBriaVideoEraserErasePromptRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/bria/bria_video_eraser/erase/prompt/requests/{request_id}/cancel'
}

export type PutBriaBriaVideoEraserErasePromptRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutBriaBriaVideoEraserErasePromptRequestsByRequestIdCancelResponse =
  PutBriaBriaVideoEraserErasePromptRequestsByRequestIdCancelResponses[keyof PutBriaBriaVideoEraserErasePromptRequestsByRequestIdCancelResponses]

export type PostBriaBriaVideoEraserErasePromptData = {
  body: SchemaBriaVideoEraserErasePromptInput
  path?: never
  query?: never
  url: '/bria/bria_video_eraser/erase/prompt'
}

export type PostBriaBriaVideoEraserErasePromptResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostBriaBriaVideoEraserErasePromptResponse =
  PostBriaBriaVideoEraserErasePromptResponses[keyof PostBriaBriaVideoEraserErasePromptResponses]

export type GetBriaBriaVideoEraserErasePromptRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/bria/bria_video_eraser/erase/prompt/requests/{request_id}'
}

export type GetBriaBriaVideoEraserErasePromptRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaBriaVideoEraserErasePromptOutput
}

export type GetBriaBriaVideoEraserErasePromptRequestsByRequestIdResponse =
  GetBriaBriaVideoEraserErasePromptRequestsByRequestIdResponses[keyof GetBriaBriaVideoEraserErasePromptRequestsByRequestIdResponses]

export type GetWanV26ReferenceToVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/wan/v2.6/reference-to-video/requests/{request_id}/status'
}

export type GetWanV26ReferenceToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetWanV26ReferenceToVideoRequestsByRequestIdStatusResponse =
  GetWanV26ReferenceToVideoRequestsByRequestIdStatusResponses[keyof GetWanV26ReferenceToVideoRequestsByRequestIdStatusResponses]

export type PutWanV26ReferenceToVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/wan/v2.6/reference-to-video/requests/{request_id}/cancel'
}

export type PutWanV26ReferenceToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutWanV26ReferenceToVideoRequestsByRequestIdCancelResponse =
  PutWanV26ReferenceToVideoRequestsByRequestIdCancelResponses[keyof PutWanV26ReferenceToVideoRequestsByRequestIdCancelResponses]

export type PostWanV26ReferenceToVideoData = {
  body: SchemaV26ReferenceToVideoInput
  path?: never
  query?: never
  url: '/wan/v2.6/reference-to-video'
}

export type PostWanV26ReferenceToVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostWanV26ReferenceToVideoResponse =
  PostWanV26ReferenceToVideoResponses[keyof PostWanV26ReferenceToVideoResponses]

export type GetWanV26ReferenceToVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/wan/v2.6/reference-to-video/requests/{request_id}'
}

export type GetWanV26ReferenceToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaV26ReferenceToVideoOutput
}

export type GetWanV26ReferenceToVideoRequestsByRequestIdResponse =
  GetWanV26ReferenceToVideoRequestsByRequestIdResponses[keyof GetWanV26ReferenceToVideoRequestsByRequestIdResponses]

export type GetFalAiVeo31FastExtendVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/veo3.1/fast/extend-video/requests/{request_id}/status'
}

export type GetFalAiVeo31FastExtendVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiVeo31FastExtendVideoRequestsByRequestIdStatusResponse =
  GetFalAiVeo31FastExtendVideoRequestsByRequestIdStatusResponses[keyof GetFalAiVeo31FastExtendVideoRequestsByRequestIdStatusResponses]

export type PutFalAiVeo31FastExtendVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/veo3.1/fast/extend-video/requests/{request_id}/cancel'
}

export type PutFalAiVeo31FastExtendVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiVeo31FastExtendVideoRequestsByRequestIdCancelResponse =
  PutFalAiVeo31FastExtendVideoRequestsByRequestIdCancelResponses[keyof PutFalAiVeo31FastExtendVideoRequestsByRequestIdCancelResponses]

export type PostFalAiVeo31FastExtendVideoData = {
  body: SchemaVeo31FastExtendVideoInput
  path?: never
  query?: never
  url: '/fal-ai/veo3.1/fast/extend-video'
}

export type PostFalAiVeo31FastExtendVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiVeo31FastExtendVideoResponse =
  PostFalAiVeo31FastExtendVideoResponses[keyof PostFalAiVeo31FastExtendVideoResponses]

export type GetFalAiVeo31FastExtendVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/veo3.1/fast/extend-video/requests/{request_id}'
}

export type GetFalAiVeo31FastExtendVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaVeo31FastExtendVideoOutput
}

export type GetFalAiVeo31FastExtendVideoRequestsByRequestIdResponse =
  GetFalAiVeo31FastExtendVideoRequestsByRequestIdResponses[keyof GetFalAiVeo31FastExtendVideoRequestsByRequestIdResponses]

export type GetFalAiVeo31ExtendVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/veo3.1/extend-video/requests/{request_id}/status'
}

export type GetFalAiVeo31ExtendVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiVeo31ExtendVideoRequestsByRequestIdStatusResponse =
  GetFalAiVeo31ExtendVideoRequestsByRequestIdStatusResponses[keyof GetFalAiVeo31ExtendVideoRequestsByRequestIdStatusResponses]

export type PutFalAiVeo31ExtendVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/veo3.1/extend-video/requests/{request_id}/cancel'
}

export type PutFalAiVeo31ExtendVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiVeo31ExtendVideoRequestsByRequestIdCancelResponse =
  PutFalAiVeo31ExtendVideoRequestsByRequestIdCancelResponses[keyof PutFalAiVeo31ExtendVideoRequestsByRequestIdCancelResponses]

export type PostFalAiVeo31ExtendVideoData = {
  body: SchemaVeo31ExtendVideoInput
  path?: never
  query?: never
  url: '/fal-ai/veo3.1/extend-video'
}

export type PostFalAiVeo31ExtendVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiVeo31ExtendVideoResponse =
  PostFalAiVeo31ExtendVideoResponses[keyof PostFalAiVeo31ExtendVideoResponses]

export type GetFalAiVeo31ExtendVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/veo3.1/extend-video/requests/{request_id}'
}

export type GetFalAiVeo31ExtendVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaVeo31ExtendVideoOutput
}

export type GetFalAiVeo31ExtendVideoRequestsByRequestIdResponse =
  GetFalAiVeo31ExtendVideoRequestsByRequestIdResponses[keyof GetFalAiVeo31ExtendVideoRequestsByRequestIdResponses]

export type GetFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/kling-video/o1/standard/video-to-video/reference/requests/{request_id}/status'
  }

export type GetFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdStatusResponses]

export type PutFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/kling-video/o1/standard/video-to-video/reference/requests/{request_id}/cancel'
  }

export type PutFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdCancelResponses]

export type PostFalAiKlingVideoO1StandardVideoToVideoReferenceData = {
  body: SchemaKlingVideoO1StandardVideoToVideoReferenceInput
  path?: never
  query?: never
  url: '/fal-ai/kling-video/o1/standard/video-to-video/reference'
}

export type PostFalAiKlingVideoO1StandardVideoToVideoReferenceResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiKlingVideoO1StandardVideoToVideoReferenceResponse =
  PostFalAiKlingVideoO1StandardVideoToVideoReferenceResponses[keyof PostFalAiKlingVideoO1StandardVideoToVideoReferenceResponses]

export type GetFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/kling-video/o1/standard/video-to-video/reference/requests/{request_id}'
  }

export type GetFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaKlingVideoO1StandardVideoToVideoReferenceOutput
  }

export type GetFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdResponse =
  GetFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdResponses[keyof GetFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdResponses]

export type GetFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/kling-video/o1/standard/video-to-video/edit/requests/{request_id}/status'
  }

export type GetFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdStatusResponses]

export type PutFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/kling-video/o1/standard/video-to-video/edit/requests/{request_id}/cancel'
  }

export type PutFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdCancelResponses]

export type PostFalAiKlingVideoO1StandardVideoToVideoEditData = {
  body: SchemaKlingVideoO1StandardVideoToVideoEditInput
  path?: never
  query?: never
  url: '/fal-ai/kling-video/o1/standard/video-to-video/edit'
}

export type PostFalAiKlingVideoO1StandardVideoToVideoEditResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiKlingVideoO1StandardVideoToVideoEditResponse =
  PostFalAiKlingVideoO1StandardVideoToVideoEditResponses[keyof PostFalAiKlingVideoO1StandardVideoToVideoEditResponses]

export type GetFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/kling-video/o1/standard/video-to-video/edit/requests/{request_id}'
  }

export type GetFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaKlingVideoO1StandardVideoToVideoEditOutput
  }

export type GetFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdResponse =
  GetFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdResponses[keyof GetFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdResponses]

export type GetFalAiSteadyDancerRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/steady-dancer/requests/{request_id}/status'
}

export type GetFalAiSteadyDancerRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiSteadyDancerRequestsByRequestIdStatusResponse =
  GetFalAiSteadyDancerRequestsByRequestIdStatusResponses[keyof GetFalAiSteadyDancerRequestsByRequestIdStatusResponses]

export type PutFalAiSteadyDancerRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/steady-dancer/requests/{request_id}/cancel'
}

export type PutFalAiSteadyDancerRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiSteadyDancerRequestsByRequestIdCancelResponse =
  PutFalAiSteadyDancerRequestsByRequestIdCancelResponses[keyof PutFalAiSteadyDancerRequestsByRequestIdCancelResponses]

export type PostFalAiSteadyDancerData = {
  body: SchemaSteadyDancerInput
  path?: never
  query?: never
  url: '/fal-ai/steady-dancer'
}

export type PostFalAiSteadyDancerResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiSteadyDancerResponse =
  PostFalAiSteadyDancerResponses[keyof PostFalAiSteadyDancerResponses]

export type GetFalAiSteadyDancerRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/steady-dancer/requests/{request_id}'
}

export type GetFalAiSteadyDancerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSteadyDancerOutput
}

export type GetFalAiSteadyDancerRequestsByRequestIdResponse =
  GetFalAiSteadyDancerRequestsByRequestIdResponses[keyof GetFalAiSteadyDancerRequestsByRequestIdResponses]

export type GetFalAiOneToAllAnimation13bRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/one-to-all-animation/1.3b/requests/{request_id}/status'
}

export type GetFalAiOneToAllAnimation13bRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiOneToAllAnimation13bRequestsByRequestIdStatusResponse =
  GetFalAiOneToAllAnimation13bRequestsByRequestIdStatusResponses[keyof GetFalAiOneToAllAnimation13bRequestsByRequestIdStatusResponses]

export type PutFalAiOneToAllAnimation13bRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/one-to-all-animation/1.3b/requests/{request_id}/cancel'
}

export type PutFalAiOneToAllAnimation13bRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiOneToAllAnimation13bRequestsByRequestIdCancelResponse =
  PutFalAiOneToAllAnimation13bRequestsByRequestIdCancelResponses[keyof PutFalAiOneToAllAnimation13bRequestsByRequestIdCancelResponses]

export type PostFalAiOneToAllAnimation13bData = {
  body: SchemaOneToAllAnimation13bInput
  path?: never
  query?: never
  url: '/fal-ai/one-to-all-animation/1.3b'
}

export type PostFalAiOneToAllAnimation13bResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiOneToAllAnimation13bResponse =
  PostFalAiOneToAllAnimation13bResponses[keyof PostFalAiOneToAllAnimation13bResponses]

export type GetFalAiOneToAllAnimation13bRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/one-to-all-animation/1.3b/requests/{request_id}'
}

export type GetFalAiOneToAllAnimation13bRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaOneToAllAnimation13bOutput
}

export type GetFalAiOneToAllAnimation13bRequestsByRequestIdResponse =
  GetFalAiOneToAllAnimation13bRequestsByRequestIdResponses[keyof GetFalAiOneToAllAnimation13bRequestsByRequestIdResponses]

export type GetFalAiOneToAllAnimation14bRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/one-to-all-animation/14b/requests/{request_id}/status'
}

export type GetFalAiOneToAllAnimation14bRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiOneToAllAnimation14bRequestsByRequestIdStatusResponse =
  GetFalAiOneToAllAnimation14bRequestsByRequestIdStatusResponses[keyof GetFalAiOneToAllAnimation14bRequestsByRequestIdStatusResponses]

export type PutFalAiOneToAllAnimation14bRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/one-to-all-animation/14b/requests/{request_id}/cancel'
}

export type PutFalAiOneToAllAnimation14bRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiOneToAllAnimation14bRequestsByRequestIdCancelResponse =
  PutFalAiOneToAllAnimation14bRequestsByRequestIdCancelResponses[keyof PutFalAiOneToAllAnimation14bRequestsByRequestIdCancelResponses]

export type PostFalAiOneToAllAnimation14bData = {
  body: SchemaOneToAllAnimation14bInput
  path?: never
  query?: never
  url: '/fal-ai/one-to-all-animation/14b'
}

export type PostFalAiOneToAllAnimation14bResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiOneToAllAnimation14bResponse =
  PostFalAiOneToAllAnimation14bResponses[keyof PostFalAiOneToAllAnimation14bResponses]

export type GetFalAiOneToAllAnimation14bRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/one-to-all-animation/14b/requests/{request_id}'
}

export type GetFalAiOneToAllAnimation14bRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaOneToAllAnimation14bOutput
}

export type GetFalAiOneToAllAnimation14bRequestsByRequestIdResponse =
  GetFalAiOneToAllAnimation14bRequestsByRequestIdResponses[keyof GetFalAiOneToAllAnimation14bRequestsByRequestIdResponses]

export type GetFalAiWanVisionEnhancerRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/wan-vision-enhancer/requests/{request_id}/status'
}

export type GetFalAiWanVisionEnhancerRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiWanVisionEnhancerRequestsByRequestIdStatusResponse =
  GetFalAiWanVisionEnhancerRequestsByRequestIdStatusResponses[keyof GetFalAiWanVisionEnhancerRequestsByRequestIdStatusResponses]

export type PutFalAiWanVisionEnhancerRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-vision-enhancer/requests/{request_id}/cancel'
}

export type PutFalAiWanVisionEnhancerRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiWanVisionEnhancerRequestsByRequestIdCancelResponse =
  PutFalAiWanVisionEnhancerRequestsByRequestIdCancelResponses[keyof PutFalAiWanVisionEnhancerRequestsByRequestIdCancelResponses]

export type PostFalAiWanVisionEnhancerData = {
  body: SchemaWanVisionEnhancerInput
  path?: never
  query?: never
  url: '/fal-ai/wan-vision-enhancer'
}

export type PostFalAiWanVisionEnhancerResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWanVisionEnhancerResponse =
  PostFalAiWanVisionEnhancerResponses[keyof PostFalAiWanVisionEnhancerResponses]

export type GetFalAiWanVisionEnhancerRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-vision-enhancer/requests/{request_id}'
}

export type GetFalAiWanVisionEnhancerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWanVisionEnhancerOutput
}

export type GetFalAiWanVisionEnhancerRequestsByRequestIdResponse =
  GetFalAiWanVisionEnhancerRequestsByRequestIdResponses[keyof GetFalAiWanVisionEnhancerRequestsByRequestIdResponses]

export type GetFalAiSyncLipsyncReact1RequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/sync-lipsync/react-1/requests/{request_id}/status'
}

export type GetFalAiSyncLipsyncReact1RequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiSyncLipsyncReact1RequestsByRequestIdStatusResponse =
  GetFalAiSyncLipsyncReact1RequestsByRequestIdStatusResponses[keyof GetFalAiSyncLipsyncReact1RequestsByRequestIdStatusResponses]

export type PutFalAiSyncLipsyncReact1RequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/sync-lipsync/react-1/requests/{request_id}/cancel'
}

export type PutFalAiSyncLipsyncReact1RequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiSyncLipsyncReact1RequestsByRequestIdCancelResponse =
  PutFalAiSyncLipsyncReact1RequestsByRequestIdCancelResponses[keyof PutFalAiSyncLipsyncReact1RequestsByRequestIdCancelResponses]

export type PostFalAiSyncLipsyncReact1Data = {
  body: SchemaSyncLipsyncReact1Input
  path?: never
  query?: never
  url: '/fal-ai/sync-lipsync/react-1'
}

export type PostFalAiSyncLipsyncReact1Responses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiSyncLipsyncReact1Response =
  PostFalAiSyncLipsyncReact1Responses[keyof PostFalAiSyncLipsyncReact1Responses]

export type GetFalAiSyncLipsyncReact1RequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/sync-lipsync/react-1/requests/{request_id}'
}

export type GetFalAiSyncLipsyncReact1RequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSyncLipsyncReact1Output
}

export type GetFalAiSyncLipsyncReact1RequestsByRequestIdResponse =
  GetFalAiSyncLipsyncReact1RequestsByRequestIdResponses[keyof GetFalAiSyncLipsyncReact1RequestsByRequestIdResponses]

export type GetVeedVideoBackgroundRemovalFastRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/veed/video-background-removal/fast/requests/{request_id}/status'
}

export type GetVeedVideoBackgroundRemovalFastRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetVeedVideoBackgroundRemovalFastRequestsByRequestIdStatusResponse =
  GetVeedVideoBackgroundRemovalFastRequestsByRequestIdStatusResponses[keyof GetVeedVideoBackgroundRemovalFastRequestsByRequestIdStatusResponses]

export type PutVeedVideoBackgroundRemovalFastRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/veed/video-background-removal/fast/requests/{request_id}/cancel'
}

export type PutVeedVideoBackgroundRemovalFastRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutVeedVideoBackgroundRemovalFastRequestsByRequestIdCancelResponse =
  PutVeedVideoBackgroundRemovalFastRequestsByRequestIdCancelResponses[keyof PutVeedVideoBackgroundRemovalFastRequestsByRequestIdCancelResponses]

export type PostVeedVideoBackgroundRemovalFastData = {
  body: SchemaVideoBackgroundRemovalFastInput
  path?: never
  query?: never
  url: '/veed/video-background-removal/fast'
}

export type PostVeedVideoBackgroundRemovalFastResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostVeedVideoBackgroundRemovalFastResponse =
  PostVeedVideoBackgroundRemovalFastResponses[keyof PostVeedVideoBackgroundRemovalFastResponses]

export type GetVeedVideoBackgroundRemovalFastRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/veed/video-background-removal/fast/requests/{request_id}'
}

export type GetVeedVideoBackgroundRemovalFastRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaVideoBackgroundRemovalFastOutput
}

export type GetVeedVideoBackgroundRemovalFastRequestsByRequestIdResponse =
  GetVeedVideoBackgroundRemovalFastRequestsByRequestIdResponses[keyof GetVeedVideoBackgroundRemovalFastRequestsByRequestIdResponses]

export type GetFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/kling-video/o1/video-to-video/edit/requests/{request_id}/status'
  }

export type GetFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdStatusResponses]

export type PutFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/kling-video/o1/video-to-video/edit/requests/{request_id}/cancel'
  }

export type PutFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdCancelResponses]

export type PostFalAiKlingVideoO1VideoToVideoEditData = {
  body: SchemaKlingVideoO1VideoToVideoEditInput
  path?: never
  query?: never
  url: '/fal-ai/kling-video/o1/video-to-video/edit'
}

export type PostFalAiKlingVideoO1VideoToVideoEditResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiKlingVideoO1VideoToVideoEditResponse =
  PostFalAiKlingVideoO1VideoToVideoEditResponses[keyof PostFalAiKlingVideoO1VideoToVideoEditResponses]

export type GetFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/kling-video/o1/video-to-video/edit/requests/{request_id}'
}

export type GetFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaKlingVideoO1VideoToVideoEditOutput
}

export type GetFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdResponse =
  GetFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdResponses[keyof GetFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdResponses]

export type GetFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/kling-video/o1/video-to-video/reference/requests/{request_id}/status'
  }

export type GetFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdStatusResponses]

export type PutFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/kling-video/o1/video-to-video/reference/requests/{request_id}/cancel'
  }

export type PutFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdCancelResponses]

export type PostFalAiKlingVideoO1VideoToVideoReferenceData = {
  body: SchemaKlingVideoO1VideoToVideoReferenceInput
  path?: never
  query?: never
  url: '/fal-ai/kling-video/o1/video-to-video/reference'
}

export type PostFalAiKlingVideoO1VideoToVideoReferenceResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiKlingVideoO1VideoToVideoReferenceResponse =
  PostFalAiKlingVideoO1VideoToVideoReferenceResponses[keyof PostFalAiKlingVideoO1VideoToVideoReferenceResponses]

export type GetFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/kling-video/o1/video-to-video/reference/requests/{request_id}'
}

export type GetFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaKlingVideoO1VideoToVideoReferenceOutput
  }

export type GetFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdResponse =
  GetFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdResponses[keyof GetFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdResponses]

export type GetVeedVideoBackgroundRemovalRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/veed/video-background-removal/requests/{request_id}/status'
}

export type GetVeedVideoBackgroundRemovalRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetVeedVideoBackgroundRemovalRequestsByRequestIdStatusResponse =
  GetVeedVideoBackgroundRemovalRequestsByRequestIdStatusResponses[keyof GetVeedVideoBackgroundRemovalRequestsByRequestIdStatusResponses]

export type PutVeedVideoBackgroundRemovalRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/veed/video-background-removal/requests/{request_id}/cancel'
}

export type PutVeedVideoBackgroundRemovalRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutVeedVideoBackgroundRemovalRequestsByRequestIdCancelResponse =
  PutVeedVideoBackgroundRemovalRequestsByRequestIdCancelResponses[keyof PutVeedVideoBackgroundRemovalRequestsByRequestIdCancelResponses]

export type PostVeedVideoBackgroundRemovalData = {
  body: SchemaVideoBackgroundRemovalInput
  path?: never
  query?: never
  url: '/veed/video-background-removal'
}

export type PostVeedVideoBackgroundRemovalResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostVeedVideoBackgroundRemovalResponse =
  PostVeedVideoBackgroundRemovalResponses[keyof PostVeedVideoBackgroundRemovalResponses]

export type GetVeedVideoBackgroundRemovalRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/veed/video-background-removal/requests/{request_id}'
}

export type GetVeedVideoBackgroundRemovalRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaVideoBackgroundRemovalOutput
}

export type GetVeedVideoBackgroundRemovalRequestsByRequestIdResponse =
  GetVeedVideoBackgroundRemovalRequestsByRequestIdResponses[keyof GetVeedVideoBackgroundRemovalRequestsByRequestIdResponses]

export type GetVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/veed/video-background-removal/green-screen/requests/{request_id}/status'
  }

export type GetVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdStatusResponse =
  GetVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdStatusResponses[keyof GetVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdStatusResponses]

export type PutVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/veed/video-background-removal/green-screen/requests/{request_id}/cancel'
  }

export type PutVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdCancelResponse =
  PutVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdCancelResponses[keyof PutVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdCancelResponses]

export type PostVeedVideoBackgroundRemovalGreenScreenData = {
  body: SchemaVideoBackgroundRemovalGreenScreenInput
  path?: never
  query?: never
  url: '/veed/video-background-removal/green-screen'
}

export type PostVeedVideoBackgroundRemovalGreenScreenResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostVeedVideoBackgroundRemovalGreenScreenResponse =
  PostVeedVideoBackgroundRemovalGreenScreenResponses[keyof PostVeedVideoBackgroundRemovalGreenScreenResponses]

export type GetVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/veed/video-background-removal/green-screen/requests/{request_id}'
}

export type GetVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaVideoBackgroundRemovalGreenScreenOutput
  }

export type GetVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdResponse =
  GetVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdResponses[keyof GetVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdResponses]

export type GetFalAiLtx2RetakeVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/ltx-2/retake-video/requests/{request_id}/status'
}

export type GetFalAiLtx2RetakeVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiLtx2RetakeVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtx2RetakeVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtx2RetakeVideoRequestsByRequestIdStatusResponses]

export type PutFalAiLtx2RetakeVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-2/retake-video/requests/{request_id}/cancel'
}

export type PutFalAiLtx2RetakeVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiLtx2RetakeVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtx2RetakeVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtx2RetakeVideoRequestsByRequestIdCancelResponses]

export type PostFalAiLtx2RetakeVideoData = {
  body: SchemaLtx2RetakeVideoInput
  path?: never
  query?: never
  url: '/fal-ai/ltx-2/retake-video'
}

export type PostFalAiLtx2RetakeVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLtx2RetakeVideoResponse =
  PostFalAiLtx2RetakeVideoResponses[keyof PostFalAiLtx2RetakeVideoResponses]

export type GetFalAiLtx2RetakeVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-2/retake-video/requests/{request_id}'
}

export type GetFalAiLtx2RetakeVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLtx2RetakeVideoOutput
}

export type GetFalAiLtx2RetakeVideoRequestsByRequestIdResponse =
  GetFalAiLtx2RetakeVideoRequestsByRequestIdResponses[keyof GetFalAiLtx2RetakeVideoRequestsByRequestIdResponses]

export type GetDecartLucyEditFastRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/decart/lucy-edit/fast/requests/{request_id}/status'
}

export type GetDecartLucyEditFastRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetDecartLucyEditFastRequestsByRequestIdStatusResponse =
  GetDecartLucyEditFastRequestsByRequestIdStatusResponses[keyof GetDecartLucyEditFastRequestsByRequestIdStatusResponses]

export type PutDecartLucyEditFastRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/decart/lucy-edit/fast/requests/{request_id}/cancel'
}

export type PutDecartLucyEditFastRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutDecartLucyEditFastRequestsByRequestIdCancelResponse =
  PutDecartLucyEditFastRequestsByRequestIdCancelResponses[keyof PutDecartLucyEditFastRequestsByRequestIdCancelResponses]

export type PostDecartLucyEditFastData = {
  body: SchemaLucyEditFastInput
  path?: never
  query?: never
  url: '/decart/lucy-edit/fast'
}

export type PostDecartLucyEditFastResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostDecartLucyEditFastResponse =
  PostDecartLucyEditFastResponses[keyof PostDecartLucyEditFastResponses]

export type GetDecartLucyEditFastRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/decart/lucy-edit/fast/requests/{request_id}'
}

export type GetDecartLucyEditFastRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLucyEditFastOutput
}

export type GetDecartLucyEditFastRequestsByRequestIdResponse =
  GetDecartLucyEditFastRequestsByRequestIdResponses[keyof GetDecartLucyEditFastRequestsByRequestIdResponses]

export type GetFalAiSam3VideoRleRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/sam-3/video-rle/requests/{request_id}/status'
}

export type GetFalAiSam3VideoRleRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiSam3VideoRleRequestsByRequestIdStatusResponse =
  GetFalAiSam3VideoRleRequestsByRequestIdStatusResponses[keyof GetFalAiSam3VideoRleRequestsByRequestIdStatusResponses]

export type PutFalAiSam3VideoRleRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/sam-3/video-rle/requests/{request_id}/cancel'
}

export type PutFalAiSam3VideoRleRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiSam3VideoRleRequestsByRequestIdCancelResponse =
  PutFalAiSam3VideoRleRequestsByRequestIdCancelResponses[keyof PutFalAiSam3VideoRleRequestsByRequestIdCancelResponses]

export type PostFalAiSam3VideoRleData = {
  body: SchemaSam3VideoRleInput
  path?: never
  query?: never
  url: '/fal-ai/sam-3/video-rle'
}

export type PostFalAiSam3VideoRleResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiSam3VideoRleResponse =
  PostFalAiSam3VideoRleResponses[keyof PostFalAiSam3VideoRleResponses]

export type GetFalAiSam3VideoRleRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/sam-3/video-rle/requests/{request_id}'
}

export type GetFalAiSam3VideoRleRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSam3VideoRleOutput
}

export type GetFalAiSam3VideoRleRequestsByRequestIdResponse =
  GetFalAiSam3VideoRleRequestsByRequestIdResponses[keyof GetFalAiSam3VideoRleRequestsByRequestIdResponses]

export type GetFalAiSam3VideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/sam-3/video/requests/{request_id}/status'
}

export type GetFalAiSam3VideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiSam3VideoRequestsByRequestIdStatusResponse =
  GetFalAiSam3VideoRequestsByRequestIdStatusResponses[keyof GetFalAiSam3VideoRequestsByRequestIdStatusResponses]

export type PutFalAiSam3VideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/sam-3/video/requests/{request_id}/cancel'
}

export type PutFalAiSam3VideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiSam3VideoRequestsByRequestIdCancelResponse =
  PutFalAiSam3VideoRequestsByRequestIdCancelResponses[keyof PutFalAiSam3VideoRequestsByRequestIdCancelResponses]

export type PostFalAiSam3VideoData = {
  body: SchemaSam3VideoInput
  path?: never
  query?: never
  url: '/fal-ai/sam-3/video'
}

export type PostFalAiSam3VideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiSam3VideoResponse =
  PostFalAiSam3VideoResponses[keyof PostFalAiSam3VideoResponses]

export type GetFalAiSam3VideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/sam-3/video/requests/{request_id}'
}

export type GetFalAiSam3VideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSam3VideoOutput
}

export type GetFalAiSam3VideoRequestsByRequestIdResponse =
  GetFalAiSam3VideoRequestsByRequestIdResponses[keyof GetFalAiSam3VideoRequestsByRequestIdResponses]

export type GetFalAiEdittoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/editto/requests/{request_id}/status'
}

export type GetFalAiEdittoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiEdittoRequestsByRequestIdStatusResponse =
  GetFalAiEdittoRequestsByRequestIdStatusResponses[keyof GetFalAiEdittoRequestsByRequestIdStatusResponses]

export type PutFalAiEdittoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/editto/requests/{request_id}/cancel'
}

export type PutFalAiEdittoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiEdittoRequestsByRequestIdCancelResponse =
  PutFalAiEdittoRequestsByRequestIdCancelResponses[keyof PutFalAiEdittoRequestsByRequestIdCancelResponses]

export type PostFalAiEdittoData = {
  body: SchemaEdittoInput
  path?: never
  query?: never
  url: '/fal-ai/editto'
}

export type PostFalAiEdittoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiEdittoResponse =
  PostFalAiEdittoResponses[keyof PostFalAiEdittoResponses]

export type GetFalAiEdittoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/editto/requests/{request_id}'
}

export type GetFalAiEdittoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaEdittoOutput
}

export type GetFalAiEdittoRequestsByRequestIdResponse =
  GetFalAiEdittoRequestsByRequestIdResponses[keyof GetFalAiEdittoRequestsByRequestIdResponses]

export type GetFalAiFlashvsrUpscaleVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/flashvsr/upscale/video/requests/{request_id}/status'
}

export type GetFalAiFlashvsrUpscaleVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiFlashvsrUpscaleVideoRequestsByRequestIdStatusResponse =
  GetFalAiFlashvsrUpscaleVideoRequestsByRequestIdStatusResponses[keyof GetFalAiFlashvsrUpscaleVideoRequestsByRequestIdStatusResponses]

export type PutFalAiFlashvsrUpscaleVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/flashvsr/upscale/video/requests/{request_id}/cancel'
}

export type PutFalAiFlashvsrUpscaleVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiFlashvsrUpscaleVideoRequestsByRequestIdCancelResponse =
  PutFalAiFlashvsrUpscaleVideoRequestsByRequestIdCancelResponses[keyof PutFalAiFlashvsrUpscaleVideoRequestsByRequestIdCancelResponses]

export type PostFalAiFlashvsrUpscaleVideoData = {
  body: SchemaFlashvsrUpscaleVideoInput
  path?: never
  query?: never
  url: '/fal-ai/flashvsr/upscale/video'
}

export type PostFalAiFlashvsrUpscaleVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiFlashvsrUpscaleVideoResponse =
  PostFalAiFlashvsrUpscaleVideoResponses[keyof PostFalAiFlashvsrUpscaleVideoResponses]

export type GetFalAiFlashvsrUpscaleVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/flashvsr/upscale/video/requests/{request_id}'
}

export type GetFalAiFlashvsrUpscaleVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaFlashvsrUpscaleVideoOutput
}

export type GetFalAiFlashvsrUpscaleVideoRequestsByRequestIdResponse =
  GetFalAiFlashvsrUpscaleVideoRequestsByRequestIdResponses[keyof GetFalAiFlashvsrUpscaleVideoRequestsByRequestIdResponses]

export type GetFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/workflow-utilities/auto-subtitle/requests/{request_id}/status'
  }

export type GetFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdStatusResponse =
  GetFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdStatusResponses[keyof GetFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdStatusResponses]

export type PutFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/workflow-utilities/auto-subtitle/requests/{request_id}/cancel'
  }

export type PutFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdCancelResponse =
  PutFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdCancelResponses[keyof PutFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdCancelResponses]

export type PostFalAiWorkflowUtilitiesAutoSubtitleData = {
  body: SchemaWorkflowUtilitiesAutoSubtitleInput
  path?: never
  query?: never
  url: '/fal-ai/workflow-utilities/auto-subtitle'
}

export type PostFalAiWorkflowUtilitiesAutoSubtitleResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWorkflowUtilitiesAutoSubtitleResponse =
  PostFalAiWorkflowUtilitiesAutoSubtitleResponses[keyof PostFalAiWorkflowUtilitiesAutoSubtitleResponses]

export type GetFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/workflow-utilities/auto-subtitle/requests/{request_id}'
}

export type GetFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaWorkflowUtilitiesAutoSubtitleOutput
  }

export type GetFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdResponse =
  GetFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdResponses[keyof GetFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdResponses]

export type GetFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/bytedance-upscaler/upscale/video/requests/{request_id}/status'
  }

export type GetFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdStatusResponse =
  GetFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdStatusResponses[keyof GetFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdStatusResponses]

export type PutFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/bytedance-upscaler/upscale/video/requests/{request_id}/cancel'
  }

export type PutFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdCancelResponse =
  PutFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdCancelResponses[keyof PutFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdCancelResponses]

export type PostFalAiBytedanceUpscalerUpscaleVideoData = {
  body: SchemaBytedanceUpscalerUpscaleVideoInput
  path?: never
  query?: never
  url: '/fal-ai/bytedance-upscaler/upscale/video'
}

export type PostFalAiBytedanceUpscalerUpscaleVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiBytedanceUpscalerUpscaleVideoResponse =
  PostFalAiBytedanceUpscalerUpscaleVideoResponses[keyof PostFalAiBytedanceUpscalerUpscaleVideoResponses]

export type GetFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/bytedance-upscaler/upscale/video/requests/{request_id}'
}

export type GetFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaBytedanceUpscalerUpscaleVideoOutput
  }

export type GetFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdResponse =
  GetFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdResponses[keyof GetFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdResponses]

export type GetFalAiVideoAsPromptRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/video-as-prompt/requests/{request_id}/status'
}

export type GetFalAiVideoAsPromptRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiVideoAsPromptRequestsByRequestIdStatusResponse =
  GetFalAiVideoAsPromptRequestsByRequestIdStatusResponses[keyof GetFalAiVideoAsPromptRequestsByRequestIdStatusResponses]

export type PutFalAiVideoAsPromptRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/video-as-prompt/requests/{request_id}/cancel'
}

export type PutFalAiVideoAsPromptRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiVideoAsPromptRequestsByRequestIdCancelResponse =
  PutFalAiVideoAsPromptRequestsByRequestIdCancelResponses[keyof PutFalAiVideoAsPromptRequestsByRequestIdCancelResponses]

export type PostFalAiVideoAsPromptData = {
  body: SchemaVideoAsPromptInput
  path?: never
  query?: never
  url: '/fal-ai/video-as-prompt'
}

export type PostFalAiVideoAsPromptResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiVideoAsPromptResponse =
  PostFalAiVideoAsPromptResponses[keyof PostFalAiVideoAsPromptResponses]

export type GetFalAiVideoAsPromptRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/video-as-prompt/requests/{request_id}'
}

export type GetFalAiVideoAsPromptRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaVideoAsPromptOutput
}

export type GetFalAiVideoAsPromptRequestsByRequestIdResponse =
  GetFalAiVideoAsPromptRequestsByRequestIdResponses[keyof GetFalAiVideoAsPromptRequestsByRequestIdResponses]

export type GetFalAiBirefnetV2VideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/birefnet/v2/video/requests/{request_id}/status'
}

export type GetFalAiBirefnetV2VideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiBirefnetV2VideoRequestsByRequestIdStatusResponse =
  GetFalAiBirefnetV2VideoRequestsByRequestIdStatusResponses[keyof GetFalAiBirefnetV2VideoRequestsByRequestIdStatusResponses]

export type PutFalAiBirefnetV2VideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/birefnet/v2/video/requests/{request_id}/cancel'
}

export type PutFalAiBirefnetV2VideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiBirefnetV2VideoRequestsByRequestIdCancelResponse =
  PutFalAiBirefnetV2VideoRequestsByRequestIdCancelResponses[keyof PutFalAiBirefnetV2VideoRequestsByRequestIdCancelResponses]

export type PostFalAiBirefnetV2VideoData = {
  body: SchemaBirefnetV2VideoInput
  path?: never
  query?: never
  url: '/fal-ai/birefnet/v2/video'
}

export type PostFalAiBirefnetV2VideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiBirefnetV2VideoResponse =
  PostFalAiBirefnetV2VideoResponses[keyof PostFalAiBirefnetV2VideoResponses]

export type GetFalAiBirefnetV2VideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/birefnet/v2/video/requests/{request_id}'
}

export type GetFalAiBirefnetV2VideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaBirefnetV2VideoOutput
}

export type GetFalAiBirefnetV2VideoRequestsByRequestIdResponse =
  GetFalAiBirefnetV2VideoRequestsByRequestIdResponses[keyof GetFalAiBirefnetV2VideoRequestsByRequestIdResponses]

export type GetFalAiViduQ2VideoExtensionProRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/vidu/q2/video-extension/pro/requests/{request_id}/status'
}

export type GetFalAiViduQ2VideoExtensionProRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiViduQ2VideoExtensionProRequestsByRequestIdStatusResponse =
  GetFalAiViduQ2VideoExtensionProRequestsByRequestIdStatusResponses[keyof GetFalAiViduQ2VideoExtensionProRequestsByRequestIdStatusResponses]

export type PutFalAiViduQ2VideoExtensionProRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/vidu/q2/video-extension/pro/requests/{request_id}/cancel'
}

export type PutFalAiViduQ2VideoExtensionProRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiViduQ2VideoExtensionProRequestsByRequestIdCancelResponse =
  PutFalAiViduQ2VideoExtensionProRequestsByRequestIdCancelResponses[keyof PutFalAiViduQ2VideoExtensionProRequestsByRequestIdCancelResponses]

export type PostFalAiViduQ2VideoExtensionProData = {
  body: SchemaViduQ2VideoExtensionProInput
  path?: never
  query?: never
  url: '/fal-ai/vidu/q2/video-extension/pro'
}

export type PostFalAiViduQ2VideoExtensionProResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiViduQ2VideoExtensionProResponse =
  PostFalAiViduQ2VideoExtensionProResponses[keyof PostFalAiViduQ2VideoExtensionProResponses]

export type GetFalAiViduQ2VideoExtensionProRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/vidu/q2/video-extension/pro/requests/{request_id}'
}

export type GetFalAiViduQ2VideoExtensionProRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaViduQ2VideoExtensionProOutput
}

export type GetFalAiViduQ2VideoExtensionProRequestsByRequestIdResponse =
  GetFalAiViduQ2VideoExtensionProRequestsByRequestIdResponses[keyof GetFalAiViduQ2VideoExtensionProRequestsByRequestIdResponses]

export type GetMireloAiSfxV15VideoToVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/mirelo-ai/sfx-v1.5/video-to-video/requests/{request_id}/status'
}

export type GetMireloAiSfxV15VideoToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetMireloAiSfxV15VideoToVideoRequestsByRequestIdStatusResponse =
  GetMireloAiSfxV15VideoToVideoRequestsByRequestIdStatusResponses[keyof GetMireloAiSfxV15VideoToVideoRequestsByRequestIdStatusResponses]

export type PutMireloAiSfxV15VideoToVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/mirelo-ai/sfx-v1.5/video-to-video/requests/{request_id}/cancel'
}

export type PutMireloAiSfxV15VideoToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutMireloAiSfxV15VideoToVideoRequestsByRequestIdCancelResponse =
  PutMireloAiSfxV15VideoToVideoRequestsByRequestIdCancelResponses[keyof PutMireloAiSfxV15VideoToVideoRequestsByRequestIdCancelResponses]

export type PostMireloAiSfxV15VideoToVideoData = {
  body: SchemaSfxV15VideoToVideoInput
  path?: never
  query?: never
  url: '/mirelo-ai/sfx-v1.5/video-to-video'
}

export type PostMireloAiSfxV15VideoToVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostMireloAiSfxV15VideoToVideoResponse =
  PostMireloAiSfxV15VideoToVideoResponses[keyof PostMireloAiSfxV15VideoToVideoResponses]

export type GetMireloAiSfxV15VideoToVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/mirelo-ai/sfx-v1.5/video-to-video/requests/{request_id}'
}

export type GetMireloAiSfxV15VideoToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSfxV15VideoToVideoOutput
}

export type GetMireloAiSfxV15VideoToVideoRequestsByRequestIdResponse =
  GetMireloAiSfxV15VideoToVideoRequestsByRequestIdResponses[keyof GetMireloAiSfxV15VideoToVideoRequestsByRequestIdResponses]

export type GetFalAiKreaWan14bVideoToVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/krea-wan-14b/video-to-video/requests/{request_id}/status'
}

export type GetFalAiKreaWan14bVideoToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiKreaWan14bVideoToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKreaWan14bVideoToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKreaWan14bVideoToVideoRequestsByRequestIdStatusResponses]

export type PutFalAiKreaWan14bVideoToVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/krea-wan-14b/video-to-video/requests/{request_id}/cancel'
}

export type PutFalAiKreaWan14bVideoToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiKreaWan14bVideoToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKreaWan14bVideoToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKreaWan14bVideoToVideoRequestsByRequestIdCancelResponses]

export type PostFalAiKreaWan14bVideoToVideoData = {
  body: SchemaKreaWan14bVideoToVideoInput
  path?: never
  query?: never
  url: '/fal-ai/krea-wan-14b/video-to-video'
}

export type PostFalAiKreaWan14bVideoToVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiKreaWan14bVideoToVideoResponse =
  PostFalAiKreaWan14bVideoToVideoResponses[keyof PostFalAiKreaWan14bVideoToVideoResponses]

export type GetFalAiKreaWan14bVideoToVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/krea-wan-14b/video-to-video/requests/{request_id}'
}

export type GetFalAiKreaWan14bVideoToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaKreaWan14bVideoToVideoOutput
}

export type GetFalAiKreaWan14bVideoToVideoRequestsByRequestIdResponse =
  GetFalAiKreaWan14bVideoToVideoRequestsByRequestIdResponses[keyof GetFalAiKreaWan14bVideoToVideoRequestsByRequestIdResponses]

export type GetFalAiSora2VideoToVideoRemixRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/sora-2/video-to-video/remix/requests/{request_id}/status'
}

export type GetFalAiSora2VideoToVideoRemixRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiSora2VideoToVideoRemixRequestsByRequestIdStatusResponse =
  GetFalAiSora2VideoToVideoRemixRequestsByRequestIdStatusResponses[keyof GetFalAiSora2VideoToVideoRemixRequestsByRequestIdStatusResponses]

export type PutFalAiSora2VideoToVideoRemixRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/sora-2/video-to-video/remix/requests/{request_id}/cancel'
}

export type PutFalAiSora2VideoToVideoRemixRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiSora2VideoToVideoRemixRequestsByRequestIdCancelResponse =
  PutFalAiSora2VideoToVideoRemixRequestsByRequestIdCancelResponses[keyof PutFalAiSora2VideoToVideoRemixRequestsByRequestIdCancelResponses]

export type PostFalAiSora2VideoToVideoRemixData = {
  body: SchemaSora2VideoToVideoRemixInput
  path?: never
  query?: never
  url: '/fal-ai/sora-2/video-to-video/remix'
}

export type PostFalAiSora2VideoToVideoRemixResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiSora2VideoToVideoRemixResponse =
  PostFalAiSora2VideoToVideoRemixResponses[keyof PostFalAiSora2VideoToVideoRemixResponses]

export type GetFalAiSora2VideoToVideoRemixRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/sora-2/video-to-video/remix/requests/{request_id}'
}

export type GetFalAiSora2VideoToVideoRemixRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSora2VideoToVideoRemixOutput
}

export type GetFalAiSora2VideoToVideoRemixRequestsByRequestIdResponse =
  GetFalAiSora2VideoToVideoRemixRequestsByRequestIdResponses[keyof GetFalAiSora2VideoToVideoRemixRequestsByRequestIdResponses]

export type GetFalAiWanVaceAppsLongReframeRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/wan-vace-apps/long-reframe/requests/{request_id}/status'
}

export type GetFalAiWanVaceAppsLongReframeRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiWanVaceAppsLongReframeRequestsByRequestIdStatusResponse =
  GetFalAiWanVaceAppsLongReframeRequestsByRequestIdStatusResponses[keyof GetFalAiWanVaceAppsLongReframeRequestsByRequestIdStatusResponses]

export type PutFalAiWanVaceAppsLongReframeRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-vace-apps/long-reframe/requests/{request_id}/cancel'
}

export type PutFalAiWanVaceAppsLongReframeRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiWanVaceAppsLongReframeRequestsByRequestIdCancelResponse =
  PutFalAiWanVaceAppsLongReframeRequestsByRequestIdCancelResponses[keyof PutFalAiWanVaceAppsLongReframeRequestsByRequestIdCancelResponses]

export type PostFalAiWanVaceAppsLongReframeData = {
  body: SchemaWanVaceAppsLongReframeInput
  path?: never
  query?: never
  url: '/fal-ai/wan-vace-apps/long-reframe'
}

export type PostFalAiWanVaceAppsLongReframeResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWanVaceAppsLongReframeResponse =
  PostFalAiWanVaceAppsLongReframeResponses[keyof PostFalAiWanVaceAppsLongReframeResponses]

export type GetFalAiWanVaceAppsLongReframeRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-vace-apps/long-reframe/requests/{request_id}'
}

export type GetFalAiWanVaceAppsLongReframeRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWanVaceAppsLongReframeOutput
}

export type GetFalAiWanVaceAppsLongReframeRequestsByRequestIdResponse =
  GetFalAiWanVaceAppsLongReframeRequestsByRequestIdResponses[keyof GetFalAiWanVaceAppsLongReframeRequestsByRequestIdResponses]

export type GetFalAiInfinitalkVideoToVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/infinitalk/video-to-video/requests/{request_id}/status'
}

export type GetFalAiInfinitalkVideoToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiInfinitalkVideoToVideoRequestsByRequestIdStatusResponse =
  GetFalAiInfinitalkVideoToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiInfinitalkVideoToVideoRequestsByRequestIdStatusResponses]

export type PutFalAiInfinitalkVideoToVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/infinitalk/video-to-video/requests/{request_id}/cancel'
}

export type PutFalAiInfinitalkVideoToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiInfinitalkVideoToVideoRequestsByRequestIdCancelResponse =
  PutFalAiInfinitalkVideoToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiInfinitalkVideoToVideoRequestsByRequestIdCancelResponses]

export type PostFalAiInfinitalkVideoToVideoData = {
  body: SchemaInfinitalkVideoToVideoInput
  path?: never
  query?: never
  url: '/fal-ai/infinitalk/video-to-video'
}

export type PostFalAiInfinitalkVideoToVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiInfinitalkVideoToVideoResponse =
  PostFalAiInfinitalkVideoToVideoResponses[keyof PostFalAiInfinitalkVideoToVideoResponses]

export type GetFalAiInfinitalkVideoToVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/infinitalk/video-to-video/requests/{request_id}'
}

export type GetFalAiInfinitalkVideoToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaInfinitalkVideoToVideoOutput
}

export type GetFalAiInfinitalkVideoToVideoRequestsByRequestIdResponse =
  GetFalAiInfinitalkVideoToVideoRequestsByRequestIdResponses[keyof GetFalAiInfinitalkVideoToVideoRequestsByRequestIdResponses]

export type GetFalAiSeedvrUpscaleVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/seedvr/upscale/video/requests/{request_id}/status'
}

export type GetFalAiSeedvrUpscaleVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiSeedvrUpscaleVideoRequestsByRequestIdStatusResponse =
  GetFalAiSeedvrUpscaleVideoRequestsByRequestIdStatusResponses[keyof GetFalAiSeedvrUpscaleVideoRequestsByRequestIdStatusResponses]

export type PutFalAiSeedvrUpscaleVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/seedvr/upscale/video/requests/{request_id}/cancel'
}

export type PutFalAiSeedvrUpscaleVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiSeedvrUpscaleVideoRequestsByRequestIdCancelResponse =
  PutFalAiSeedvrUpscaleVideoRequestsByRequestIdCancelResponses[keyof PutFalAiSeedvrUpscaleVideoRequestsByRequestIdCancelResponses]

export type PostFalAiSeedvrUpscaleVideoData = {
  body: SchemaSeedvrUpscaleVideoInput
  path?: never
  query?: never
  url: '/fal-ai/seedvr/upscale/video'
}

export type PostFalAiSeedvrUpscaleVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiSeedvrUpscaleVideoResponse =
  PostFalAiSeedvrUpscaleVideoResponses[keyof PostFalAiSeedvrUpscaleVideoResponses]

export type GetFalAiSeedvrUpscaleVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/seedvr/upscale/video/requests/{request_id}'
}

export type GetFalAiSeedvrUpscaleVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSeedvrUpscaleVideoOutput
}

export type GetFalAiSeedvrUpscaleVideoRequestsByRequestIdResponse =
  GetFalAiSeedvrUpscaleVideoRequestsByRequestIdResponses[keyof GetFalAiSeedvrUpscaleVideoRequestsByRequestIdResponses]

export type GetFalAiWanVaceAppsVideoEditRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/wan-vace-apps/video-edit/requests/{request_id}/status'
}

export type GetFalAiWanVaceAppsVideoEditRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiWanVaceAppsVideoEditRequestsByRequestIdStatusResponse =
  GetFalAiWanVaceAppsVideoEditRequestsByRequestIdStatusResponses[keyof GetFalAiWanVaceAppsVideoEditRequestsByRequestIdStatusResponses]

export type PutFalAiWanVaceAppsVideoEditRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-vace-apps/video-edit/requests/{request_id}/cancel'
}

export type PutFalAiWanVaceAppsVideoEditRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiWanVaceAppsVideoEditRequestsByRequestIdCancelResponse =
  PutFalAiWanVaceAppsVideoEditRequestsByRequestIdCancelResponses[keyof PutFalAiWanVaceAppsVideoEditRequestsByRequestIdCancelResponses]

export type PostFalAiWanVaceAppsVideoEditData = {
  body: SchemaWanVaceAppsVideoEditInput
  path?: never
  query?: never
  url: '/fal-ai/wan-vace-apps/video-edit'
}

export type PostFalAiWanVaceAppsVideoEditResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWanVaceAppsVideoEditResponse =
  PostFalAiWanVaceAppsVideoEditResponses[keyof PostFalAiWanVaceAppsVideoEditResponses]

export type GetFalAiWanVaceAppsVideoEditRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-vace-apps/video-edit/requests/{request_id}'
}

export type GetFalAiWanVaceAppsVideoEditRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWanVaceAppsVideoEditOutput
}

export type GetFalAiWanVaceAppsVideoEditRequestsByRequestIdResponse =
  GetFalAiWanVaceAppsVideoEditRequestsByRequestIdResponses[keyof GetFalAiWanVaceAppsVideoEditRequestsByRequestIdResponses]

export type GetFalAiWanV2214bAnimateReplaceRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/wan/v2.2-14b/animate/replace/requests/{request_id}/status'
}

export type GetFalAiWanV2214bAnimateReplaceRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiWanV2214bAnimateReplaceRequestsByRequestIdStatusResponse =
  GetFalAiWanV2214bAnimateReplaceRequestsByRequestIdStatusResponses[keyof GetFalAiWanV2214bAnimateReplaceRequestsByRequestIdStatusResponses]

export type PutFalAiWanV2214bAnimateReplaceRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan/v2.2-14b/animate/replace/requests/{request_id}/cancel'
}

export type PutFalAiWanV2214bAnimateReplaceRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiWanV2214bAnimateReplaceRequestsByRequestIdCancelResponse =
  PutFalAiWanV2214bAnimateReplaceRequestsByRequestIdCancelResponses[keyof PutFalAiWanV2214bAnimateReplaceRequestsByRequestIdCancelResponses]

export type PostFalAiWanV2214bAnimateReplaceData = {
  body: SchemaWanV2214bAnimateReplaceInput
  path?: never
  query?: never
  url: '/fal-ai/wan/v2.2-14b/animate/replace'
}

export type PostFalAiWanV2214bAnimateReplaceResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWanV2214bAnimateReplaceResponse =
  PostFalAiWanV2214bAnimateReplaceResponses[keyof PostFalAiWanV2214bAnimateReplaceResponses]

export type GetFalAiWanV2214bAnimateReplaceRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan/v2.2-14b/animate/replace/requests/{request_id}'
}

export type GetFalAiWanV2214bAnimateReplaceRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWanV2214bAnimateReplaceOutput
}

export type GetFalAiWanV2214bAnimateReplaceRequestsByRequestIdResponse =
  GetFalAiWanV2214bAnimateReplaceRequestsByRequestIdResponses[keyof GetFalAiWanV2214bAnimateReplaceRequestsByRequestIdResponses]

export type GetFalAiWanV2214bAnimateMoveRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/wan/v2.2-14b/animate/move/requests/{request_id}/status'
}

export type GetFalAiWanV2214bAnimateMoveRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiWanV2214bAnimateMoveRequestsByRequestIdStatusResponse =
  GetFalAiWanV2214bAnimateMoveRequestsByRequestIdStatusResponses[keyof GetFalAiWanV2214bAnimateMoveRequestsByRequestIdStatusResponses]

export type PutFalAiWanV2214bAnimateMoveRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan/v2.2-14b/animate/move/requests/{request_id}/cancel'
}

export type PutFalAiWanV2214bAnimateMoveRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiWanV2214bAnimateMoveRequestsByRequestIdCancelResponse =
  PutFalAiWanV2214bAnimateMoveRequestsByRequestIdCancelResponses[keyof PutFalAiWanV2214bAnimateMoveRequestsByRequestIdCancelResponses]

export type PostFalAiWanV2214bAnimateMoveData = {
  body: SchemaWanV2214bAnimateMoveInput
  path?: never
  query?: never
  url: '/fal-ai/wan/v2.2-14b/animate/move'
}

export type PostFalAiWanV2214bAnimateMoveResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWanV2214bAnimateMoveResponse =
  PostFalAiWanV2214bAnimateMoveResponses[keyof PostFalAiWanV2214bAnimateMoveResponses]

export type GetFalAiWanV2214bAnimateMoveRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan/v2.2-14b/animate/move/requests/{request_id}'
}

export type GetFalAiWanV2214bAnimateMoveRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWanV2214bAnimateMoveOutput
}

export type GetFalAiWanV2214bAnimateMoveRequestsByRequestIdResponse =
  GetFalAiWanV2214bAnimateMoveRequestsByRequestIdResponses[keyof GetFalAiWanV2214bAnimateMoveRequestsByRequestIdResponses]

export type GetDecartLucyEditProRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/decart/lucy-edit/pro/requests/{request_id}/status'
}

export type GetDecartLucyEditProRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetDecartLucyEditProRequestsByRequestIdStatusResponse =
  GetDecartLucyEditProRequestsByRequestIdStatusResponses[keyof GetDecartLucyEditProRequestsByRequestIdStatusResponses]

export type PutDecartLucyEditProRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/decart/lucy-edit/pro/requests/{request_id}/cancel'
}

export type PutDecartLucyEditProRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutDecartLucyEditProRequestsByRequestIdCancelResponse =
  PutDecartLucyEditProRequestsByRequestIdCancelResponses[keyof PutDecartLucyEditProRequestsByRequestIdCancelResponses]

export type PostDecartLucyEditProData = {
  body: SchemaLucyEditProInput
  path?: never
  query?: never
  url: '/decart/lucy-edit/pro'
}

export type PostDecartLucyEditProResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostDecartLucyEditProResponse =
  PostDecartLucyEditProResponses[keyof PostDecartLucyEditProResponses]

export type GetDecartLucyEditProRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/decart/lucy-edit/pro/requests/{request_id}'
}

export type GetDecartLucyEditProRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLucyEditProOutput
}

export type GetDecartLucyEditProRequestsByRequestIdResponse =
  GetDecartLucyEditProRequestsByRequestIdResponses[keyof GetDecartLucyEditProRequestsByRequestIdResponses]

export type GetDecartLucyEditDevRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/decart/lucy-edit/dev/requests/{request_id}/status'
}

export type GetDecartLucyEditDevRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetDecartLucyEditDevRequestsByRequestIdStatusResponse =
  GetDecartLucyEditDevRequestsByRequestIdStatusResponses[keyof GetDecartLucyEditDevRequestsByRequestIdStatusResponses]

export type PutDecartLucyEditDevRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/decart/lucy-edit/dev/requests/{request_id}/cancel'
}

export type PutDecartLucyEditDevRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutDecartLucyEditDevRequestsByRequestIdCancelResponse =
  PutDecartLucyEditDevRequestsByRequestIdCancelResponses[keyof PutDecartLucyEditDevRequestsByRequestIdCancelResponses]

export type PostDecartLucyEditDevData = {
  body: SchemaLucyEditDevInput
  path?: never
  query?: never
  url: '/decart/lucy-edit/dev'
}

export type PostDecartLucyEditDevResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostDecartLucyEditDevResponse =
  PostDecartLucyEditDevResponses[keyof PostDecartLucyEditDevResponses]

export type GetDecartLucyEditDevRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/decart/lucy-edit/dev/requests/{request_id}'
}

export type GetDecartLucyEditDevRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLucyEditDevOutput
}

export type GetDecartLucyEditDevRequestsByRequestIdResponse =
  GetDecartLucyEditDevRequestsByRequestIdResponses[keyof GetDecartLucyEditDevRequestsByRequestIdResponses]

export type GetFalAiWan22VaceFunA14bReframeRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/wan-22-vace-fun-a14b/reframe/requests/{request_id}/status'
}

export type GetFalAiWan22VaceFunA14bReframeRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiWan22VaceFunA14bReframeRequestsByRequestIdStatusResponse =
  GetFalAiWan22VaceFunA14bReframeRequestsByRequestIdStatusResponses[keyof GetFalAiWan22VaceFunA14bReframeRequestsByRequestIdStatusResponses]

export type PutFalAiWan22VaceFunA14bReframeRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-22-vace-fun-a14b/reframe/requests/{request_id}/cancel'
}

export type PutFalAiWan22VaceFunA14bReframeRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiWan22VaceFunA14bReframeRequestsByRequestIdCancelResponse =
  PutFalAiWan22VaceFunA14bReframeRequestsByRequestIdCancelResponses[keyof PutFalAiWan22VaceFunA14bReframeRequestsByRequestIdCancelResponses]

export type PostFalAiWan22VaceFunA14bReframeData = {
  body: SchemaWan22VaceFunA14bReframeInput
  path?: never
  query?: never
  url: '/fal-ai/wan-22-vace-fun-a14b/reframe'
}

export type PostFalAiWan22VaceFunA14bReframeResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWan22VaceFunA14bReframeResponse =
  PostFalAiWan22VaceFunA14bReframeResponses[keyof PostFalAiWan22VaceFunA14bReframeResponses]

export type GetFalAiWan22VaceFunA14bReframeRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-22-vace-fun-a14b/reframe/requests/{request_id}'
}

export type GetFalAiWan22VaceFunA14bReframeRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWan22VaceFunA14bReframeOutput
}

export type GetFalAiWan22VaceFunA14bReframeRequestsByRequestIdResponse =
  GetFalAiWan22VaceFunA14bReframeRequestsByRequestIdResponses[keyof GetFalAiWan22VaceFunA14bReframeRequestsByRequestIdResponses]

export type GetFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/wan-22-vace-fun-a14b/outpainting/requests/{request_id}/status'
}

export type GetFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdStatusResponse =
  GetFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdStatusResponses[keyof GetFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdStatusResponses]

export type PutFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-22-vace-fun-a14b/outpainting/requests/{request_id}/cancel'
}

export type PutFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdCancelResponse =
  PutFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdCancelResponses[keyof PutFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdCancelResponses]

export type PostFalAiWan22VaceFunA14bOutpaintingData = {
  body: SchemaWan22VaceFunA14bOutpaintingInput
  path?: never
  query?: never
  url: '/fal-ai/wan-22-vace-fun-a14b/outpainting'
}

export type PostFalAiWan22VaceFunA14bOutpaintingResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWan22VaceFunA14bOutpaintingResponse =
  PostFalAiWan22VaceFunA14bOutpaintingResponses[keyof PostFalAiWan22VaceFunA14bOutpaintingResponses]

export type GetFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-22-vace-fun-a14b/outpainting/requests/{request_id}'
}

export type GetFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWan22VaceFunA14bOutpaintingOutput
}

export type GetFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdResponse =
  GetFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdResponses[keyof GetFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdResponses]

export type GetFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/wan-22-vace-fun-a14b/inpainting/requests/{request_id}/status'
}

export type GetFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdStatusResponse =
  GetFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdStatusResponses[keyof GetFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdStatusResponses]

export type PutFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-22-vace-fun-a14b/inpainting/requests/{request_id}/cancel'
}

export type PutFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdCancelResponse =
  PutFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdCancelResponses[keyof PutFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdCancelResponses]

export type PostFalAiWan22VaceFunA14bInpaintingData = {
  body: SchemaWan22VaceFunA14bInpaintingInput
  path?: never
  query?: never
  url: '/fal-ai/wan-22-vace-fun-a14b/inpainting'
}

export type PostFalAiWan22VaceFunA14bInpaintingResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWan22VaceFunA14bInpaintingResponse =
  PostFalAiWan22VaceFunA14bInpaintingResponses[keyof PostFalAiWan22VaceFunA14bInpaintingResponses]

export type GetFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-22-vace-fun-a14b/inpainting/requests/{request_id}'
}

export type GetFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWan22VaceFunA14bInpaintingOutput
}

export type GetFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdResponse =
  GetFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdResponses[keyof GetFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdResponses]

export type GetFalAiWan22VaceFunA14bDepthRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/wan-22-vace-fun-a14b/depth/requests/{request_id}/status'
}

export type GetFalAiWan22VaceFunA14bDepthRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiWan22VaceFunA14bDepthRequestsByRequestIdStatusResponse =
  GetFalAiWan22VaceFunA14bDepthRequestsByRequestIdStatusResponses[keyof GetFalAiWan22VaceFunA14bDepthRequestsByRequestIdStatusResponses]

export type PutFalAiWan22VaceFunA14bDepthRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-22-vace-fun-a14b/depth/requests/{request_id}/cancel'
}

export type PutFalAiWan22VaceFunA14bDepthRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiWan22VaceFunA14bDepthRequestsByRequestIdCancelResponse =
  PutFalAiWan22VaceFunA14bDepthRequestsByRequestIdCancelResponses[keyof PutFalAiWan22VaceFunA14bDepthRequestsByRequestIdCancelResponses]

export type PostFalAiWan22VaceFunA14bDepthData = {
  body: SchemaWan22VaceFunA14bDepthInput
  path?: never
  query?: never
  url: '/fal-ai/wan-22-vace-fun-a14b/depth'
}

export type PostFalAiWan22VaceFunA14bDepthResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWan22VaceFunA14bDepthResponse =
  PostFalAiWan22VaceFunA14bDepthResponses[keyof PostFalAiWan22VaceFunA14bDepthResponses]

export type GetFalAiWan22VaceFunA14bDepthRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-22-vace-fun-a14b/depth/requests/{request_id}'
}

export type GetFalAiWan22VaceFunA14bDepthRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWan22VaceFunA14bDepthOutput
}

export type GetFalAiWan22VaceFunA14bDepthRequestsByRequestIdResponse =
  GetFalAiWan22VaceFunA14bDepthRequestsByRequestIdResponses[keyof GetFalAiWan22VaceFunA14bDepthRequestsByRequestIdResponses]

export type GetFalAiWan22VaceFunA14bPoseRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/wan-22-vace-fun-a14b/pose/requests/{request_id}/status'
}

export type GetFalAiWan22VaceFunA14bPoseRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiWan22VaceFunA14bPoseRequestsByRequestIdStatusResponse =
  GetFalAiWan22VaceFunA14bPoseRequestsByRequestIdStatusResponses[keyof GetFalAiWan22VaceFunA14bPoseRequestsByRequestIdStatusResponses]

export type PutFalAiWan22VaceFunA14bPoseRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-22-vace-fun-a14b/pose/requests/{request_id}/cancel'
}

export type PutFalAiWan22VaceFunA14bPoseRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiWan22VaceFunA14bPoseRequestsByRequestIdCancelResponse =
  PutFalAiWan22VaceFunA14bPoseRequestsByRequestIdCancelResponses[keyof PutFalAiWan22VaceFunA14bPoseRequestsByRequestIdCancelResponses]

export type PostFalAiWan22VaceFunA14bPoseData = {
  body: SchemaWan22VaceFunA14bPoseInput
  path?: never
  query?: never
  url: '/fal-ai/wan-22-vace-fun-a14b/pose'
}

export type PostFalAiWan22VaceFunA14bPoseResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWan22VaceFunA14bPoseResponse =
  PostFalAiWan22VaceFunA14bPoseResponses[keyof PostFalAiWan22VaceFunA14bPoseResponses]

export type GetFalAiWan22VaceFunA14bPoseRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-22-vace-fun-a14b/pose/requests/{request_id}'
}

export type GetFalAiWan22VaceFunA14bPoseRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWan22VaceFunA14bPoseOutput
}

export type GetFalAiWan22VaceFunA14bPoseRequestsByRequestIdResponse =
  GetFalAiWan22VaceFunA14bPoseRequestsByRequestIdResponses[keyof GetFalAiWan22VaceFunA14bPoseRequestsByRequestIdResponses]

export type GetFalAiHunyuanVideoFoleyRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/hunyuan-video-foley/requests/{request_id}/status'
}

export type GetFalAiHunyuanVideoFoleyRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiHunyuanVideoFoleyRequestsByRequestIdStatusResponse =
  GetFalAiHunyuanVideoFoleyRequestsByRequestIdStatusResponses[keyof GetFalAiHunyuanVideoFoleyRequestsByRequestIdStatusResponses]

export type PutFalAiHunyuanVideoFoleyRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/hunyuan-video-foley/requests/{request_id}/cancel'
}

export type PutFalAiHunyuanVideoFoleyRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiHunyuanVideoFoleyRequestsByRequestIdCancelResponse =
  PutFalAiHunyuanVideoFoleyRequestsByRequestIdCancelResponses[keyof PutFalAiHunyuanVideoFoleyRequestsByRequestIdCancelResponses]

export type PostFalAiHunyuanVideoFoleyData = {
  body: SchemaHunyuanVideoFoleyInput
  path?: never
  query?: never
  url: '/fal-ai/hunyuan-video-foley'
}

export type PostFalAiHunyuanVideoFoleyResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiHunyuanVideoFoleyResponse =
  PostFalAiHunyuanVideoFoleyResponses[keyof PostFalAiHunyuanVideoFoleyResponses]

export type GetFalAiHunyuanVideoFoleyRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/hunyuan-video-foley/requests/{request_id}'
}

export type GetFalAiHunyuanVideoFoleyRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaHunyuanVideoFoleyOutput
}

export type GetFalAiHunyuanVideoFoleyRequestsByRequestIdResponse =
  GetFalAiHunyuanVideoFoleyRequestsByRequestIdResponses[keyof GetFalAiHunyuanVideoFoleyRequestsByRequestIdResponses]

export type GetFalAiSyncLipsyncV2ProRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/sync-lipsync/v2/pro/requests/{request_id}/status'
}

export type GetFalAiSyncLipsyncV2ProRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiSyncLipsyncV2ProRequestsByRequestIdStatusResponse =
  GetFalAiSyncLipsyncV2ProRequestsByRequestIdStatusResponses[keyof GetFalAiSyncLipsyncV2ProRequestsByRequestIdStatusResponses]

export type PutFalAiSyncLipsyncV2ProRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/sync-lipsync/v2/pro/requests/{request_id}/cancel'
}

export type PutFalAiSyncLipsyncV2ProRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiSyncLipsyncV2ProRequestsByRequestIdCancelResponse =
  PutFalAiSyncLipsyncV2ProRequestsByRequestIdCancelResponses[keyof PutFalAiSyncLipsyncV2ProRequestsByRequestIdCancelResponses]

export type PostFalAiSyncLipsyncV2ProData = {
  body: SchemaSyncLipsyncV2ProInput
  path?: never
  query?: never
  url: '/fal-ai/sync-lipsync/v2/pro'
}

export type PostFalAiSyncLipsyncV2ProResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiSyncLipsyncV2ProResponse =
  PostFalAiSyncLipsyncV2ProResponses[keyof PostFalAiSyncLipsyncV2ProResponses]

export type GetFalAiSyncLipsyncV2ProRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/sync-lipsync/v2/pro/requests/{request_id}'
}

export type GetFalAiSyncLipsyncV2ProRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSyncLipsyncV2ProOutput
}

export type GetFalAiSyncLipsyncV2ProRequestsByRequestIdResponse =
  GetFalAiSyncLipsyncV2ProRequestsByRequestIdResponses[keyof GetFalAiSyncLipsyncV2ProRequestsByRequestIdResponses]

export type GetFalAiWanFunControlRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/wan-fun-control/requests/{request_id}/status'
}

export type GetFalAiWanFunControlRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiWanFunControlRequestsByRequestIdStatusResponse =
  GetFalAiWanFunControlRequestsByRequestIdStatusResponses[keyof GetFalAiWanFunControlRequestsByRequestIdStatusResponses]

export type PutFalAiWanFunControlRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-fun-control/requests/{request_id}/cancel'
}

export type PutFalAiWanFunControlRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiWanFunControlRequestsByRequestIdCancelResponse =
  PutFalAiWanFunControlRequestsByRequestIdCancelResponses[keyof PutFalAiWanFunControlRequestsByRequestIdCancelResponses]

export type PostFalAiWanFunControlData = {
  body: SchemaWanFunControlInput
  path?: never
  query?: never
  url: '/fal-ai/wan-fun-control'
}

export type PostFalAiWanFunControlResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWanFunControlResponse =
  PostFalAiWanFunControlResponses[keyof PostFalAiWanFunControlResponses]

export type GetFalAiWanFunControlRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-fun-control/requests/{request_id}'
}

export type GetFalAiWanFunControlRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWanFunControlOutput
}

export type GetFalAiWanFunControlRequestsByRequestIdResponse =
  GetFalAiWanFunControlRequestsByRequestIdResponses[keyof GetFalAiWanFunControlRequestsByRequestIdResponses]

export type GetBriaVideoIncreaseResolutionRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/bria/video/increase-resolution/requests/{request_id}/status'
}

export type GetBriaVideoIncreaseResolutionRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetBriaVideoIncreaseResolutionRequestsByRequestIdStatusResponse =
  GetBriaVideoIncreaseResolutionRequestsByRequestIdStatusResponses[keyof GetBriaVideoIncreaseResolutionRequestsByRequestIdStatusResponses]

export type PutBriaVideoIncreaseResolutionRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/bria/video/increase-resolution/requests/{request_id}/cancel'
}

export type PutBriaVideoIncreaseResolutionRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutBriaVideoIncreaseResolutionRequestsByRequestIdCancelResponse =
  PutBriaVideoIncreaseResolutionRequestsByRequestIdCancelResponses[keyof PutBriaVideoIncreaseResolutionRequestsByRequestIdCancelResponses]

export type PostBriaVideoIncreaseResolutionData = {
  body: SchemaVideoIncreaseResolutionInput
  path?: never
  query?: never
  url: '/bria/video/increase-resolution'
}

export type PostBriaVideoIncreaseResolutionResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostBriaVideoIncreaseResolutionResponse =
  PostBriaVideoIncreaseResolutionResponses[keyof PostBriaVideoIncreaseResolutionResponses]

export type GetBriaVideoIncreaseResolutionRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/bria/video/increase-resolution/requests/{request_id}'
}

export type GetBriaVideoIncreaseResolutionRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaVideoIncreaseResolutionOutput
}

export type GetBriaVideoIncreaseResolutionRequestsByRequestIdResponse =
  GetBriaVideoIncreaseResolutionRequestsByRequestIdResponses[keyof GetBriaVideoIncreaseResolutionRequestsByRequestIdResponses]

export type GetFalAiInfinitalkRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/infinitalk/requests/{request_id}/status'
}

export type GetFalAiInfinitalkRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiInfinitalkRequestsByRequestIdStatusResponse =
  GetFalAiInfinitalkRequestsByRequestIdStatusResponses[keyof GetFalAiInfinitalkRequestsByRequestIdStatusResponses]

export type PutFalAiInfinitalkRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/infinitalk/requests/{request_id}/cancel'
}

export type PutFalAiInfinitalkRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiInfinitalkRequestsByRequestIdCancelResponse =
  PutFalAiInfinitalkRequestsByRequestIdCancelResponses[keyof PutFalAiInfinitalkRequestsByRequestIdCancelResponses]

export type PostFalAiInfinitalkData = {
  body: SchemaInfinitalkInput
  path?: never
  query?: never
  url: '/fal-ai/infinitalk'
}

export type PostFalAiInfinitalkResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiInfinitalkResponse =
  PostFalAiInfinitalkResponses[keyof PostFalAiInfinitalkResponses]

export type GetFalAiInfinitalkRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/infinitalk/requests/{request_id}'
}

export type GetFalAiInfinitalkRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaInfinitalkOutput
}

export type GetFalAiInfinitalkRequestsByRequestIdResponse =
  GetFalAiInfinitalkRequestsByRequestIdResponses[keyof GetFalAiInfinitalkRequestsByRequestIdResponses]

export type GetMireloAiSfxV1VideoToVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/mirelo-ai/sfx-v1/video-to-video/requests/{request_id}/status'
}

export type GetMireloAiSfxV1VideoToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetMireloAiSfxV1VideoToVideoRequestsByRequestIdStatusResponse =
  GetMireloAiSfxV1VideoToVideoRequestsByRequestIdStatusResponses[keyof GetMireloAiSfxV1VideoToVideoRequestsByRequestIdStatusResponses]

export type PutMireloAiSfxV1VideoToVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/mirelo-ai/sfx-v1/video-to-video/requests/{request_id}/cancel'
}

export type PutMireloAiSfxV1VideoToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutMireloAiSfxV1VideoToVideoRequestsByRequestIdCancelResponse =
  PutMireloAiSfxV1VideoToVideoRequestsByRequestIdCancelResponses[keyof PutMireloAiSfxV1VideoToVideoRequestsByRequestIdCancelResponses]

export type PostMireloAiSfxV1VideoToVideoData = {
  body: SchemaSfxV1VideoToVideoInput
  path?: never
  query?: never
  url: '/mirelo-ai/sfx-v1/video-to-video'
}

export type PostMireloAiSfxV1VideoToVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostMireloAiSfxV1VideoToVideoResponse =
  PostMireloAiSfxV1VideoToVideoResponses[keyof PostMireloAiSfxV1VideoToVideoResponses]

export type GetMireloAiSfxV1VideoToVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/mirelo-ai/sfx-v1/video-to-video/requests/{request_id}'
}

export type GetMireloAiSfxV1VideoToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSfxV1VideoToVideoOutput
}

export type GetMireloAiSfxV1VideoToVideoRequestsByRequestIdResponse =
  GetMireloAiSfxV1VideoToVideoRequestsByRequestIdResponses[keyof GetMireloAiSfxV1VideoToVideoRequestsByRequestIdResponses]

export type GetMoonvalleyMareyPoseTransferRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/moonvalley/marey/pose-transfer/requests/{request_id}/status'
}

export type GetMoonvalleyMareyPoseTransferRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetMoonvalleyMareyPoseTransferRequestsByRequestIdStatusResponse =
  GetMoonvalleyMareyPoseTransferRequestsByRequestIdStatusResponses[keyof GetMoonvalleyMareyPoseTransferRequestsByRequestIdStatusResponses]

export type PutMoonvalleyMareyPoseTransferRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/moonvalley/marey/pose-transfer/requests/{request_id}/cancel'
}

export type PutMoonvalleyMareyPoseTransferRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutMoonvalleyMareyPoseTransferRequestsByRequestIdCancelResponse =
  PutMoonvalleyMareyPoseTransferRequestsByRequestIdCancelResponses[keyof PutMoonvalleyMareyPoseTransferRequestsByRequestIdCancelResponses]

export type PostMoonvalleyMareyPoseTransferData = {
  body: SchemaMareyPoseTransferInput
  path?: never
  query?: never
  url: '/moonvalley/marey/pose-transfer'
}

export type PostMoonvalleyMareyPoseTransferResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostMoonvalleyMareyPoseTransferResponse =
  PostMoonvalleyMareyPoseTransferResponses[keyof PostMoonvalleyMareyPoseTransferResponses]

export type GetMoonvalleyMareyPoseTransferRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/moonvalley/marey/pose-transfer/requests/{request_id}'
}

export type GetMoonvalleyMareyPoseTransferRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaMareyPoseTransferOutput
}

export type GetMoonvalleyMareyPoseTransferRequestsByRequestIdResponse =
  GetMoonvalleyMareyPoseTransferRequestsByRequestIdResponses[keyof GetMoonvalleyMareyPoseTransferRequestsByRequestIdResponses]

export type GetMoonvalleyMareyMotionTransferRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/moonvalley/marey/motion-transfer/requests/{request_id}/status'
}

export type GetMoonvalleyMareyMotionTransferRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetMoonvalleyMareyMotionTransferRequestsByRequestIdStatusResponse =
  GetMoonvalleyMareyMotionTransferRequestsByRequestIdStatusResponses[keyof GetMoonvalleyMareyMotionTransferRequestsByRequestIdStatusResponses]

export type PutMoonvalleyMareyMotionTransferRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/moonvalley/marey/motion-transfer/requests/{request_id}/cancel'
}

export type PutMoonvalleyMareyMotionTransferRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutMoonvalleyMareyMotionTransferRequestsByRequestIdCancelResponse =
  PutMoonvalleyMareyMotionTransferRequestsByRequestIdCancelResponses[keyof PutMoonvalleyMareyMotionTransferRequestsByRequestIdCancelResponses]

export type PostMoonvalleyMareyMotionTransferData = {
  body: SchemaMareyMotionTransferInput
  path?: never
  query?: never
  url: '/moonvalley/marey/motion-transfer'
}

export type PostMoonvalleyMareyMotionTransferResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostMoonvalleyMareyMotionTransferResponse =
  PostMoonvalleyMareyMotionTransferResponses[keyof PostMoonvalleyMareyMotionTransferResponses]

export type GetMoonvalleyMareyMotionTransferRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/moonvalley/marey/motion-transfer/requests/{request_id}'
}

export type GetMoonvalleyMareyMotionTransferRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaMareyMotionTransferOutput
}

export type GetMoonvalleyMareyMotionTransferRequestsByRequestIdResponse =
  GetMoonvalleyMareyMotionTransferRequestsByRequestIdResponses[keyof GetMoonvalleyMareyMotionTransferRequestsByRequestIdResponses]

export type GetFalAiFfmpegApiMergeVideosRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/ffmpeg-api/merge-videos/requests/{request_id}/status'
}

export type GetFalAiFfmpegApiMergeVideosRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiFfmpegApiMergeVideosRequestsByRequestIdStatusResponse =
  GetFalAiFfmpegApiMergeVideosRequestsByRequestIdStatusResponses[keyof GetFalAiFfmpegApiMergeVideosRequestsByRequestIdStatusResponses]

export type PutFalAiFfmpegApiMergeVideosRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ffmpeg-api/merge-videos/requests/{request_id}/cancel'
}

export type PutFalAiFfmpegApiMergeVideosRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiFfmpegApiMergeVideosRequestsByRequestIdCancelResponse =
  PutFalAiFfmpegApiMergeVideosRequestsByRequestIdCancelResponses[keyof PutFalAiFfmpegApiMergeVideosRequestsByRequestIdCancelResponses]

export type PostFalAiFfmpegApiMergeVideosData = {
  body: SchemaFfmpegApiMergeVideosInput
  path?: never
  query?: never
  url: '/fal-ai/ffmpeg-api/merge-videos'
}

export type PostFalAiFfmpegApiMergeVideosResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiFfmpegApiMergeVideosResponse =
  PostFalAiFfmpegApiMergeVideosResponses[keyof PostFalAiFfmpegApiMergeVideosResponses]

export type GetFalAiFfmpegApiMergeVideosRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ffmpeg-api/merge-videos/requests/{request_id}'
}

export type GetFalAiFfmpegApiMergeVideosRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaFfmpegApiMergeVideosOutput
}

export type GetFalAiFfmpegApiMergeVideosRequestsByRequestIdResponse =
  GetFalAiFfmpegApiMergeVideosRequestsByRequestIdResponses[keyof GetFalAiFfmpegApiMergeVideosRequestsByRequestIdResponses]

export type GetFalAiWanV22A14bVideoToVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/wan/v2.2-a14b/video-to-video/requests/{request_id}/status'
}

export type GetFalAiWanV22A14bVideoToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiWanV22A14bVideoToVideoRequestsByRequestIdStatusResponse =
  GetFalAiWanV22A14bVideoToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiWanV22A14bVideoToVideoRequestsByRequestIdStatusResponses]

export type PutFalAiWanV22A14bVideoToVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan/v2.2-a14b/video-to-video/requests/{request_id}/cancel'
}

export type PutFalAiWanV22A14bVideoToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiWanV22A14bVideoToVideoRequestsByRequestIdCancelResponse =
  PutFalAiWanV22A14bVideoToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiWanV22A14bVideoToVideoRequestsByRequestIdCancelResponses]

export type PostFalAiWanV22A14bVideoToVideoData = {
  body: SchemaWanV22A14bVideoToVideoInput
  path?: never
  query?: never
  url: '/fal-ai/wan/v2.2-a14b/video-to-video'
}

export type PostFalAiWanV22A14bVideoToVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWanV22A14bVideoToVideoResponse =
  PostFalAiWanV22A14bVideoToVideoResponses[keyof PostFalAiWanV22A14bVideoToVideoResponses]

export type GetFalAiWanV22A14bVideoToVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan/v2.2-a14b/video-to-video/requests/{request_id}'
}

export type GetFalAiWanV22A14bVideoToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWanV22A14bVideoToVideoOutput
}

export type GetFalAiWanV22A14bVideoToVideoRequestsByRequestIdResponse =
  GetFalAiWanV22A14bVideoToVideoRequestsByRequestIdResponses[keyof GetFalAiWanV22A14bVideoToVideoRequestsByRequestIdResponses]

export type GetFalAiLtxv13B098DistilledExtendRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/ltxv-13b-098-distilled/extend/requests/{request_id}/status'
}

export type GetFalAiLtxv13B098DistilledExtendRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiLtxv13B098DistilledExtendRequestsByRequestIdStatusResponse =
  GetFalAiLtxv13B098DistilledExtendRequestsByRequestIdStatusResponses[keyof GetFalAiLtxv13B098DistilledExtendRequestsByRequestIdStatusResponses]

export type PutFalAiLtxv13B098DistilledExtendRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltxv-13b-098-distilled/extend/requests/{request_id}/cancel'
}

export type PutFalAiLtxv13B098DistilledExtendRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiLtxv13B098DistilledExtendRequestsByRequestIdCancelResponse =
  PutFalAiLtxv13B098DistilledExtendRequestsByRequestIdCancelResponses[keyof PutFalAiLtxv13B098DistilledExtendRequestsByRequestIdCancelResponses]

export type PostFalAiLtxv13B098DistilledExtendData = {
  body: SchemaLtxv13B098DistilledExtendInput
  path?: never
  query?: never
  url: '/fal-ai/ltxv-13b-098-distilled/extend'
}

export type PostFalAiLtxv13B098DistilledExtendResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLtxv13B098DistilledExtendResponse =
  PostFalAiLtxv13B098DistilledExtendResponses[keyof PostFalAiLtxv13B098DistilledExtendResponses]

export type GetFalAiLtxv13B098DistilledExtendRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltxv-13b-098-distilled/extend/requests/{request_id}'
}

export type GetFalAiLtxv13B098DistilledExtendRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLtxv13B098DistilledExtendOutput
}

export type GetFalAiLtxv13B098DistilledExtendRequestsByRequestIdResponse =
  GetFalAiLtxv13B098DistilledExtendRequestsByRequestIdResponses[keyof GetFalAiLtxv13B098DistilledExtendRequestsByRequestIdResponses]

export type GetFalAiRifeVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/rife/video/requests/{request_id}/status'
}

export type GetFalAiRifeVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiRifeVideoRequestsByRequestIdStatusResponse =
  GetFalAiRifeVideoRequestsByRequestIdStatusResponses[keyof GetFalAiRifeVideoRequestsByRequestIdStatusResponses]

export type PutFalAiRifeVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/rife/video/requests/{request_id}/cancel'
}

export type PutFalAiRifeVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiRifeVideoRequestsByRequestIdCancelResponse =
  PutFalAiRifeVideoRequestsByRequestIdCancelResponses[keyof PutFalAiRifeVideoRequestsByRequestIdCancelResponses]

export type PostFalAiRifeVideoData = {
  body: SchemaRifeVideoInput
  path?: never
  query?: never
  url: '/fal-ai/rife/video'
}

export type PostFalAiRifeVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiRifeVideoResponse =
  PostFalAiRifeVideoResponses[keyof PostFalAiRifeVideoResponses]

export type GetFalAiRifeVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/rife/video/requests/{request_id}'
}

export type GetFalAiRifeVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaRifeVideoOutput
}

export type GetFalAiRifeVideoRequestsByRequestIdResponse =
  GetFalAiRifeVideoRequestsByRequestIdResponses[keyof GetFalAiRifeVideoRequestsByRequestIdResponses]

export type GetFalAiFilmVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/film/video/requests/{request_id}/status'
}

export type GetFalAiFilmVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiFilmVideoRequestsByRequestIdStatusResponse =
  GetFalAiFilmVideoRequestsByRequestIdStatusResponses[keyof GetFalAiFilmVideoRequestsByRequestIdStatusResponses]

export type PutFalAiFilmVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/film/video/requests/{request_id}/cancel'
}

export type PutFalAiFilmVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiFilmVideoRequestsByRequestIdCancelResponse =
  PutFalAiFilmVideoRequestsByRequestIdCancelResponses[keyof PutFalAiFilmVideoRequestsByRequestIdCancelResponses]

export type PostFalAiFilmVideoData = {
  body: SchemaFilmVideoInput
  path?: never
  query?: never
  url: '/fal-ai/film/video'
}

export type PostFalAiFilmVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiFilmVideoResponse =
  PostFalAiFilmVideoResponses[keyof PostFalAiFilmVideoResponses]

export type GetFalAiFilmVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/film/video/requests/{request_id}'
}

export type GetFalAiFilmVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaFilmVideoOutput
}

export type GetFalAiFilmVideoRequestsByRequestIdResponse =
  GetFalAiFilmVideoRequestsByRequestIdResponses[keyof GetFalAiFilmVideoRequestsByRequestIdResponses]

export type GetFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/luma-dream-machine/ray-2-flash/modify/requests/{request_id}/status'
  }

export type GetFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdStatusResponse =
  GetFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdStatusResponses[keyof GetFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdStatusResponses]

export type PutFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/luma-dream-machine/ray-2-flash/modify/requests/{request_id}/cancel'
  }

export type PutFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdCancelResponse =
  PutFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdCancelResponses[keyof PutFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdCancelResponses]

export type PostFalAiLumaDreamMachineRay2FlashModifyData = {
  body: SchemaLumaDreamMachineRay2FlashModifyInput
  path?: never
  query?: never
  url: '/fal-ai/luma-dream-machine/ray-2-flash/modify'
}

export type PostFalAiLumaDreamMachineRay2FlashModifyResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLumaDreamMachineRay2FlashModifyResponse =
  PostFalAiLumaDreamMachineRay2FlashModifyResponses[keyof PostFalAiLumaDreamMachineRay2FlashModifyResponses]

export type GetFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/luma-dream-machine/ray-2-flash/modify/requests/{request_id}'
}

export type GetFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaLumaDreamMachineRay2FlashModifyOutput
  }

export type GetFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdResponse =
  GetFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdResponses[keyof GetFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdResponses]

export type GetFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/ltxv-13b-098-distilled/multiconditioning/requests/{request_id}/status'
  }

export type GetFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdStatusResponse =
  GetFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdStatusResponses[keyof GetFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdStatusResponses]

export type PutFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/ltxv-13b-098-distilled/multiconditioning/requests/{request_id}/cancel'
  }

export type PutFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdCancelResponse =
  PutFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdCancelResponses[keyof PutFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdCancelResponses]

export type PostFalAiLtxv13B098DistilledMulticonditioningData = {
  body: SchemaLtxv13B098DistilledMulticonditioningInput
  path?: never
  query?: never
  url: '/fal-ai/ltxv-13b-098-distilled/multiconditioning'
}

export type PostFalAiLtxv13B098DistilledMulticonditioningResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLtxv13B098DistilledMulticonditioningResponse =
  PostFalAiLtxv13B098DistilledMulticonditioningResponses[keyof PostFalAiLtxv13B098DistilledMulticonditioningResponses]

export type GetFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/ltxv-13b-098-distilled/multiconditioning/requests/{request_id}'
  }

export type GetFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaLtxv13B098DistilledMulticonditioningOutput
  }

export type GetFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdResponse =
  GetFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdResponses[keyof GetFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdResponses]

export type GetFalAiPixverseSoundEffectsRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/pixverse/sound-effects/requests/{request_id}/status'
}

export type GetFalAiPixverseSoundEffectsRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiPixverseSoundEffectsRequestsByRequestIdStatusResponse =
  GetFalAiPixverseSoundEffectsRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseSoundEffectsRequestsByRequestIdStatusResponses]

export type PutFalAiPixverseSoundEffectsRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/pixverse/sound-effects/requests/{request_id}/cancel'
}

export type PutFalAiPixverseSoundEffectsRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiPixverseSoundEffectsRequestsByRequestIdCancelResponse =
  PutFalAiPixverseSoundEffectsRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseSoundEffectsRequestsByRequestIdCancelResponses]

export type PostFalAiPixverseSoundEffectsData = {
  body: SchemaPixverseSoundEffectsInput
  path?: never
  query?: never
  url: '/fal-ai/pixverse/sound-effects'
}

export type PostFalAiPixverseSoundEffectsResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiPixverseSoundEffectsResponse =
  PostFalAiPixverseSoundEffectsResponses[keyof PostFalAiPixverseSoundEffectsResponses]

export type GetFalAiPixverseSoundEffectsRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/pixverse/sound-effects/requests/{request_id}'
}

export type GetFalAiPixverseSoundEffectsRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaPixverseSoundEffectsOutput
}

export type GetFalAiPixverseSoundEffectsRequestsByRequestIdResponse =
  GetFalAiPixverseSoundEffectsRequestsByRequestIdResponses[keyof GetFalAiPixverseSoundEffectsRequestsByRequestIdResponses]

export type GetFalAiThinksoundAudioRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/thinksound/audio/requests/{request_id}/status'
}

export type GetFalAiThinksoundAudioRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiThinksoundAudioRequestsByRequestIdStatusResponse =
  GetFalAiThinksoundAudioRequestsByRequestIdStatusResponses[keyof GetFalAiThinksoundAudioRequestsByRequestIdStatusResponses]

export type PutFalAiThinksoundAudioRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/thinksound/audio/requests/{request_id}/cancel'
}

export type PutFalAiThinksoundAudioRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiThinksoundAudioRequestsByRequestIdCancelResponse =
  PutFalAiThinksoundAudioRequestsByRequestIdCancelResponses[keyof PutFalAiThinksoundAudioRequestsByRequestIdCancelResponses]

export type PostFalAiThinksoundAudioData = {
  body: SchemaThinksoundAudioInput
  path?: never
  query?: never
  url: '/fal-ai/thinksound/audio'
}

export type PostFalAiThinksoundAudioResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiThinksoundAudioResponse =
  PostFalAiThinksoundAudioResponses[keyof PostFalAiThinksoundAudioResponses]

export type GetFalAiThinksoundAudioRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/thinksound/audio/requests/{request_id}'
}

export type GetFalAiThinksoundAudioRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaThinksoundAudioOutput
}

export type GetFalAiThinksoundAudioRequestsByRequestIdResponse =
  GetFalAiThinksoundAudioRequestsByRequestIdResponses[keyof GetFalAiThinksoundAudioRequestsByRequestIdResponses]

export type GetFalAiThinksoundRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/thinksound/requests/{request_id}/status'
}

export type GetFalAiThinksoundRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiThinksoundRequestsByRequestIdStatusResponse =
  GetFalAiThinksoundRequestsByRequestIdStatusResponses[keyof GetFalAiThinksoundRequestsByRequestIdStatusResponses]

export type PutFalAiThinksoundRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/thinksound/requests/{request_id}/cancel'
}

export type PutFalAiThinksoundRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiThinksoundRequestsByRequestIdCancelResponse =
  PutFalAiThinksoundRequestsByRequestIdCancelResponses[keyof PutFalAiThinksoundRequestsByRequestIdCancelResponses]

export type PostFalAiThinksoundData = {
  body: SchemaThinksoundInput
  path?: never
  query?: never
  url: '/fal-ai/thinksound'
}

export type PostFalAiThinksoundResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiThinksoundResponse =
  PostFalAiThinksoundResponses[keyof PostFalAiThinksoundResponses]

export type GetFalAiThinksoundRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/thinksound/requests/{request_id}'
}

export type GetFalAiThinksoundRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaThinksoundOutput
}

export type GetFalAiThinksoundRequestsByRequestIdResponse =
  GetFalAiThinksoundRequestsByRequestIdResponses[keyof GetFalAiThinksoundRequestsByRequestIdResponses]

export type GetFalAiPixverseExtendFastRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/pixverse/extend/fast/requests/{request_id}/status'
}

export type GetFalAiPixverseExtendFastRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiPixverseExtendFastRequestsByRequestIdStatusResponse =
  GetFalAiPixverseExtendFastRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseExtendFastRequestsByRequestIdStatusResponses]

export type PutFalAiPixverseExtendFastRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/pixverse/extend/fast/requests/{request_id}/cancel'
}

export type PutFalAiPixverseExtendFastRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiPixverseExtendFastRequestsByRequestIdCancelResponse =
  PutFalAiPixverseExtendFastRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseExtendFastRequestsByRequestIdCancelResponses]

export type PostFalAiPixverseExtendFastData = {
  body: SchemaPixverseExtendFastInput
  path?: never
  query?: never
  url: '/fal-ai/pixverse/extend/fast'
}

export type PostFalAiPixverseExtendFastResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiPixverseExtendFastResponse =
  PostFalAiPixverseExtendFastResponses[keyof PostFalAiPixverseExtendFastResponses]

export type GetFalAiPixverseExtendFastRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/pixverse/extend/fast/requests/{request_id}'
}

export type GetFalAiPixverseExtendFastRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaPixverseExtendFastOutput
}

export type GetFalAiPixverseExtendFastRequestsByRequestIdResponse =
  GetFalAiPixverseExtendFastRequestsByRequestIdResponses[keyof GetFalAiPixverseExtendFastRequestsByRequestIdResponses]

export type GetFalAiPixverseExtendRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/pixverse/extend/requests/{request_id}/status'
}

export type GetFalAiPixverseExtendRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiPixverseExtendRequestsByRequestIdStatusResponse =
  GetFalAiPixverseExtendRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseExtendRequestsByRequestIdStatusResponses]

export type PutFalAiPixverseExtendRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/pixverse/extend/requests/{request_id}/cancel'
}

export type PutFalAiPixverseExtendRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiPixverseExtendRequestsByRequestIdCancelResponse =
  PutFalAiPixverseExtendRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseExtendRequestsByRequestIdCancelResponses]

export type PostFalAiPixverseExtendData = {
  body: SchemaPixverseExtendInput
  path?: never
  query?: never
  url: '/fal-ai/pixverse/extend'
}

export type PostFalAiPixverseExtendResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiPixverseExtendResponse =
  PostFalAiPixverseExtendResponses[keyof PostFalAiPixverseExtendResponses]

export type GetFalAiPixverseExtendRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/pixverse/extend/requests/{request_id}'
}

export type GetFalAiPixverseExtendRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaPixverseExtendOutput
}

export type GetFalAiPixverseExtendRequestsByRequestIdResponse =
  GetFalAiPixverseExtendRequestsByRequestIdResponses[keyof GetFalAiPixverseExtendRequestsByRequestIdResponses]

export type GetFalAiPixverseLipsyncRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/pixverse/lipsync/requests/{request_id}/status'
}

export type GetFalAiPixverseLipsyncRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiPixverseLipsyncRequestsByRequestIdStatusResponse =
  GetFalAiPixverseLipsyncRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseLipsyncRequestsByRequestIdStatusResponses]

export type PutFalAiPixverseLipsyncRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/pixverse/lipsync/requests/{request_id}/cancel'
}

export type PutFalAiPixverseLipsyncRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiPixverseLipsyncRequestsByRequestIdCancelResponse =
  PutFalAiPixverseLipsyncRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseLipsyncRequestsByRequestIdCancelResponses]

export type PostFalAiPixverseLipsyncData = {
  body: SchemaPixverseLipsyncInput
  path?: never
  query?: never
  url: '/fal-ai/pixverse/lipsync'
}

export type PostFalAiPixverseLipsyncResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiPixverseLipsyncResponse =
  PostFalAiPixverseLipsyncResponses[keyof PostFalAiPixverseLipsyncResponses]

export type GetFalAiPixverseLipsyncRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/pixverse/lipsync/requests/{request_id}'
}

export type GetFalAiPixverseLipsyncRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaPixverseLipsyncOutput
}

export type GetFalAiPixverseLipsyncRequestsByRequestIdResponse =
  GetFalAiPixverseLipsyncRequestsByRequestIdResponses[keyof GetFalAiPixverseLipsyncRequestsByRequestIdResponses]

export type GetFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/luma-dream-machine/ray-2/modify/requests/{request_id}/status'
}

export type GetFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdStatusResponse =
  GetFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdStatusResponses[keyof GetFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdStatusResponses]

export type PutFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/luma-dream-machine/ray-2/modify/requests/{request_id}/cancel'
}

export type PutFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdCancelResponse =
  PutFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdCancelResponses[keyof PutFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdCancelResponses]

export type PostFalAiLumaDreamMachineRay2ModifyData = {
  body: SchemaLumaDreamMachineRay2ModifyInput
  path?: never
  query?: never
  url: '/fal-ai/luma-dream-machine/ray-2/modify'
}

export type PostFalAiLumaDreamMachineRay2ModifyResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLumaDreamMachineRay2ModifyResponse =
  PostFalAiLumaDreamMachineRay2ModifyResponses[keyof PostFalAiLumaDreamMachineRay2ModifyResponses]

export type GetFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/luma-dream-machine/ray-2/modify/requests/{request_id}'
}

export type GetFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLumaDreamMachineRay2ModifyOutput
}

export type GetFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdResponse =
  GetFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdResponses[keyof GetFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdResponses]

export type GetFalAiWanVace14bReframeRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/wan-vace-14b/reframe/requests/{request_id}/status'
}

export type GetFalAiWanVace14bReframeRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiWanVace14bReframeRequestsByRequestIdStatusResponse =
  GetFalAiWanVace14bReframeRequestsByRequestIdStatusResponses[keyof GetFalAiWanVace14bReframeRequestsByRequestIdStatusResponses]

export type PutFalAiWanVace14bReframeRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-vace-14b/reframe/requests/{request_id}/cancel'
}

export type PutFalAiWanVace14bReframeRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiWanVace14bReframeRequestsByRequestIdCancelResponse =
  PutFalAiWanVace14bReframeRequestsByRequestIdCancelResponses[keyof PutFalAiWanVace14bReframeRequestsByRequestIdCancelResponses]

export type PostFalAiWanVace14bReframeData = {
  body: SchemaWanVace14bReframeInput
  path?: never
  query?: never
  url: '/fal-ai/wan-vace-14b/reframe'
}

export type PostFalAiWanVace14bReframeResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWanVace14bReframeResponse =
  PostFalAiWanVace14bReframeResponses[keyof PostFalAiWanVace14bReframeResponses]

export type GetFalAiWanVace14bReframeRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-vace-14b/reframe/requests/{request_id}'
}

export type GetFalAiWanVace14bReframeRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWanVace14bReframeOutput
}

export type GetFalAiWanVace14bReframeRequestsByRequestIdResponse =
  GetFalAiWanVace14bReframeRequestsByRequestIdResponses[keyof GetFalAiWanVace14bReframeRequestsByRequestIdResponses]

export type GetFalAiWanVace14bOutpaintingRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/wan-vace-14b/outpainting/requests/{request_id}/status'
}

export type GetFalAiWanVace14bOutpaintingRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiWanVace14bOutpaintingRequestsByRequestIdStatusResponse =
  GetFalAiWanVace14bOutpaintingRequestsByRequestIdStatusResponses[keyof GetFalAiWanVace14bOutpaintingRequestsByRequestIdStatusResponses]

export type PutFalAiWanVace14bOutpaintingRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-vace-14b/outpainting/requests/{request_id}/cancel'
}

export type PutFalAiWanVace14bOutpaintingRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiWanVace14bOutpaintingRequestsByRequestIdCancelResponse =
  PutFalAiWanVace14bOutpaintingRequestsByRequestIdCancelResponses[keyof PutFalAiWanVace14bOutpaintingRequestsByRequestIdCancelResponses]

export type PostFalAiWanVace14bOutpaintingData = {
  body: SchemaWanVace14bOutpaintingInput
  path?: never
  query?: never
  url: '/fal-ai/wan-vace-14b/outpainting'
}

export type PostFalAiWanVace14bOutpaintingResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWanVace14bOutpaintingResponse =
  PostFalAiWanVace14bOutpaintingResponses[keyof PostFalAiWanVace14bOutpaintingResponses]

export type GetFalAiWanVace14bOutpaintingRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-vace-14b/outpainting/requests/{request_id}'
}

export type GetFalAiWanVace14bOutpaintingRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWanVace14bOutpaintingOutput
}

export type GetFalAiWanVace14bOutpaintingRequestsByRequestIdResponse =
  GetFalAiWanVace14bOutpaintingRequestsByRequestIdResponses[keyof GetFalAiWanVace14bOutpaintingRequestsByRequestIdResponses]

export type GetFalAiWanVace14bInpaintingRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/wan-vace-14b/inpainting/requests/{request_id}/status'
}

export type GetFalAiWanVace14bInpaintingRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiWanVace14bInpaintingRequestsByRequestIdStatusResponse =
  GetFalAiWanVace14bInpaintingRequestsByRequestIdStatusResponses[keyof GetFalAiWanVace14bInpaintingRequestsByRequestIdStatusResponses]

export type PutFalAiWanVace14bInpaintingRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-vace-14b/inpainting/requests/{request_id}/cancel'
}

export type PutFalAiWanVace14bInpaintingRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiWanVace14bInpaintingRequestsByRequestIdCancelResponse =
  PutFalAiWanVace14bInpaintingRequestsByRequestIdCancelResponses[keyof PutFalAiWanVace14bInpaintingRequestsByRequestIdCancelResponses]

export type PostFalAiWanVace14bInpaintingData = {
  body: SchemaWanVace14bInpaintingInput
  path?: never
  query?: never
  url: '/fal-ai/wan-vace-14b/inpainting'
}

export type PostFalAiWanVace14bInpaintingResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWanVace14bInpaintingResponse =
  PostFalAiWanVace14bInpaintingResponses[keyof PostFalAiWanVace14bInpaintingResponses]

export type GetFalAiWanVace14bInpaintingRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-vace-14b/inpainting/requests/{request_id}'
}

export type GetFalAiWanVace14bInpaintingRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWanVace14bInpaintingOutput
}

export type GetFalAiWanVace14bInpaintingRequestsByRequestIdResponse =
  GetFalAiWanVace14bInpaintingRequestsByRequestIdResponses[keyof GetFalAiWanVace14bInpaintingRequestsByRequestIdResponses]

export type GetFalAiWanVace14bPoseRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/wan-vace-14b/pose/requests/{request_id}/status'
}

export type GetFalAiWanVace14bPoseRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiWanVace14bPoseRequestsByRequestIdStatusResponse =
  GetFalAiWanVace14bPoseRequestsByRequestIdStatusResponses[keyof GetFalAiWanVace14bPoseRequestsByRequestIdStatusResponses]

export type PutFalAiWanVace14bPoseRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-vace-14b/pose/requests/{request_id}/cancel'
}

export type PutFalAiWanVace14bPoseRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiWanVace14bPoseRequestsByRequestIdCancelResponse =
  PutFalAiWanVace14bPoseRequestsByRequestIdCancelResponses[keyof PutFalAiWanVace14bPoseRequestsByRequestIdCancelResponses]

export type PostFalAiWanVace14bPoseData = {
  body: SchemaWanVace14bPoseInput
  path?: never
  query?: never
  url: '/fal-ai/wan-vace-14b/pose'
}

export type PostFalAiWanVace14bPoseResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWanVace14bPoseResponse =
  PostFalAiWanVace14bPoseResponses[keyof PostFalAiWanVace14bPoseResponses]

export type GetFalAiWanVace14bPoseRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-vace-14b/pose/requests/{request_id}'
}

export type GetFalAiWanVace14bPoseRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWanVace14bPoseOutput
}

export type GetFalAiWanVace14bPoseRequestsByRequestIdResponse =
  GetFalAiWanVace14bPoseRequestsByRequestIdResponses[keyof GetFalAiWanVace14bPoseRequestsByRequestIdResponses]

export type GetFalAiWanVace14bDepthRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/wan-vace-14b/depth/requests/{request_id}/status'
}

export type GetFalAiWanVace14bDepthRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiWanVace14bDepthRequestsByRequestIdStatusResponse =
  GetFalAiWanVace14bDepthRequestsByRequestIdStatusResponses[keyof GetFalAiWanVace14bDepthRequestsByRequestIdStatusResponses]

export type PutFalAiWanVace14bDepthRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-vace-14b/depth/requests/{request_id}/cancel'
}

export type PutFalAiWanVace14bDepthRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiWanVace14bDepthRequestsByRequestIdCancelResponse =
  PutFalAiWanVace14bDepthRequestsByRequestIdCancelResponses[keyof PutFalAiWanVace14bDepthRequestsByRequestIdCancelResponses]

export type PostFalAiWanVace14bDepthData = {
  body: SchemaWanVace14bDepthInput
  path?: never
  query?: never
  url: '/fal-ai/wan-vace-14b/depth'
}

export type PostFalAiWanVace14bDepthResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWanVace14bDepthResponse =
  PostFalAiWanVace14bDepthResponses[keyof PostFalAiWanVace14bDepthResponses]

export type GetFalAiWanVace14bDepthRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-vace-14b/depth/requests/{request_id}'
}

export type GetFalAiWanVace14bDepthRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWanVace14bDepthOutput
}

export type GetFalAiWanVace14bDepthRequestsByRequestIdResponse =
  GetFalAiWanVace14bDepthRequestsByRequestIdResponses[keyof GetFalAiWanVace14bDepthRequestsByRequestIdResponses]

export type GetFalAiDwposeVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/dwpose/video/requests/{request_id}/status'
}

export type GetFalAiDwposeVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiDwposeVideoRequestsByRequestIdStatusResponse =
  GetFalAiDwposeVideoRequestsByRequestIdStatusResponses[keyof GetFalAiDwposeVideoRequestsByRequestIdStatusResponses]

export type PutFalAiDwposeVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/dwpose/video/requests/{request_id}/cancel'
}

export type PutFalAiDwposeVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiDwposeVideoRequestsByRequestIdCancelResponse =
  PutFalAiDwposeVideoRequestsByRequestIdCancelResponses[keyof PutFalAiDwposeVideoRequestsByRequestIdCancelResponses]

export type PostFalAiDwposeVideoData = {
  body: SchemaDwposeVideoInput
  path?: never
  query?: never
  url: '/fal-ai/dwpose/video'
}

export type PostFalAiDwposeVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiDwposeVideoResponse =
  PostFalAiDwposeVideoResponses[keyof PostFalAiDwposeVideoResponses]

export type GetFalAiDwposeVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/dwpose/video/requests/{request_id}'
}

export type GetFalAiDwposeVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaDwposeVideoOutput
}

export type GetFalAiDwposeVideoRequestsByRequestIdResponse =
  GetFalAiDwposeVideoRequestsByRequestIdResponses[keyof GetFalAiDwposeVideoRequestsByRequestIdResponses]

export type GetFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/ffmpeg-api/merge-audio-video/requests/{request_id}/status'
}

export type GetFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdStatusResponse =
  GetFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdStatusResponses[keyof GetFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdStatusResponses]

export type PutFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ffmpeg-api/merge-audio-video/requests/{request_id}/cancel'
}

export type PutFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdCancelResponse =
  PutFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdCancelResponses[keyof PutFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdCancelResponses]

export type PostFalAiFfmpegApiMergeAudioVideoData = {
  body: SchemaFfmpegApiMergeAudioVideoInput
  path?: never
  query?: never
  url: '/fal-ai/ffmpeg-api/merge-audio-video'
}

export type PostFalAiFfmpegApiMergeAudioVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiFfmpegApiMergeAudioVideoResponse =
  PostFalAiFfmpegApiMergeAudioVideoResponses[keyof PostFalAiFfmpegApiMergeAudioVideoResponses]

export type GetFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ffmpeg-api/merge-audio-video/requests/{request_id}'
}

export type GetFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaFfmpegApiMergeAudioVideoOutput
}

export type GetFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdResponse =
  GetFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdResponses[keyof GetFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdResponses]

export type GetFalAiWanVace13bRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/wan-vace-1-3b/requests/{request_id}/status'
}

export type GetFalAiWanVace13bRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiWanVace13bRequestsByRequestIdStatusResponse =
  GetFalAiWanVace13bRequestsByRequestIdStatusResponses[keyof GetFalAiWanVace13bRequestsByRequestIdStatusResponses]

export type PutFalAiWanVace13bRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-vace-1-3b/requests/{request_id}/cancel'
}

export type PutFalAiWanVace13bRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiWanVace13bRequestsByRequestIdCancelResponse =
  PutFalAiWanVace13bRequestsByRequestIdCancelResponses[keyof PutFalAiWanVace13bRequestsByRequestIdCancelResponses]

export type PostFalAiWanVace13bData = {
  body: SchemaWanVace13bInput
  path?: never
  query?: never
  url: '/fal-ai/wan-vace-1-3b'
}

export type PostFalAiWanVace13bResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWanVace13bResponse =
  PostFalAiWanVace13bResponses[keyof PostFalAiWanVace13bResponses]

export type GetFalAiWanVace13bRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-vace-1-3b/requests/{request_id}'
}

export type GetFalAiWanVace13bRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWanVace13bOutput
}

export type GetFalAiWanVace13bRequestsByRequestIdResponse =
  GetFalAiWanVace13bRequestsByRequestIdResponses[keyof GetFalAiWanVace13bRequestsByRequestIdResponses]

export type GetFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/luma-dream-machine/ray-2-flash/reframe/requests/{request_id}/status'
  }

export type GetFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdStatusResponse =
  GetFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdStatusResponses[keyof GetFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdStatusResponses]

export type PutFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/luma-dream-machine/ray-2-flash/reframe/requests/{request_id}/cancel'
  }

export type PutFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdCancelResponse =
  PutFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdCancelResponses[keyof PutFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdCancelResponses]

export type PostFalAiLumaDreamMachineRay2FlashReframeData = {
  body: SchemaLumaDreamMachineRay2FlashReframeInput
  path?: never
  query?: never
  url: '/fal-ai/luma-dream-machine/ray-2-flash/reframe'
}

export type PostFalAiLumaDreamMachineRay2FlashReframeResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLumaDreamMachineRay2FlashReframeResponse =
  PostFalAiLumaDreamMachineRay2FlashReframeResponses[keyof PostFalAiLumaDreamMachineRay2FlashReframeResponses]

export type GetFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/luma-dream-machine/ray-2-flash/reframe/requests/{request_id}'
}

export type GetFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaLumaDreamMachineRay2FlashReframeOutput
  }

export type GetFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdResponse =
  GetFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdResponses[keyof GetFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdResponses]

export type GetFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/luma-dream-machine/ray-2/reframe/requests/{request_id}/status'
}

export type GetFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdStatusResponse =
  GetFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdStatusResponses[keyof GetFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdStatusResponses]

export type PutFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/luma-dream-machine/ray-2/reframe/requests/{request_id}/cancel'
}

export type PutFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdCancelResponse =
  PutFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdCancelResponses[keyof PutFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdCancelResponses]

export type PostFalAiLumaDreamMachineRay2ReframeData = {
  body: SchemaLumaDreamMachineRay2ReframeInput
  path?: never
  query?: never
  url: '/fal-ai/luma-dream-machine/ray-2/reframe'
}

export type PostFalAiLumaDreamMachineRay2ReframeResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLumaDreamMachineRay2ReframeResponse =
  PostFalAiLumaDreamMachineRay2ReframeResponses[keyof PostFalAiLumaDreamMachineRay2ReframeResponses]

export type GetFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/luma-dream-machine/ray-2/reframe/requests/{request_id}'
}

export type GetFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLumaDreamMachineRay2ReframeOutput
}

export type GetFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdResponse =
  GetFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdResponses[keyof GetFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdResponses]

export type GetVeedLipsyncRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/veed/lipsync/requests/{request_id}/status'
}

export type GetVeedLipsyncRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetVeedLipsyncRequestsByRequestIdStatusResponse =
  GetVeedLipsyncRequestsByRequestIdStatusResponses[keyof GetVeedLipsyncRequestsByRequestIdStatusResponses]

export type PutVeedLipsyncRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/veed/lipsync/requests/{request_id}/cancel'
}

export type PutVeedLipsyncRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutVeedLipsyncRequestsByRequestIdCancelResponse =
  PutVeedLipsyncRequestsByRequestIdCancelResponses[keyof PutVeedLipsyncRequestsByRequestIdCancelResponses]

export type PostVeedLipsyncData = {
  body: SchemaLipsyncInput
  path?: never
  query?: never
  url: '/veed/lipsync'
}

export type PostVeedLipsyncResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostVeedLipsyncResponse =
  PostVeedLipsyncResponses[keyof PostVeedLipsyncResponses]

export type GetVeedLipsyncRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/veed/lipsync/requests/{request_id}'
}

export type GetVeedLipsyncRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLipsyncOutput
}

export type GetVeedLipsyncRequestsByRequestIdResponse =
  GetVeedLipsyncRequestsByRequestIdResponses[keyof GetVeedLipsyncRequestsByRequestIdResponses]

export type GetFalAiWanVace14bRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/wan-vace-14b/requests/{request_id}/status'
}

export type GetFalAiWanVace14bRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiWanVace14bRequestsByRequestIdStatusResponse =
  GetFalAiWanVace14bRequestsByRequestIdStatusResponses[keyof GetFalAiWanVace14bRequestsByRequestIdStatusResponses]

export type PutFalAiWanVace14bRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-vace-14b/requests/{request_id}/cancel'
}

export type PutFalAiWanVace14bRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiWanVace14bRequestsByRequestIdCancelResponse =
  PutFalAiWanVace14bRequestsByRequestIdCancelResponses[keyof PutFalAiWanVace14bRequestsByRequestIdCancelResponses]

export type PostFalAiWanVace14bData = {
  body: SchemaWanVace14bInput
  path?: never
  query?: never
  url: '/fal-ai/wan-vace-14b'
}

export type PostFalAiWanVace14bResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWanVace14bResponse =
  PostFalAiWanVace14bResponses[keyof PostFalAiWanVace14bResponses]

export type GetFalAiWanVace14bRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-vace-14b/requests/{request_id}'
}

export type GetFalAiWanVace14bRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWanVace14bOutput
}

export type GetFalAiWanVace14bRequestsByRequestIdResponse =
  GetFalAiWanVace14bRequestsByRequestIdResponses[keyof GetFalAiWanVace14bRequestsByRequestIdResponses]

export type GetFalAiLtxVideo13bDistilledExtendRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/ltx-video-13b-distilled/extend/requests/{request_id}/status'
}

export type GetFalAiLtxVideo13bDistilledExtendRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiLtxVideo13bDistilledExtendRequestsByRequestIdStatusResponse =
  GetFalAiLtxVideo13bDistilledExtendRequestsByRequestIdStatusResponses[keyof GetFalAiLtxVideo13bDistilledExtendRequestsByRequestIdStatusResponses]

export type PutFalAiLtxVideo13bDistilledExtendRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-video-13b-distilled/extend/requests/{request_id}/cancel'
}

export type PutFalAiLtxVideo13bDistilledExtendRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiLtxVideo13bDistilledExtendRequestsByRequestIdCancelResponse =
  PutFalAiLtxVideo13bDistilledExtendRequestsByRequestIdCancelResponses[keyof PutFalAiLtxVideo13bDistilledExtendRequestsByRequestIdCancelResponses]

export type PostFalAiLtxVideo13bDistilledExtendData = {
  body: SchemaLtxVideo13bDistilledExtendInput
  path?: never
  query?: never
  url: '/fal-ai/ltx-video-13b-distilled/extend'
}

export type PostFalAiLtxVideo13bDistilledExtendResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLtxVideo13bDistilledExtendResponse =
  PostFalAiLtxVideo13bDistilledExtendResponses[keyof PostFalAiLtxVideo13bDistilledExtendResponses]

export type GetFalAiLtxVideo13bDistilledExtendRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-video-13b-distilled/extend/requests/{request_id}'
}

export type GetFalAiLtxVideo13bDistilledExtendRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLtxVideo13bDistilledExtendOutput
}

export type GetFalAiLtxVideo13bDistilledExtendRequestsByRequestIdResponse =
  GetFalAiLtxVideo13bDistilledExtendRequestsByRequestIdResponses[keyof GetFalAiLtxVideo13bDistilledExtendRequestsByRequestIdResponses]

export type GetFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/ltx-video-13b-distilled/multiconditioning/requests/{request_id}/status'
  }

export type GetFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdStatusResponse =
  GetFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdStatusResponses[keyof GetFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdStatusResponses]

export type PutFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/ltx-video-13b-distilled/multiconditioning/requests/{request_id}/cancel'
  }

export type PutFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdCancelResponse =
  PutFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdCancelResponses[keyof PutFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdCancelResponses]

export type PostFalAiLtxVideo13bDistilledMulticonditioningData = {
  body: SchemaLtxVideo13bDistilledMulticonditioningInput
  path?: never
  query?: never
  url: '/fal-ai/ltx-video-13b-distilled/multiconditioning'
}

export type PostFalAiLtxVideo13bDistilledMulticonditioningResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLtxVideo13bDistilledMulticonditioningResponse =
  PostFalAiLtxVideo13bDistilledMulticonditioningResponses[keyof PostFalAiLtxVideo13bDistilledMulticonditioningResponses]

export type GetFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/ltx-video-13b-distilled/multiconditioning/requests/{request_id}'
  }

export type GetFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaLtxVideo13bDistilledMulticonditioningOutput
  }

export type GetFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdResponse =
  GetFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdResponses[keyof GetFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdResponses]

export type GetFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/ltx-video-13b-dev/multiconditioning/requests/{request_id}/status'
  }

export type GetFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdStatusResponse =
  GetFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdStatusResponses[keyof GetFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdStatusResponses]

export type PutFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/ltx-video-13b-dev/multiconditioning/requests/{request_id}/cancel'
  }

export type PutFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdCancelResponse =
  PutFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdCancelResponses[keyof PutFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdCancelResponses]

export type PostFalAiLtxVideo13bDevMulticonditioningData = {
  body: SchemaLtxVideo13bDevMulticonditioningInput
  path?: never
  query?: never
  url: '/fal-ai/ltx-video-13b-dev/multiconditioning'
}

export type PostFalAiLtxVideo13bDevMulticonditioningResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLtxVideo13bDevMulticonditioningResponse =
  PostFalAiLtxVideo13bDevMulticonditioningResponses[keyof PostFalAiLtxVideo13bDevMulticonditioningResponses]

export type GetFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-video-13b-dev/multiconditioning/requests/{request_id}'
}

export type GetFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaLtxVideo13bDevMulticonditioningOutput
  }

export type GetFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdResponse =
  GetFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdResponses[keyof GetFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdResponses]

export type GetFalAiLtxVideo13bDevExtendRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/ltx-video-13b-dev/extend/requests/{request_id}/status'
}

export type GetFalAiLtxVideo13bDevExtendRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiLtxVideo13bDevExtendRequestsByRequestIdStatusResponse =
  GetFalAiLtxVideo13bDevExtendRequestsByRequestIdStatusResponses[keyof GetFalAiLtxVideo13bDevExtendRequestsByRequestIdStatusResponses]

export type PutFalAiLtxVideo13bDevExtendRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-video-13b-dev/extend/requests/{request_id}/cancel'
}

export type PutFalAiLtxVideo13bDevExtendRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiLtxVideo13bDevExtendRequestsByRequestIdCancelResponse =
  PutFalAiLtxVideo13bDevExtendRequestsByRequestIdCancelResponses[keyof PutFalAiLtxVideo13bDevExtendRequestsByRequestIdCancelResponses]

export type PostFalAiLtxVideo13bDevExtendData = {
  body: SchemaLtxVideo13bDevExtendInput
  path?: never
  query?: never
  url: '/fal-ai/ltx-video-13b-dev/extend'
}

export type PostFalAiLtxVideo13bDevExtendResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLtxVideo13bDevExtendResponse =
  PostFalAiLtxVideo13bDevExtendResponses[keyof PostFalAiLtxVideo13bDevExtendResponses]

export type GetFalAiLtxVideo13bDevExtendRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-video-13b-dev/extend/requests/{request_id}'
}

export type GetFalAiLtxVideo13bDevExtendRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLtxVideo13bDevExtendOutput
}

export type GetFalAiLtxVideo13bDevExtendRequestsByRequestIdResponse =
  GetFalAiLtxVideo13bDevExtendRequestsByRequestIdResponses[keyof GetFalAiLtxVideo13bDevExtendRequestsByRequestIdResponses]

export type GetFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/ltx-video-lora/multiconditioning/requests/{request_id}/status'
  }

export type GetFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdStatusResponse =
  GetFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdStatusResponses[keyof GetFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdStatusResponses]

export type PutFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/ltx-video-lora/multiconditioning/requests/{request_id}/cancel'
  }

export type PutFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdCancelResponse =
  PutFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdCancelResponses[keyof PutFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdCancelResponses]

export type PostFalAiLtxVideoLoraMulticonditioningData = {
  body: SchemaLtxVideoLoraMulticonditioningInput
  path?: never
  query?: never
  url: '/fal-ai/ltx-video-lora/multiconditioning'
}

export type PostFalAiLtxVideoLoraMulticonditioningResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLtxVideoLoraMulticonditioningResponse =
  PostFalAiLtxVideoLoraMulticonditioningResponses[keyof PostFalAiLtxVideoLoraMulticonditioningResponses]

export type GetFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-video-lora/multiconditioning/requests/{request_id}'
}

export type GetFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaLtxVideoLoraMulticonditioningOutput
  }

export type GetFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdResponse =
  GetFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdResponses[keyof GetFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdResponses]

export type GetFalAiMagiExtendVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/magi/extend-video/requests/{request_id}/status'
}

export type GetFalAiMagiExtendVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiMagiExtendVideoRequestsByRequestIdStatusResponse =
  GetFalAiMagiExtendVideoRequestsByRequestIdStatusResponses[keyof GetFalAiMagiExtendVideoRequestsByRequestIdStatusResponses]

export type PutFalAiMagiExtendVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/magi/extend-video/requests/{request_id}/cancel'
}

export type PutFalAiMagiExtendVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiMagiExtendVideoRequestsByRequestIdCancelResponse =
  PutFalAiMagiExtendVideoRequestsByRequestIdCancelResponses[keyof PutFalAiMagiExtendVideoRequestsByRequestIdCancelResponses]

export type PostFalAiMagiExtendVideoData = {
  body: SchemaMagiExtendVideoInput
  path?: never
  query?: never
  url: '/fal-ai/magi/extend-video'
}

export type PostFalAiMagiExtendVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiMagiExtendVideoResponse =
  PostFalAiMagiExtendVideoResponses[keyof PostFalAiMagiExtendVideoResponses]

export type GetFalAiMagiExtendVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/magi/extend-video/requests/{request_id}'
}

export type GetFalAiMagiExtendVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaMagiExtendVideoOutput
}

export type GetFalAiMagiExtendVideoRequestsByRequestIdResponse =
  GetFalAiMagiExtendVideoRequestsByRequestIdResponses[keyof GetFalAiMagiExtendVideoRequestsByRequestIdResponses]

export type GetFalAiMagiDistilledExtendVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/magi-distilled/extend-video/requests/{request_id}/status'
}

export type GetFalAiMagiDistilledExtendVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiMagiDistilledExtendVideoRequestsByRequestIdStatusResponse =
  GetFalAiMagiDistilledExtendVideoRequestsByRequestIdStatusResponses[keyof GetFalAiMagiDistilledExtendVideoRequestsByRequestIdStatusResponses]

export type PutFalAiMagiDistilledExtendVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/magi-distilled/extend-video/requests/{request_id}/cancel'
}

export type PutFalAiMagiDistilledExtendVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiMagiDistilledExtendVideoRequestsByRequestIdCancelResponse =
  PutFalAiMagiDistilledExtendVideoRequestsByRequestIdCancelResponses[keyof PutFalAiMagiDistilledExtendVideoRequestsByRequestIdCancelResponses]

export type PostFalAiMagiDistilledExtendVideoData = {
  body: SchemaMagiDistilledExtendVideoInput
  path?: never
  query?: never
  url: '/fal-ai/magi-distilled/extend-video'
}

export type PostFalAiMagiDistilledExtendVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiMagiDistilledExtendVideoResponse =
  PostFalAiMagiDistilledExtendVideoResponses[keyof PostFalAiMagiDistilledExtendVideoResponses]

export type GetFalAiMagiDistilledExtendVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/magi-distilled/extend-video/requests/{request_id}'
}

export type GetFalAiMagiDistilledExtendVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaMagiDistilledExtendVideoOutput
}

export type GetFalAiMagiDistilledExtendVideoRequestsByRequestIdResponse =
  GetFalAiMagiDistilledExtendVideoRequestsByRequestIdResponses[keyof GetFalAiMagiDistilledExtendVideoRequestsByRequestIdResponses]

export type GetFalAiWanVaceRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/wan-vace/requests/{request_id}/status'
}

export type GetFalAiWanVaceRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiWanVaceRequestsByRequestIdStatusResponse =
  GetFalAiWanVaceRequestsByRequestIdStatusResponses[keyof GetFalAiWanVaceRequestsByRequestIdStatusResponses]

export type PutFalAiWanVaceRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-vace/requests/{request_id}/cancel'
}

export type PutFalAiWanVaceRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiWanVaceRequestsByRequestIdCancelResponse =
  PutFalAiWanVaceRequestsByRequestIdCancelResponses[keyof PutFalAiWanVaceRequestsByRequestIdCancelResponses]

export type PostFalAiWanVaceData = {
  body: SchemaWanVaceInput
  path?: never
  query?: never
  url: '/fal-ai/wan-vace'
}

export type PostFalAiWanVaceResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWanVaceResponse =
  PostFalAiWanVaceResponses[keyof PostFalAiWanVaceResponses]

export type GetFalAiWanVaceRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan-vace/requests/{request_id}'
}

export type GetFalAiWanVaceRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWanVaceOutput
}

export type GetFalAiWanVaceRequestsByRequestIdResponse =
  GetFalAiWanVaceRequestsByRequestIdResponses[keyof GetFalAiWanVaceRequestsByRequestIdResponses]

export type GetCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/cassetteai/video-sound-effects-generator/requests/{request_id}/status'
  }

export type GetCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdStatusResponse =
  GetCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdStatusResponses[keyof GetCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdStatusResponses]

export type PutCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/cassetteai/video-sound-effects-generator/requests/{request_id}/cancel'
  }

export type PutCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdCancelResponse =
  PutCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdCancelResponses[keyof PutCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdCancelResponses]

export type PostCassetteaiVideoSoundEffectsGeneratorData = {
  body: SchemaVideoSoundEffectsGeneratorInput
  path?: never
  query?: never
  url: '/cassetteai/video-sound-effects-generator'
}

export type PostCassetteaiVideoSoundEffectsGeneratorResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostCassetteaiVideoSoundEffectsGeneratorResponse =
  PostCassetteaiVideoSoundEffectsGeneratorResponses[keyof PostCassetteaiVideoSoundEffectsGeneratorResponses]

export type GetCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/cassetteai/video-sound-effects-generator/requests/{request_id}'
}

export type GetCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaVideoSoundEffectsGeneratorOutput
  }

export type GetCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdResponse =
  GetCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdResponses[keyof GetCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdResponses]

export type GetFalAiSyncLipsyncV2RequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/sync-lipsync/v2/requests/{request_id}/status'
}

export type GetFalAiSyncLipsyncV2RequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiSyncLipsyncV2RequestsByRequestIdStatusResponse =
  GetFalAiSyncLipsyncV2RequestsByRequestIdStatusResponses[keyof GetFalAiSyncLipsyncV2RequestsByRequestIdStatusResponses]

export type PutFalAiSyncLipsyncV2RequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/sync-lipsync/v2/requests/{request_id}/cancel'
}

export type PutFalAiSyncLipsyncV2RequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiSyncLipsyncV2RequestsByRequestIdCancelResponse =
  PutFalAiSyncLipsyncV2RequestsByRequestIdCancelResponses[keyof PutFalAiSyncLipsyncV2RequestsByRequestIdCancelResponses]

export type PostFalAiSyncLipsyncV2Data = {
  body: SchemaSyncLipsyncV2Input
  path?: never
  query?: never
  url: '/fal-ai/sync-lipsync/v2'
}

export type PostFalAiSyncLipsyncV2Responses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiSyncLipsyncV2Response =
  PostFalAiSyncLipsyncV2Responses[keyof PostFalAiSyncLipsyncV2Responses]

export type GetFalAiSyncLipsyncV2RequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/sync-lipsync/v2/requests/{request_id}'
}

export type GetFalAiSyncLipsyncV2RequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSyncLipsyncV2Output
}

export type GetFalAiSyncLipsyncV2RequestsByRequestIdResponse =
  GetFalAiSyncLipsyncV2RequestsByRequestIdResponses[keyof GetFalAiSyncLipsyncV2RequestsByRequestIdResponses]

export type GetFalAiLatentsyncRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/latentsync/requests/{request_id}/status'
}

export type GetFalAiLatentsyncRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiLatentsyncRequestsByRequestIdStatusResponse =
  GetFalAiLatentsyncRequestsByRequestIdStatusResponses[keyof GetFalAiLatentsyncRequestsByRequestIdStatusResponses]

export type PutFalAiLatentsyncRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/latentsync/requests/{request_id}/cancel'
}

export type PutFalAiLatentsyncRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiLatentsyncRequestsByRequestIdCancelResponse =
  PutFalAiLatentsyncRequestsByRequestIdCancelResponses[keyof PutFalAiLatentsyncRequestsByRequestIdCancelResponses]

export type PostFalAiLatentsyncData = {
  body: SchemaLatentsyncInput
  path?: never
  query?: never
  url: '/fal-ai/latentsync'
}

export type PostFalAiLatentsyncResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLatentsyncResponse =
  PostFalAiLatentsyncResponses[keyof PostFalAiLatentsyncResponses]

export type GetFalAiLatentsyncRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/latentsync/requests/{request_id}'
}

export type GetFalAiLatentsyncRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLatentsyncOutput
}

export type GetFalAiLatentsyncRequestsByRequestIdResponse =
  GetFalAiLatentsyncRequestsByRequestIdResponses[keyof GetFalAiLatentsyncRequestsByRequestIdResponses]

export type GetFalAiPikaV2PikadditionsRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/pika/v2/pikadditions/requests/{request_id}/status'
}

export type GetFalAiPikaV2PikadditionsRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiPikaV2PikadditionsRequestsByRequestIdStatusResponse =
  GetFalAiPikaV2PikadditionsRequestsByRequestIdStatusResponses[keyof GetFalAiPikaV2PikadditionsRequestsByRequestIdStatusResponses]

export type PutFalAiPikaV2PikadditionsRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/pika/v2/pikadditions/requests/{request_id}/cancel'
}

export type PutFalAiPikaV2PikadditionsRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiPikaV2PikadditionsRequestsByRequestIdCancelResponse =
  PutFalAiPikaV2PikadditionsRequestsByRequestIdCancelResponses[keyof PutFalAiPikaV2PikadditionsRequestsByRequestIdCancelResponses]

export type PostFalAiPikaV2PikadditionsData = {
  body: SchemaPikaV2PikadditionsInput
  path?: never
  query?: never
  url: '/fal-ai/pika/v2/pikadditions'
}

export type PostFalAiPikaV2PikadditionsResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiPikaV2PikadditionsResponse =
  PostFalAiPikaV2PikadditionsResponses[keyof PostFalAiPikaV2PikadditionsResponses]

export type GetFalAiPikaV2PikadditionsRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/pika/v2/pikadditions/requests/{request_id}'
}

export type GetFalAiPikaV2PikadditionsRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaPikaV2PikadditionsOutput
}

export type GetFalAiPikaV2PikadditionsRequestsByRequestIdResponse =
  GetFalAiPikaV2PikadditionsRequestsByRequestIdResponses[keyof GetFalAiPikaV2PikadditionsRequestsByRequestIdResponses]

export type GetFalAiLtxVideoV095MulticonditioningRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/ltx-video-v095/multiconditioning/requests/{request_id}/status'
  }

export type GetFalAiLtxVideoV095MulticonditioningRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiLtxVideoV095MulticonditioningRequestsByRequestIdStatusResponse =
  GetFalAiLtxVideoV095MulticonditioningRequestsByRequestIdStatusResponses[keyof GetFalAiLtxVideoV095MulticonditioningRequestsByRequestIdStatusResponses]

export type PutFalAiLtxVideoV095MulticonditioningRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/ltx-video-v095/multiconditioning/requests/{request_id}/cancel'
  }

export type PutFalAiLtxVideoV095MulticonditioningRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiLtxVideoV095MulticonditioningRequestsByRequestIdCancelResponse =
  PutFalAiLtxVideoV095MulticonditioningRequestsByRequestIdCancelResponses[keyof PutFalAiLtxVideoV095MulticonditioningRequestsByRequestIdCancelResponses]

export type PostFalAiLtxVideoV095MulticonditioningData = {
  body: SchemaLtxVideoV095MulticonditioningInput
  path?: never
  query?: never
  url: '/fal-ai/ltx-video-v095/multiconditioning'
}

export type PostFalAiLtxVideoV095MulticonditioningResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLtxVideoV095MulticonditioningResponse =
  PostFalAiLtxVideoV095MulticonditioningResponses[keyof PostFalAiLtxVideoV095MulticonditioningResponses]

export type GetFalAiLtxVideoV095MulticonditioningRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-video-v095/multiconditioning/requests/{request_id}'
}

export type GetFalAiLtxVideoV095MulticonditioningRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaLtxVideoV095MulticonditioningOutput
  }

export type GetFalAiLtxVideoV095MulticonditioningRequestsByRequestIdResponse =
  GetFalAiLtxVideoV095MulticonditioningRequestsByRequestIdResponses[keyof GetFalAiLtxVideoV095MulticonditioningRequestsByRequestIdResponses]

export type GetFalAiLtxVideoV095ExtendRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/ltx-video-v095/extend/requests/{request_id}/status'
}

export type GetFalAiLtxVideoV095ExtendRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiLtxVideoV095ExtendRequestsByRequestIdStatusResponse =
  GetFalAiLtxVideoV095ExtendRequestsByRequestIdStatusResponses[keyof GetFalAiLtxVideoV095ExtendRequestsByRequestIdStatusResponses]

export type PutFalAiLtxVideoV095ExtendRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-video-v095/extend/requests/{request_id}/cancel'
}

export type PutFalAiLtxVideoV095ExtendRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiLtxVideoV095ExtendRequestsByRequestIdCancelResponse =
  PutFalAiLtxVideoV095ExtendRequestsByRequestIdCancelResponses[keyof PutFalAiLtxVideoV095ExtendRequestsByRequestIdCancelResponses]

export type PostFalAiLtxVideoV095ExtendData = {
  body: SchemaLtxVideoV095ExtendInput
  path?: never
  query?: never
  url: '/fal-ai/ltx-video-v095/extend'
}

export type PostFalAiLtxVideoV095ExtendResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLtxVideoV095ExtendResponse =
  PostFalAiLtxVideoV095ExtendResponses[keyof PostFalAiLtxVideoV095ExtendResponses]

export type GetFalAiLtxVideoV095ExtendRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-video-v095/extend/requests/{request_id}'
}

export type GetFalAiLtxVideoV095ExtendRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLtxVideoV095ExtendOutput
}

export type GetFalAiLtxVideoV095ExtendRequestsByRequestIdResponse =
  GetFalAiLtxVideoV095ExtendRequestsByRequestIdResponses[keyof GetFalAiLtxVideoV095ExtendRequestsByRequestIdResponses]

export type GetFalAiTopazUpscaleVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/topaz/upscale/video/requests/{request_id}/status'
}

export type GetFalAiTopazUpscaleVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiTopazUpscaleVideoRequestsByRequestIdStatusResponse =
  GetFalAiTopazUpscaleVideoRequestsByRequestIdStatusResponses[keyof GetFalAiTopazUpscaleVideoRequestsByRequestIdStatusResponses]

export type PutFalAiTopazUpscaleVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/topaz/upscale/video/requests/{request_id}/cancel'
}

export type PutFalAiTopazUpscaleVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiTopazUpscaleVideoRequestsByRequestIdCancelResponse =
  PutFalAiTopazUpscaleVideoRequestsByRequestIdCancelResponses[keyof PutFalAiTopazUpscaleVideoRequestsByRequestIdCancelResponses]

export type PostFalAiTopazUpscaleVideoData = {
  body: SchemaTopazUpscaleVideoInput
  path?: never
  query?: never
  url: '/fal-ai/topaz/upscale/video'
}

export type PostFalAiTopazUpscaleVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiTopazUpscaleVideoResponse =
  PostFalAiTopazUpscaleVideoResponses[keyof PostFalAiTopazUpscaleVideoResponses]

export type GetFalAiTopazUpscaleVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/topaz/upscale/video/requests/{request_id}'
}

export type GetFalAiTopazUpscaleVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaTopazUpscaleVideoOutput
}

export type GetFalAiTopazUpscaleVideoRequestsByRequestIdResponse =
  GetFalAiTopazUpscaleVideoRequestsByRequestIdResponses[keyof GetFalAiTopazUpscaleVideoRequestsByRequestIdResponses]

export type GetFalAiBenV2VideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/ben/v2/video/requests/{request_id}/status'
}

export type GetFalAiBenV2VideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiBenV2VideoRequestsByRequestIdStatusResponse =
  GetFalAiBenV2VideoRequestsByRequestIdStatusResponses[keyof GetFalAiBenV2VideoRequestsByRequestIdStatusResponses]

export type PutFalAiBenV2VideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ben/v2/video/requests/{request_id}/cancel'
}

export type PutFalAiBenV2VideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiBenV2VideoRequestsByRequestIdCancelResponse =
  PutFalAiBenV2VideoRequestsByRequestIdCancelResponses[keyof PutFalAiBenV2VideoRequestsByRequestIdCancelResponses]

export type PostFalAiBenV2VideoData = {
  body: SchemaBenV2VideoInput
  path?: never
  query?: never
  url: '/fal-ai/ben/v2/video'
}

export type PostFalAiBenV2VideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiBenV2VideoResponse =
  PostFalAiBenV2VideoResponses[keyof PostFalAiBenV2VideoResponses]

export type GetFalAiBenV2VideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ben/v2/video/requests/{request_id}'
}

export type GetFalAiBenV2VideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaBenV2VideoOutput
}

export type GetFalAiBenV2VideoRequestsByRequestIdResponse =
  GetFalAiBenV2VideoRequestsByRequestIdResponses[keyof GetFalAiBenV2VideoRequestsByRequestIdResponses]

export type GetFalAiHunyuanVideoVideoToVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/hunyuan-video/video-to-video/requests/{request_id}/status'
}

export type GetFalAiHunyuanVideoVideoToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiHunyuanVideoVideoToVideoRequestsByRequestIdStatusResponse =
  GetFalAiHunyuanVideoVideoToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiHunyuanVideoVideoToVideoRequestsByRequestIdStatusResponses]

export type PutFalAiHunyuanVideoVideoToVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/hunyuan-video/video-to-video/requests/{request_id}/cancel'
}

export type PutFalAiHunyuanVideoVideoToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiHunyuanVideoVideoToVideoRequestsByRequestIdCancelResponse =
  PutFalAiHunyuanVideoVideoToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiHunyuanVideoVideoToVideoRequestsByRequestIdCancelResponses]

export type PostFalAiHunyuanVideoVideoToVideoData = {
  body: SchemaHunyuanVideoVideoToVideoInput
  path?: never
  query?: never
  url: '/fal-ai/hunyuan-video/video-to-video'
}

export type PostFalAiHunyuanVideoVideoToVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiHunyuanVideoVideoToVideoResponse =
  PostFalAiHunyuanVideoVideoToVideoResponses[keyof PostFalAiHunyuanVideoVideoToVideoResponses]

export type GetFalAiHunyuanVideoVideoToVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/hunyuan-video/video-to-video/requests/{request_id}'
}

export type GetFalAiHunyuanVideoVideoToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaHunyuanVideoVideoToVideoOutput
}

export type GetFalAiHunyuanVideoVideoToVideoRequestsByRequestIdResponse =
  GetFalAiHunyuanVideoVideoToVideoRequestsByRequestIdResponses[keyof GetFalAiHunyuanVideoVideoToVideoRequestsByRequestIdResponses]

export type GetFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/hunyuan-video-lora/video-to-video/requests/{request_id}/status'
  }

export type GetFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdStatusResponse =
  GetFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdStatusResponses]

export type PutFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/hunyuan-video-lora/video-to-video/requests/{request_id}/cancel'
  }

export type PutFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdCancelResponse =
  PutFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdCancelResponses]

export type PostFalAiHunyuanVideoLoraVideoToVideoData = {
  body: SchemaHunyuanVideoLoraVideoToVideoInput
  path?: never
  query?: never
  url: '/fal-ai/hunyuan-video-lora/video-to-video'
}

export type PostFalAiHunyuanVideoLoraVideoToVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiHunyuanVideoLoraVideoToVideoResponse =
  PostFalAiHunyuanVideoLoraVideoToVideoResponses[keyof PostFalAiHunyuanVideoLoraVideoToVideoResponses]

export type GetFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/hunyuan-video-lora/video-to-video/requests/{request_id}'
}

export type GetFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaHunyuanVideoLoraVideoToVideoOutput
}

export type GetFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdResponse =
  GetFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdResponses[keyof GetFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdResponses]

export type GetFalAiFfmpegApiComposeRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/ffmpeg-api/compose/requests/{request_id}/status'
}

export type GetFalAiFfmpegApiComposeRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiFfmpegApiComposeRequestsByRequestIdStatusResponse =
  GetFalAiFfmpegApiComposeRequestsByRequestIdStatusResponses[keyof GetFalAiFfmpegApiComposeRequestsByRequestIdStatusResponses]

export type PutFalAiFfmpegApiComposeRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ffmpeg-api/compose/requests/{request_id}/cancel'
}

export type PutFalAiFfmpegApiComposeRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiFfmpegApiComposeRequestsByRequestIdCancelResponse =
  PutFalAiFfmpegApiComposeRequestsByRequestIdCancelResponses[keyof PutFalAiFfmpegApiComposeRequestsByRequestIdCancelResponses]

export type PostFalAiFfmpegApiComposeData = {
  body: SchemaFfmpegApiComposeInput
  path?: never
  query?: never
  url: '/fal-ai/ffmpeg-api/compose'
}

export type PostFalAiFfmpegApiComposeResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiFfmpegApiComposeResponse =
  PostFalAiFfmpegApiComposeResponses[keyof PostFalAiFfmpegApiComposeResponses]

export type GetFalAiFfmpegApiComposeRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ffmpeg-api/compose/requests/{request_id}'
}

export type GetFalAiFfmpegApiComposeRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaFfmpegApiComposeOutput
}

export type GetFalAiFfmpegApiComposeRequestsByRequestIdResponse =
  GetFalAiFfmpegApiComposeRequestsByRequestIdResponses[keyof GetFalAiFfmpegApiComposeRequestsByRequestIdResponses]

export type GetFalAiSyncLipsyncRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/sync-lipsync/requests/{request_id}/status'
}

export type GetFalAiSyncLipsyncRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiSyncLipsyncRequestsByRequestIdStatusResponse =
  GetFalAiSyncLipsyncRequestsByRequestIdStatusResponses[keyof GetFalAiSyncLipsyncRequestsByRequestIdStatusResponses]

export type PutFalAiSyncLipsyncRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/sync-lipsync/requests/{request_id}/cancel'
}

export type PutFalAiSyncLipsyncRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiSyncLipsyncRequestsByRequestIdCancelResponse =
  PutFalAiSyncLipsyncRequestsByRequestIdCancelResponses[keyof PutFalAiSyncLipsyncRequestsByRequestIdCancelResponses]

export type PostFalAiSyncLipsyncData = {
  body: SchemaSyncLipsyncInput
  path?: never
  query?: never
  url: '/fal-ai/sync-lipsync'
}

export type PostFalAiSyncLipsyncResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiSyncLipsyncResponse =
  PostFalAiSyncLipsyncResponses[keyof PostFalAiSyncLipsyncResponses]

export type GetFalAiSyncLipsyncRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/sync-lipsync/requests/{request_id}'
}

export type GetFalAiSyncLipsyncRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSyncLipsyncOutput
}

export type GetFalAiSyncLipsyncRequestsByRequestIdResponse =
  GetFalAiSyncLipsyncRequestsByRequestIdResponses[keyof GetFalAiSyncLipsyncRequestsByRequestIdResponses]

export type GetFalAiAutoCaptionRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/auto-caption/requests/{request_id}/status'
}

export type GetFalAiAutoCaptionRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiAutoCaptionRequestsByRequestIdStatusResponse =
  GetFalAiAutoCaptionRequestsByRequestIdStatusResponses[keyof GetFalAiAutoCaptionRequestsByRequestIdStatusResponses]

export type PutFalAiAutoCaptionRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/auto-caption/requests/{request_id}/cancel'
}

export type PutFalAiAutoCaptionRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiAutoCaptionRequestsByRequestIdCancelResponse =
  PutFalAiAutoCaptionRequestsByRequestIdCancelResponses[keyof PutFalAiAutoCaptionRequestsByRequestIdCancelResponses]

export type PostFalAiAutoCaptionData = {
  body: SchemaAutoCaptionInput
  path?: never
  query?: never
  url: '/fal-ai/auto-caption'
}

export type PostFalAiAutoCaptionResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiAutoCaptionResponse =
  PostFalAiAutoCaptionResponses[keyof PostFalAiAutoCaptionResponses]

export type GetFalAiAutoCaptionRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/auto-caption/requests/{request_id}'
}

export type GetFalAiAutoCaptionRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaAutoCaptionOutput
}

export type GetFalAiAutoCaptionRequestsByRequestIdResponse =
  GetFalAiAutoCaptionRequestsByRequestIdResponses[keyof GetFalAiAutoCaptionRequestsByRequestIdResponses]

export type GetFalAiDubbingRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/dubbing/requests/{request_id}/status'
}

export type GetFalAiDubbingRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiDubbingRequestsByRequestIdStatusResponse =
  GetFalAiDubbingRequestsByRequestIdStatusResponses[keyof GetFalAiDubbingRequestsByRequestIdStatusResponses]

export type PutFalAiDubbingRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/dubbing/requests/{request_id}/cancel'
}

export type PutFalAiDubbingRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiDubbingRequestsByRequestIdCancelResponse =
  PutFalAiDubbingRequestsByRequestIdCancelResponses[keyof PutFalAiDubbingRequestsByRequestIdCancelResponses]

export type PostFalAiDubbingData = {
  body: SchemaDubbingInput
  path?: never
  query?: never
  url: '/fal-ai/dubbing'
}

export type PostFalAiDubbingResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiDubbingResponse =
  PostFalAiDubbingResponses[keyof PostFalAiDubbingResponses]

export type GetFalAiDubbingRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/dubbing/requests/{request_id}'
}

export type GetFalAiDubbingRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaDubbingOutput
}

export type GetFalAiDubbingRequestsByRequestIdResponse =
  GetFalAiDubbingRequestsByRequestIdResponses[keyof GetFalAiDubbingRequestsByRequestIdResponses]

export type GetFalAiVideoUpscalerRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/video-upscaler/requests/{request_id}/status'
}

export type GetFalAiVideoUpscalerRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiVideoUpscalerRequestsByRequestIdStatusResponse =
  GetFalAiVideoUpscalerRequestsByRequestIdStatusResponses[keyof GetFalAiVideoUpscalerRequestsByRequestIdStatusResponses]

export type PutFalAiVideoUpscalerRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/video-upscaler/requests/{request_id}/cancel'
}

export type PutFalAiVideoUpscalerRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiVideoUpscalerRequestsByRequestIdCancelResponse =
  PutFalAiVideoUpscalerRequestsByRequestIdCancelResponses[keyof PutFalAiVideoUpscalerRequestsByRequestIdCancelResponses]

export type PostFalAiVideoUpscalerData = {
  body: SchemaVideoUpscalerInput
  path?: never
  query?: never
  url: '/fal-ai/video-upscaler'
}

export type PostFalAiVideoUpscalerResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiVideoUpscalerResponse =
  PostFalAiVideoUpscalerResponses[keyof PostFalAiVideoUpscalerResponses]

export type GetFalAiVideoUpscalerRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/video-upscaler/requests/{request_id}'
}

export type GetFalAiVideoUpscalerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaVideoUpscalerOutput
}

export type GetFalAiVideoUpscalerRequestsByRequestIdResponse =
  GetFalAiVideoUpscalerRequestsByRequestIdResponses[keyof GetFalAiVideoUpscalerRequestsByRequestIdResponses]

export type GetFalAiCogvideox5bVideoToVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/cogvideox-5b/video-to-video/requests/{request_id}/status'
}

export type GetFalAiCogvideox5bVideoToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiCogvideox5bVideoToVideoRequestsByRequestIdStatusResponse =
  GetFalAiCogvideox5bVideoToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiCogvideox5bVideoToVideoRequestsByRequestIdStatusResponses]

export type PutFalAiCogvideox5bVideoToVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/cogvideox-5b/video-to-video/requests/{request_id}/cancel'
}

export type PutFalAiCogvideox5bVideoToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiCogvideox5bVideoToVideoRequestsByRequestIdCancelResponse =
  PutFalAiCogvideox5bVideoToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiCogvideox5bVideoToVideoRequestsByRequestIdCancelResponses]

export type PostFalAiCogvideox5bVideoToVideoData = {
  body: SchemaCogvideox5bVideoToVideoInput
  path?: never
  query?: never
  url: '/fal-ai/cogvideox-5b/video-to-video'
}

export type PostFalAiCogvideox5bVideoToVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiCogvideox5bVideoToVideoResponse =
  PostFalAiCogvideox5bVideoToVideoResponses[keyof PostFalAiCogvideox5bVideoToVideoResponses]

export type GetFalAiCogvideox5bVideoToVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/cogvideox-5b/video-to-video/requests/{request_id}'
}

export type GetFalAiCogvideox5bVideoToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaCogvideox5bVideoToVideoOutput
}

export type GetFalAiCogvideox5bVideoToVideoRequestsByRequestIdResponse =
  GetFalAiCogvideox5bVideoToVideoRequestsByRequestIdResponses[keyof GetFalAiCogvideox5bVideoToVideoRequestsByRequestIdResponses]

export type GetFalAiControlnextRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/controlnext/requests/{request_id}/status'
}

export type GetFalAiControlnextRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiControlnextRequestsByRequestIdStatusResponse =
  GetFalAiControlnextRequestsByRequestIdStatusResponses[keyof GetFalAiControlnextRequestsByRequestIdStatusResponses]

export type PutFalAiControlnextRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/controlnext/requests/{request_id}/cancel'
}

export type PutFalAiControlnextRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiControlnextRequestsByRequestIdCancelResponse =
  PutFalAiControlnextRequestsByRequestIdCancelResponses[keyof PutFalAiControlnextRequestsByRequestIdCancelResponses]

export type PostFalAiControlnextData = {
  body: SchemaControlnextInput
  path?: never
  query?: never
  url: '/fal-ai/controlnext'
}

export type PostFalAiControlnextResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiControlnextResponse =
  PostFalAiControlnextResponses[keyof PostFalAiControlnextResponses]

export type GetFalAiControlnextRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/controlnext/requests/{request_id}'
}

export type GetFalAiControlnextRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaControlnextOutput
}

export type GetFalAiControlnextRequestsByRequestIdResponse =
  GetFalAiControlnextRequestsByRequestIdResponses[keyof GetFalAiControlnextRequestsByRequestIdResponses]

export type GetFalAiSam2VideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/sam2/video/requests/{request_id}/status'
}

export type GetFalAiSam2VideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiSam2VideoRequestsByRequestIdStatusResponse =
  GetFalAiSam2VideoRequestsByRequestIdStatusResponses[keyof GetFalAiSam2VideoRequestsByRequestIdStatusResponses]

export type PutFalAiSam2VideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/sam2/video/requests/{request_id}/cancel'
}

export type PutFalAiSam2VideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiSam2VideoRequestsByRequestIdCancelResponse =
  PutFalAiSam2VideoRequestsByRequestIdCancelResponses[keyof PutFalAiSam2VideoRequestsByRequestIdCancelResponses]

export type PostFalAiSam2VideoData = {
  body: SchemaSam2VideoInput
  path?: never
  query?: never
  url: '/fal-ai/sam2/video'
}

export type PostFalAiSam2VideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiSam2VideoResponse =
  PostFalAiSam2VideoResponses[keyof PostFalAiSam2VideoResponses]

export type GetFalAiSam2VideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/sam2/video/requests/{request_id}'
}

export type GetFalAiSam2VideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSam2VideoOutput
}

export type GetFalAiSam2VideoRequestsByRequestIdResponse =
  GetFalAiSam2VideoRequestsByRequestIdResponses[keyof GetFalAiSam2VideoRequestsByRequestIdResponses]

export type GetFalAiAmtInterpolationRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/amt-interpolation/requests/{request_id}/status'
}

export type GetFalAiAmtInterpolationRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiAmtInterpolationRequestsByRequestIdStatusResponse =
  GetFalAiAmtInterpolationRequestsByRequestIdStatusResponses[keyof GetFalAiAmtInterpolationRequestsByRequestIdStatusResponses]

export type PutFalAiAmtInterpolationRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/amt-interpolation/requests/{request_id}/cancel'
}

export type PutFalAiAmtInterpolationRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiAmtInterpolationRequestsByRequestIdCancelResponse =
  PutFalAiAmtInterpolationRequestsByRequestIdCancelResponses[keyof PutFalAiAmtInterpolationRequestsByRequestIdCancelResponses]

export type PostFalAiAmtInterpolationData = {
  body: SchemaAmtInterpolationInput
  path?: never
  query?: never
  url: '/fal-ai/amt-interpolation'
}

export type PostFalAiAmtInterpolationResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiAmtInterpolationResponse =
  PostFalAiAmtInterpolationResponses[keyof PostFalAiAmtInterpolationResponses]

export type GetFalAiAmtInterpolationRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/amt-interpolation/requests/{request_id}'
}

export type GetFalAiAmtInterpolationRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaAmtInterpolationOutput
}

export type GetFalAiAmtInterpolationRequestsByRequestIdResponse =
  GetFalAiAmtInterpolationRequestsByRequestIdResponses[keyof GetFalAiAmtInterpolationRequestsByRequestIdResponses]

export type GetFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/fast-animatediff/turbo/video-to-video/requests/{request_id}/status'
  }

export type GetFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdStatusResponse =
  GetFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdStatusResponses]

export type PutFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/fast-animatediff/turbo/video-to-video/requests/{request_id}/cancel'
  }

export type PutFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdCancelResponse =
  PutFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdCancelResponses]

export type PostFalAiFastAnimatediffTurboVideoToVideoData = {
  body: SchemaFastAnimatediffTurboVideoToVideoInput
  path?: never
  query?: never
  url: '/fal-ai/fast-animatediff/turbo/video-to-video'
}

export type PostFalAiFastAnimatediffTurboVideoToVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiFastAnimatediffTurboVideoToVideoResponse =
  PostFalAiFastAnimatediffTurboVideoToVideoResponses[keyof PostFalAiFastAnimatediffTurboVideoToVideoResponses]

export type GetFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/fast-animatediff/turbo/video-to-video/requests/{request_id}'
}

export type GetFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaFastAnimatediffTurboVideoToVideoOutput
  }

export type GetFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdResponse =
  GetFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdResponses[keyof GetFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdResponses]

export type GetFalAiFastAnimatediffVideoToVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/fast-animatediff/video-to-video/requests/{request_id}/status'
}

export type GetFalAiFastAnimatediffVideoToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiFastAnimatediffVideoToVideoRequestsByRequestIdStatusResponse =
  GetFalAiFastAnimatediffVideoToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiFastAnimatediffVideoToVideoRequestsByRequestIdStatusResponses]

export type PutFalAiFastAnimatediffVideoToVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/fast-animatediff/video-to-video/requests/{request_id}/cancel'
}

export type PutFalAiFastAnimatediffVideoToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiFastAnimatediffVideoToVideoRequestsByRequestIdCancelResponse =
  PutFalAiFastAnimatediffVideoToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiFastAnimatediffVideoToVideoRequestsByRequestIdCancelResponses]

export type PostFalAiFastAnimatediffVideoToVideoData = {
  body: SchemaFastAnimatediffVideoToVideoInput
  path?: never
  query?: never
  url: '/fal-ai/fast-animatediff/video-to-video'
}

export type PostFalAiFastAnimatediffVideoToVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiFastAnimatediffVideoToVideoResponse =
  PostFalAiFastAnimatediffVideoToVideoResponses[keyof PostFalAiFastAnimatediffVideoToVideoResponses]

export type GetFalAiFastAnimatediffVideoToVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/fast-animatediff/video-to-video/requests/{request_id}'
}

export type GetFalAiFastAnimatediffVideoToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaFastAnimatediffVideoToVideoOutput
}

export type GetFalAiFastAnimatediffVideoToVideoRequestsByRequestIdResponse =
  GetFalAiFastAnimatediffVideoToVideoRequestsByRequestIdResponses[keyof GetFalAiFastAnimatediffVideoToVideoRequestsByRequestIdResponses]
