// This file is auto-generated by @hey-api/openapi-ts

import { z } from 'zod'

export const zFile = z.object({
  url: z.url(),
  content_type: z.optional(z.string()),
  file_name: z.optional(z.string()),
  file_size: z.optional(z.int()),
})

export const zQueueStatus = z.object({
  status: z.enum(['IN_PROGRESS', 'COMPLETED', 'FAILED']),
  response_url: z.optional(z.url()),
})

/**
 * InputRemoveBackgroundModel
 */
export const zVideoBackgroundRemovalInput = z.object({
  video_url: z.string().register(z.globalRegistry, {
    description:
      'Input video to remove background from. Size should be less than 14142x14142 and duration less than 30s.',
  }),
  output_container_and_codec: z.optional(
    z
      .enum([
        'mp4_h265',
        'mp4_h264',
        'webm_vp9',
        'mov_h265',
        'mov_proresks',
        'mkv_h265',
        'mkv_h264',
        'mkv_vp9',
        'gif',
      ])
      .register(z.globalRegistry, {
        description:
          'Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, mov_h265, mov_proresks, mkv_h265, mkv_h264, mkv_vp9, gif.',
      }),
  ),
  background_color: z.optional(
    z
      .enum([
        'Transparent',
        'Black',
        'White',
        'Gray',
        'Red',
        'Green',
        'Blue',
        'Yellow',
        'Cyan',
        'Magenta',
        'Orange',
      ])
      .register(z.globalRegistry, {
        description:
          'Background color. Options: Transparent, Black, White, Gray, Red, Green, Blue, Yellow, Cyan, Magenta, Orange.',
      }),
  ),
})

/**
 * Video
 */
export const zVideo = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * File
 */
export const zBriaVideoBackgroundRemovalFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * OutputRemoveBackgroundModel
 */
export const zVideoBackgroundRemovalOutput = z.object({
  video: z.union([zVideo, zBriaVideoBackgroundRemovalFile]),
})

/**
 * BaseInput
 */
export const zMmaudioV2Input = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt to generate the audio for.',
  }),
  video_url: z.string().register(z.globalRegistry, {
    description: 'The URL of the video to generate the audio for.',
  }),
  num_steps: z
    .optional(
      z.int().gte(4).lte(50).register(z.globalRegistry, {
        description: 'The number of steps to generate the audio for.',
      }),
    )
    .default(25),
  duration: z
    .optional(
      z.number().gte(1).lte(30).register(z.globalRegistry, {
        description: 'The duration of the audio to generate.',
      }),
    )
    .default(8),
  cfg_strength: z
    .optional(
      z.number().gte(0).lte(20).register(z.globalRegistry, {
        description: 'The strength of Classifier Free Guidance.',
      }),
    )
    .default(4.5),
  seed: z.optional(
    z.int().gte(0).lte(65535).register(z.globalRegistry, {
      description: 'The seed for the random number generator',
    }),
  ),
  mask_away_clip: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to mask away the clip.',
      }),
    )
    .default(false),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'The negative prompt to generate the audio for.',
      }),
    )
    .default(''),
})

/**
 * File
 */
export const zFalAiMmaudioV2File = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * Output
 */
export const zMmaudioV2Output = z.object({
  video: zFalAiMmaudioV2File,
})

/**
 * FaceSwapInputVideo
 *
 * Input schema for image ↔ video face swap
 */
export const zAiFaceSwapFaceswapvideoInput = z
  .object({
    source_face_url: z.string().register(z.globalRegistry, {
      description: 'Source face image',
    }),
    target_video_url: z.string().register(z.globalRegistry, {
      description: 'Target video URL',
    }),
  })
  .register(z.globalRegistry, {
    description: 'Input schema for image ↔ video face swap',
  })

/**
 * Video
 */
export const zHalfMoonAiAiFaceSwapFaceswapvideoVideo = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * FaceFusionVideoOutput
 *
 * FaceFusion output payload when video content is generated
 */
export const zAiFaceSwapFaceswapvideoOutput = z
  .object({
    processing_time_ms: z.optional(z.union([z.int(), z.unknown()])),
    video: zHalfMoonAiAiFaceSwapFaceswapvideoVideo,
  })
  .register(z.globalRegistry, {
    description: 'FaceFusion output payload when video content is generated',
  })

/**
 * LoRAInput
 *
 * LoRA weight configuration.
 */
export const zLoRaInput = z
  .object({
    path: z.string().register(z.globalRegistry, {
      description: 'URL, HuggingFace repo ID (owner/repo) to lora weights.',
    }),
    scale: z
      .optional(
        z.number().gte(0).lte(4).register(z.globalRegistry, {
          description: 'Scale factor for LoRA application (0.0 to 4.0).',
        }),
      )
      .default(1),
    weight_name: z.optional(z.union([z.string(), z.unknown()])),
  })
  .register(z.globalRegistry, {
    description: 'LoRA weight configuration.',
  })

/**
 * VideoFile
 */
export const zVideoFile = z.object({
  height: z.optional(z.union([z.int(), z.unknown()])),
  duration: z.optional(z.union([z.number(), z.unknown()])),
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  fps: z.optional(z.union([z.number(), z.unknown()])),
  width: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  num_frames: z.optional(z.union([z.int(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * LTX2VideoToVideoOutput
 */
export const zLtx219bDistilledVideoToVideoLoraOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for the generation.',
  }),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for the random number generator.',
  }),
  video: zVideoFile,
})

/**
 * ImageSize
 */
export const zImageSize = z.object({
  height: z
    .optional(
      z.int().lte(14142).register(z.globalRegistry, {
        description: 'The height of the generated image.',
      }),
    )
    .default(512),
  width: z
    .optional(
      z.int().lte(14142).register(z.globalRegistry, {
        description: 'The width of the generated image.',
      }),
    )
    .default(512),
})

/**
 * LTX2DistilledVideoToVideoInput
 */
export const zLtx219bDistilledVideoToVideoInput = z.object({
  use_multiscale: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.',
      }),
    )
    .default(true),
  video_url: z.string().register(z.globalRegistry, {
    description: 'The URL of the video to generate the video from.',
  }),
  acceleration: z.optional(
    z.enum(['none', 'regular', 'high', 'full']).register(z.globalRegistry, {
      description: 'The acceleration level to use.',
    }),
  ),
  ic_lora_scale: z
    .optional(
      z.number().gte(0).lte(1).register(z.globalRegistry, {
        description:
          'The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA.',
      }),
    )
    .default(1),
  generate_audio: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to generate audio for the video.',
      }),
    )
    .default(true),
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt to generate the video from.',
  }),
  fps: z
    .optional(
      z.number().gte(1).lte(60).register(z.globalRegistry, {
        description: 'The frames per second of the generated video.',
      }),
    )
    .default(25),
  camera_lora: z.optional(
    z
      .enum([
        'dolly_in',
        'dolly_out',
        'dolly_left',
        'dolly_right',
        'jib_up',
        'jib_down',
        'static',
        'none',
      ])
      .register(z.globalRegistry, {
        description:
          'The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.',
      }),
  ),
  video_size: z.optional(
    z.union([
      zImageSize,
      z.enum([
        'auto',
        'square_hd',
        'square',
        'portrait_4_3',
        'portrait_16_9',
        'landscape_4_3',
        'landscape_16_9',
      ]),
    ]),
  ),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enable the safety checker.',
      }),
    )
    .default(true),
  camera_lora_scale: z
    .optional(
      z.number().gte(0).lte(1).register(z.globalRegistry, {
        description:
          'The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.',
      }),
    )
    .default(1),
  image_strength: z
    .optional(
      z.number().gte(0).lte(1).register(z.globalRegistry, {
        description:
          'The strength of the image to use for the video generation.',
      }),
    )
    .default(1),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'The negative prompt to generate the video from.',
      }),
    )
    .default(
      'blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.',
    ),
  preprocessor: z.optional(
    z.enum(['depth', 'canny', 'pose', 'none']).register(z.globalRegistry, {
      description:
        'The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type.',
    }),
  ),
  ic_lora: z.optional(
    z
      .enum([
        'match_preprocessor',
        'canny',
        'depth',
        'pose',
        'detailer',
        'none',
      ])
      .register(z.globalRegistry, {
        description:
          'The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)',
      }),
  ),
  video_output_type: z.optional(
    z
      .enum(['X264 (.mp4)', 'VP9 (.webm)', 'PRORES4444 (.mov)', 'GIF (.gif)'])
      .register(z.globalRegistry, {
        description: 'The output type of the generated video.',
      }),
  ),
  num_frames: z
    .optional(
      z.int().gte(9).lte(481).register(z.globalRegistry, {
        description: 'The number of frames to generate.',
      }),
    )
    .default(121),
  video_write_mode: z.optional(
    z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
      description: 'The write mode of the generated video.',
    }),
  ),
  video_strength: z
    .optional(
      z.number().gte(0).lte(1).register(z.globalRegistry, {
        description:
          'Video conditioning strength. Lower values represent more freedom given to the model to change the video content.',
      }),
    )
    .default(1),
  image_url: z.optional(z.union([z.string(), z.unknown()])),
  video_quality: z.optional(
    z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
      description: 'The quality of the generated video.',
    }),
  ),
  sync_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
      }),
    )
    .default(false),
  enable_prompt_expansion: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enable prompt expansion.',
      }),
    )
    .default(false),
  match_input_fps: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "When true, match the output FPS to the input video's FPS instead of using the default target FPS.",
      }),
    )
    .default(true),
  seed: z.optional(z.union([z.int(), z.unknown()])),
  match_video_length: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames.',
      }),
    )
    .default(true),
})

/**
 * VideoFile
 */
export const zFalAiLtx219bDistilledVideoToVideoVideoFile = z.object({
  height: z.optional(z.union([z.int(), z.unknown()])),
  duration: z.optional(z.union([z.number(), z.unknown()])),
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  fps: z.optional(z.union([z.number(), z.unknown()])),
  width: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  num_frames: z.optional(z.union([z.int(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * LTX2VideoToVideoOutput
 */
export const zLtx219bDistilledVideoToVideoOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for the generation.',
  }),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for the random number generator.',
  }),
  video: zFalAiLtx219bDistilledVideoToVideoVideoFile,
})

/**
 * LoRAInput
 *
 * LoRA weight configuration.
 */
export const zFalAiLtx219bVideoToVideoLoraLoRaInput = z
  .object({
    path: z.string().register(z.globalRegistry, {
      description: 'URL, HuggingFace repo ID (owner/repo) to lora weights.',
    }),
    scale: z
      .optional(
        z.number().gte(0).lte(4).register(z.globalRegistry, {
          description: 'Scale factor for LoRA application (0.0 to 4.0).',
        }),
      )
      .default(1),
    weight_name: z.optional(z.union([z.string(), z.unknown()])),
  })
  .register(z.globalRegistry, {
    description: 'LoRA weight configuration.',
  })

/**
 * VideoFile
 */
export const zFalAiLtx219bVideoToVideoLoraVideoFile = z.object({
  height: z.optional(z.union([z.int(), z.unknown()])),
  duration: z.optional(z.union([z.number(), z.unknown()])),
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  fps: z.optional(z.union([z.number(), z.unknown()])),
  width: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  num_frames: z.optional(z.union([z.int(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * LTX2VideoToVideoOutput
 */
export const zLtx219bVideoToVideoLoraOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for the generation.',
  }),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for the random number generator.',
  }),
  video: zFalAiLtx219bVideoToVideoLoraVideoFile,
})

/**
 * ImageSize
 */
export const zFalAiLtx219bVideoToVideoImageSize = z.object({
  height: z
    .optional(
      z.int().lte(14142).register(z.globalRegistry, {
        description: 'The height of the generated image.',
      }),
    )
    .default(512),
  width: z
    .optional(
      z.int().lte(14142).register(z.globalRegistry, {
        description: 'The width of the generated image.',
      }),
    )
    .default(512),
})

/**
 * LTX2VideoToVideoInput
 */
export const zLtx219bVideoToVideoInput = z.object({
  use_multiscale: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.',
      }),
    )
    .default(true),
  video_url: z.string().register(z.globalRegistry, {
    description: 'The URL of the video to generate the video from.',
  }),
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt to generate the video from.',
  }),
  ic_lora_scale: z
    .optional(
      z.number().gte(0).lte(1).register(z.globalRegistry, {
        description:
          'The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA.',
      }),
    )
    .default(1),
  generate_audio: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to generate audio for the video.',
      }),
    )
    .default(true),
  video_size: z.optional(
    z.union([
      zFalAiLtx219bVideoToVideoImageSize,
      z.enum([
        'auto',
        'square_hd',
        'square',
        'portrait_4_3',
        'portrait_16_9',
        'landscape_4_3',
        'landscape_16_9',
      ]),
    ]),
  ),
  guidance_scale: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description: 'The guidance scale to use.',
      }),
    )
    .default(3),
  camera_lora_scale: z
    .optional(
      z.number().gte(0).lte(1).register(z.globalRegistry, {
        description:
          'The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.',
      }),
    )
    .default(1),
  num_frames: z
    .optional(
      z.int().gte(9).lte(481).register(z.globalRegistry, {
        description: 'The number of frames to generate.',
      }),
    )
    .default(121),
  video_strength: z
    .optional(
      z.number().gte(0).lte(1).register(z.globalRegistry, {
        description:
          'Video conditioning strength. Lower values represent more freedom given to the model to change the video content.',
      }),
    )
    .default(1),
  video_output_type: z.optional(
    z
      .enum(['X264 (.mp4)', 'VP9 (.webm)', 'PRORES4444 (.mov)', 'GIF (.gif)'])
      .register(z.globalRegistry, {
        description: 'The output type of the generated video.',
      }),
  ),
  image_url: z.optional(z.union([z.string(), z.unknown()])),
  video_quality: z.optional(
    z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
      description: 'The quality of the generated video.',
    }),
  ),
  sync_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
      }),
    )
    .default(false),
  enable_prompt_expansion: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enable prompt expansion.',
      }),
    )
    .default(false),
  seed: z.optional(z.union([z.int(), z.unknown()])),
  match_video_length: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames.',
      }),
    )
    .default(true),
  acceleration: z.optional(
    z.enum(['none', 'regular', 'high', 'full']).register(z.globalRegistry, {
      description: 'The acceleration level to use.',
    }),
  ),
  fps: z
    .optional(
      z.number().gte(1).lte(60).register(z.globalRegistry, {
        description: 'The frames per second of the generated video.',
      }),
    )
    .default(25),
  camera_lora: z.optional(
    z
      .enum([
        'dolly_in',
        'dolly_out',
        'dolly_left',
        'dolly_right',
        'jib_up',
        'jib_down',
        'static',
        'none',
      ])
      .register(z.globalRegistry, {
        description:
          'The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.',
      }),
  ),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enable the safety checker.',
      }),
    )
    .default(true),
  image_strength: z
    .optional(
      z.number().gte(0).lte(1).register(z.globalRegistry, {
        description:
          'The strength of the image to use for the video generation.',
      }),
    )
    .default(1),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'The negative prompt to generate the video from.',
      }),
    )
    .default(
      'blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.',
    ),
  preprocessor: z.optional(
    z.enum(['depth', 'canny', 'pose', 'none']).register(z.globalRegistry, {
      description:
        'The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type.',
    }),
  ),
  ic_lora: z.optional(
    z
      .enum([
        'match_preprocessor',
        'canny',
        'depth',
        'pose',
        'detailer',
        'none',
      ])
      .register(z.globalRegistry, {
        description:
          'The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)',
      }),
  ),
  video_write_mode: z.optional(
    z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
      description: 'The write mode of the generated video.',
    }),
  ),
  num_inference_steps: z
    .optional(
      z.int().gte(8).lte(50).register(z.globalRegistry, {
        description: 'The number of inference steps to use.',
      }),
    )
    .default(40),
  match_input_fps: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "When true, match the output FPS to the input video's FPS instead of using the default target FPS.",
      }),
    )
    .default(true),
})

/**
 * VideoFile
 */
export const zFalAiLtx219bVideoToVideoVideoFile = z.object({
  height: z.optional(z.union([z.int(), z.unknown()])),
  duration: z.optional(z.union([z.number(), z.unknown()])),
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  fps: z.optional(z.union([z.number(), z.unknown()])),
  width: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  num_frames: z.optional(z.union([z.int(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * LTX2VideoToVideoOutput
 */
export const zLtx219bVideoToVideoOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for the generation.',
  }),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for the random number generator.',
  }),
  video: zFalAiLtx219bVideoToVideoVideoFile,
})

/**
 * LoRAInput
 *
 * LoRA weight configuration.
 */
export const zFalAiLtx219bDistilledExtendVideoLoraLoRaInput = z
  .object({
    path: z.string().register(z.globalRegistry, {
      description: 'URL, HuggingFace repo ID (owner/repo) to lora weights.',
    }),
    scale: z
      .optional(
        z.number().gte(0).lte(4).register(z.globalRegistry, {
          description: 'Scale factor for LoRA application (0.0 to 4.0).',
        }),
      )
      .default(1),
    weight_name: z.optional(z.union([z.string(), z.unknown()])),
  })
  .register(z.globalRegistry, {
    description: 'LoRA weight configuration.',
  })

/**
 * VideoFile
 */
export const zFalAiLtx219bDistilledExtendVideoLoraVideoFile = z.object({
  height: z.optional(z.union([z.int(), z.unknown()])),
  duration: z.optional(z.union([z.number(), z.unknown()])),
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  fps: z.optional(z.union([z.number(), z.unknown()])),
  width: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  num_frames: z.optional(z.union([z.int(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * LTX2ExtendVideoOutput
 */
export const zLtx219bDistilledExtendVideoLoraOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for the generation.',
  }),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for the random number generator.',
  }),
  video: zFalAiLtx219bDistilledExtendVideoLoraVideoFile,
})

/**
 * ImageSize
 */
export const zFalAiLtx219bDistilledExtendVideoImageSize = z.object({
  height: z
    .optional(
      z.int().lte(14142).register(z.globalRegistry, {
        description: 'The height of the generated image.',
      }),
    )
    .default(512),
  width: z
    .optional(
      z.int().lte(14142).register(z.globalRegistry, {
        description: 'The width of the generated image.',
      }),
    )
    .default(512),
})

/**
 * LTX2DistilledExtendVideoInput
 */
export const zLtx219bDistilledExtendVideoInput = z.object({
  use_multiscale: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.',
      }),
    )
    .default(true),
  video_url: z.string().register(z.globalRegistry, {
    description: 'The URL of the video to extend.',
  }),
  acceleration: z.optional(
    z.enum(['none', 'regular', 'high', 'full']).register(z.globalRegistry, {
      description: 'The acceleration level to use.',
    }),
  ),
  generate_audio: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to generate audio for the video.',
      }),
    )
    .default(true),
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt to generate the video from.',
  }),
  fps: z
    .optional(
      z.number().gte(1).lte(60).register(z.globalRegistry, {
        description: 'The frames per second of the generated video.',
      }),
    )
    .default(25),
  camera_lora: z.optional(
    z
      .enum([
        'dolly_in',
        'dolly_out',
        'dolly_left',
        'dolly_right',
        'jib_up',
        'jib_down',
        'static',
        'none',
      ])
      .register(z.globalRegistry, {
        description:
          'The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.',
      }),
  ),
  video_size: z.optional(
    z.union([
      zFalAiLtx219bDistilledExtendVideoImageSize,
      z.enum([
        'auto',
        'square_hd',
        'square',
        'portrait_4_3',
        'portrait_16_9',
        'landscape_4_3',
        'landscape_16_9',
      ]),
    ]),
  ),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enable the safety checker.',
      }),
    )
    .default(true),
  num_frames: z
    .optional(
      z.int().gte(9).lte(481).register(z.globalRegistry, {
        description: 'The number of frames to generate.',
      }),
    )
    .default(121),
  camera_lora_scale: z
    .optional(
      z.number().gte(0).lte(1).register(z.globalRegistry, {
        description:
          'The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.',
      }),
    )
    .default(1),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'The negative prompt to generate the video from.',
      }),
    )
    .default(
      'blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.',
    ),
  video_strength: z
    .optional(
      z.number().gte(0).lte(1).register(z.globalRegistry, {
        description:
          'Video conditioning strength. Lower values represent more freedom given to the model to change the video content.',
      }),
    )
    .default(1),
  video_output_type: z.optional(
    z
      .enum(['X264 (.mp4)', 'VP9 (.webm)', 'PRORES4444 (.mov)', 'GIF (.gif)'])
      .register(z.globalRegistry, {
        description: 'The output type of the generated video.',
      }),
  ),
  video_write_mode: z.optional(
    z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
      description: 'The write mode of the generated video.',
    }),
  ),
  num_context_frames: z
    .optional(
      z.int().gte(0).lte(121).register(z.globalRegistry, {
        description:
          'The number of frames to use as context for the extension.',
      }),
    )
    .default(25),
  video_quality: z.optional(
    z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
      description: 'The quality of the generated video.',
    }),
  ),
  sync_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
      }),
    )
    .default(false),
  enable_prompt_expansion: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enable prompt expansion.',
      }),
    )
    .default(false),
  match_input_fps: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "When true, match the output FPS to the input video's FPS instead of using the default target FPS.",
      }),
    )
    .default(true),
  audio_strength: z
    .optional(
      z.number().gte(0).lte(1).register(z.globalRegistry, {
        description:
          'Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.',
      }),
    )
    .default(1),
  seed: z.optional(z.union([z.int(), z.unknown()])),
})

/**
 * VideoFile
 */
export const zFalAiLtx219bDistilledExtendVideoVideoFile = z.object({
  height: z.optional(z.union([z.int(), z.unknown()])),
  duration: z.optional(z.union([z.number(), z.unknown()])),
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  fps: z.optional(z.union([z.number(), z.unknown()])),
  width: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  num_frames: z.optional(z.union([z.int(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * LTX2ExtendVideoOutput
 */
export const zLtx219bDistilledExtendVideoOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for the generation.',
  }),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for the random number generator.',
  }),
  video: zFalAiLtx219bDistilledExtendVideoVideoFile,
})

/**
 * LoRAInput
 *
 * LoRA weight configuration.
 */
export const zFalAiLtx219bExtendVideoLoraLoRaInput = z
  .object({
    path: z.string().register(z.globalRegistry, {
      description: 'URL, HuggingFace repo ID (owner/repo) to lora weights.',
    }),
    scale: z
      .optional(
        z.number().gte(0).lte(4).register(z.globalRegistry, {
          description: 'Scale factor for LoRA application (0.0 to 4.0).',
        }),
      )
      .default(1),
    weight_name: z.optional(z.union([z.string(), z.unknown()])),
  })
  .register(z.globalRegistry, {
    description: 'LoRA weight configuration.',
  })

/**
 * VideoFile
 */
export const zFalAiLtx219bExtendVideoLoraVideoFile = z.object({
  height: z.optional(z.union([z.int(), z.unknown()])),
  duration: z.optional(z.union([z.number(), z.unknown()])),
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  fps: z.optional(z.union([z.number(), z.unknown()])),
  width: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  num_frames: z.optional(z.union([z.int(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * LTX2ExtendVideoOutput
 */
export const zLtx219bExtendVideoLoraOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for the generation.',
  }),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for the random number generator.',
  }),
  video: zFalAiLtx219bExtendVideoLoraVideoFile,
})

/**
 * ImageSize
 */
export const zFalAiLtx219bExtendVideoImageSize = z.object({
  height: z
    .optional(
      z.int().lte(14142).register(z.globalRegistry, {
        description: 'The height of the generated image.',
      }),
    )
    .default(512),
  width: z
    .optional(
      z.int().lte(14142).register(z.globalRegistry, {
        description: 'The width of the generated image.',
      }),
    )
    .default(512),
})

/**
 * LTX2ExtendVideoInput
 *
 * extend_direction: ExtendDirection = Field(
 * description="Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning.",
 * default="forward",
 * ui={"important": True},
 * title="Extend Direction",
 * )
 */
export const zLtx219bExtendVideoInput = z
  .object({
    use_multiscale: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.',
        }),
      )
      .default(true),
    video_url: z.string().register(z.globalRegistry, {
      description: 'The URL of the video to extend.',
    }),
    acceleration: z.optional(
      z.enum(['none', 'regular', 'high', 'full']).register(z.globalRegistry, {
        description: 'The acceleration level to use.',
      }),
    ),
    generate_audio: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description: 'Whether to generate audio for the video.',
        }),
      )
      .default(true),
    prompt: z.string().register(z.globalRegistry, {
      description: 'The prompt to generate the video from.',
    }),
    fps: z
      .optional(
        z.number().gte(1).lte(60).register(z.globalRegistry, {
          description: 'The frames per second of the generated video.',
        }),
      )
      .default(25),
    camera_lora: z.optional(
      z
        .enum([
          'dolly_in',
          'dolly_out',
          'dolly_left',
          'dolly_right',
          'jib_up',
          'jib_down',
          'static',
          'none',
        ])
        .register(z.globalRegistry, {
          description:
            'The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.',
        }),
    ),
    video_size: z.optional(
      z.union([
        zFalAiLtx219bExtendVideoImageSize,
        z.enum([
          'auto',
          'square_hd',
          'square',
          'portrait_4_3',
          'portrait_16_9',
          'landscape_4_3',
          'landscape_16_9',
        ]),
      ]),
    ),
    enable_safety_checker: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description: 'Whether to enable the safety checker.',
        }),
      )
      .default(true),
    num_frames: z
      .optional(
        z.int().gte(9).lte(481).register(z.globalRegistry, {
          description: 'The number of frames to generate.',
        }),
      )
      .default(121),
    camera_lora_scale: z
      .optional(
        z.number().gte(0).lte(1).register(z.globalRegistry, {
          description:
            'The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.',
        }),
      )
      .default(1),
    negative_prompt: z
      .optional(
        z.string().register(z.globalRegistry, {
          description: 'The negative prompt to generate the video from.',
        }),
      )
      .default(
        'blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.',
      ),
    guidance_scale: z
      .optional(
        z.number().gte(1).lte(10).register(z.globalRegistry, {
          description: 'The guidance scale to use.',
        }),
      )
      .default(3),
    video_strength: z
      .optional(
        z.number().gte(0).lte(1).register(z.globalRegistry, {
          description:
            'Video conditioning strength. Lower values represent more freedom given to the model to change the video content.',
        }),
      )
      .default(1),
    video_output_type: z.optional(
      z
        .enum(['X264 (.mp4)', 'VP9 (.webm)', 'PRORES4444 (.mov)', 'GIF (.gif)'])
        .register(z.globalRegistry, {
          description: 'The output type of the generated video.',
        }),
    ),
    video_write_mode: z.optional(
      z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
        description: 'The write mode of the generated video.',
      }),
    ),
    num_context_frames: z
      .optional(
        z.int().gte(0).lte(121).register(z.globalRegistry, {
          description:
            'The number of frames to use as context for the extension.',
        }),
      )
      .default(25),
    video_quality: z.optional(
      z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
        description: 'The quality of the generated video.',
      }),
    ),
    sync_mode: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            "If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
        }),
      )
      .default(false),
    enable_prompt_expansion: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description: 'Whether to enable prompt expansion.',
        }),
      )
      .default(false),
    audio_strength: z
      .optional(
        z.number().gte(0).lte(1).register(z.globalRegistry, {
          description:
            'Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.',
        }),
      )
      .default(1),
    num_inference_steps: z
      .optional(
        z.int().gte(8).lte(50).register(z.globalRegistry, {
          description: 'The number of inference steps to use.',
        }),
      )
      .default(40),
    match_input_fps: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            "When true, match the output FPS to the input video's FPS instead of using the default target FPS.",
        }),
      )
      .default(true),
    seed: z.optional(z.union([z.int(), z.unknown()])),
  })
  .register(z.globalRegistry, {
    description:
      'extend_direction: ExtendDirection = Field(\n    description="Direction to extend the video. \'forward\' extends from the end of the video, \'backward\' extends from the beginning.",\n    default="forward",\n    ui={"important": True},\n    title="Extend Direction",\n)',
  })

/**
 * VideoFile
 */
export const zFalAiLtx219bExtendVideoVideoFile = z.object({
  height: z.optional(z.union([z.int(), z.unknown()])),
  duration: z.optional(z.union([z.number(), z.unknown()])),
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  fps: z.optional(z.union([z.number(), z.unknown()])),
  width: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  num_frames: z.optional(z.union([z.int(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * LTX2ExtendVideoOutput
 */
export const zLtx219bExtendVideoOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for the generation.',
  }),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for the random number generator.',
  }),
  video: zFalAiLtx219bExtendVideoVideoFile,
})

/**
 * EraseByKeyPointsInputModel
 */
export const zVideoEraseKeypointsInput = z.object({
  preserve_audio: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If true, audio will be preserved in the output video.',
      }),
    )
    .default(true),
  video_url: z.string().register(z.globalRegistry, {
    description:
      'Input video to erase object from. duration must be less than 5s.',
  }),
  output_container_and_codec: z.optional(
    z
      .enum([
        'mp4_h265',
        'mp4_h264',
        'webm_vp9',
        'gif',
        'mov_h264',
        'mov_h265',
        'mov_proresks',
        'mkv_h264',
        'mkv_h265',
        'mkv_vp9',
        'mkv_mpeg4',
      ])
      .register(z.globalRegistry, {
        description:
          'Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.',
      }),
  ),
  keypoints: z.array(z.string()).register(z.globalRegistry, {
    description:
      "Input keypoints [x,y] to erase or keep from the video. Format like so: {'x':100, 'y':100, 'type':'positive/negative'}",
  }),
  auto_trim: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'auto trim the video, to working duration ( 5s )',
      }),
    )
    .default(true),
})

/**
 * Video
 */
export const zBriaVideoEraseKeypointsVideo = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * File
 */
export const zBriaVideoEraseKeypointsFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * VideoOutput
 */
export const zVideoEraseKeypointsOutput = z.object({
  video: z.union([zBriaVideoEraseKeypointsVideo, zBriaVideoEraseKeypointsFile]),
})

/**
 * EraseByPromptInputModel
 */
export const zVideoErasePromptInput = z.object({
  preserve_audio: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If true, audio will be preserved in the output video.',
      }),
    )
    .default(true),
  video_url: z.string().register(z.globalRegistry, {
    description:
      'Input video to erase object from. duration must be less than 5s.',
  }),
  prompt: z.string().register(z.globalRegistry, {
    description: 'Input prompt to detect object to erase',
  }),
  output_container_and_codec: z.optional(
    z
      .enum([
        'mp4_h265',
        'mp4_h264',
        'webm_vp9',
        'gif',
        'mov_h264',
        'mov_h265',
        'mov_proresks',
        'mkv_h264',
        'mkv_h265',
        'mkv_vp9',
        'mkv_mpeg4',
      ])
      .register(z.globalRegistry, {
        description:
          'Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.',
      }),
  ),
  auto_trim: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'auto trim the video, to working duration ( 5s )',
      }),
    )
    .default(true),
})

/**
 * Video
 */
export const zBriaVideoErasePromptVideo = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * File
 */
export const zBriaVideoErasePromptFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * VideoOutput
 */
export const zVideoErasePromptOutput = z.object({
  video: z.union([zBriaVideoErasePromptVideo, zBriaVideoErasePromptFile]),
})

/**
 * EraseInputModel
 */
export const zVideoEraseMaskInput = z.object({
  preserve_audio: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If true, audio will be preserved in the output video.',
      }),
    )
    .default(true),
  video_url: z.string().register(z.globalRegistry, {
    description:
      'Input video to erase object from. duration must be less than 5s.',
  }),
  output_container_and_codec: z.optional(
    z
      .enum([
        'mp4_h265',
        'mp4_h264',
        'webm_vp9',
        'gif',
        'mov_h264',
        'mov_h265',
        'mov_proresks',
        'mkv_h264',
        'mkv_h265',
        'mkv_vp9',
        'mkv_mpeg4',
      ])
      .register(z.globalRegistry, {
        description:
          'Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.',
      }),
  ),
  mask_video_url: z.string().register(z.globalRegistry, {
    description:
      'Input video to mask erase object from. duration must be less than 5s.',
  }),
  auto_trim: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'auto trim the video, to working duration ( 5s )',
      }),
    )
    .default(true),
})

/**
 * Video
 */
export const zBriaVideoEraseMaskVideo = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * File
 */
export const zBriaVideoEraseMaskFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * VideoOutput
 */
export const zVideoEraseMaskOutput = z.object({
  video: z.union([zBriaVideoEraseMaskVideo, zBriaVideoEraseMaskFile]),
})

/**
 * RelightParameters
 *
 * Relighting parameters for video relighting operations.
 *
 * Used with relight_condition_type 'ic' (intrinsic conditioning).
 */
export const zRelightParameters = z
  .object({
    relight_prompt: z.string().register(z.globalRegistry, {
      description: 'Text prompt describing the desired lighting condition.',
    }),
    bg_source: z.optional(
      z.enum(['Left', 'Right', 'Top', 'Bottom']).register(z.globalRegistry, {
        description: 'Direction of the light source (used for IC-light).',
      }),
    ),
    use_sky_mask: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description: 'Whether to use sky masking for outdoor scenes.',
        }),
      )
      .default(false),
    cfg: z
      .optional(
        z.number().gte(1).lte(10).register(z.globalRegistry, {
          description: 'Classifier-free guidance scale for relighting.',
        }),
      )
      .default(2),
  })
  .register(z.globalRegistry, {
    description:
      "Relighting parameters for video relighting operations.\n\nUsed with relight_condition_type 'ic' (intrinsic conditioning).",
  })

/**
 * LightXRelightRequest
 *
 * Relighting-only request (minimal schema).
 */
export const zLightxRelightInput = z
  .object({
    prompt: z.optional(
      z.string().register(z.globalRegistry, {
        description:
          'Optional text prompt. If omitted, Light-X will auto-caption the video.',
      }),
    ),
    video_url: z.string().register(z.globalRegistry, {
      description: 'URL of the input video.',
    }),
    relight_parameters: z.optional(zRelightParameters),
    ref_id: z
      .optional(
        z.int().gte(0).register(z.globalRegistry, {
          description:
            'Frame index to use as referencen to relight the video with reference.',
        }),
      )
      .default(0),
    relit_cond_img_url: z.optional(
      z.string().register(z.globalRegistry, {
        description:
          "URL of conditioning image. Required for relight_condition_type='ref'/'hdr'. Also required for relight_condition_type='bg' (background image).",
      }),
    ),
    relit_cond_type: z.optional(
      z.enum(['ic', 'ref', 'hdr', 'bg']).register(z.globalRegistry, {
        description: 'Relight condition type.',
      }),
    ),
    seed: z.optional(
      z.int().register(z.globalRegistry, {
        description:
          'Random seed for reproducibility. If None, a random seed is chosen.',
      }),
    ),
  })
  .register(z.globalRegistry, {
    description: 'Relighting-only request (minimal schema).',
  })

/**
 * File
 */
export const zFalAiLightxRelightFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * LightXOutput
 */
export const zLightxRelightOutput = z.object({
  viz_video: z.optional(zFalAiLightxRelightFile),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  input_video: z.optional(zFalAiLightxRelightFile),
  video: zFalAiLightxRelightFile,
})

/**
 * TrajectoryParameters
 *
 * Camera trajectory parameters for re-camera operations.
 *
 * Each list represents interpolation values across frames:
 * - theta: Horizontal rotation angles (degrees)
 * - phi: Vertical rotation angles (degrees)
 * - radius: Camera distance scaling factors
 */
export const zTrajectoryParameters = z
  .object({
    theta: z.array(z.number()).register(z.globalRegistry, {
      description: 'Horizontal rotation angles (degrees) for each keyframe.',
    }),
    radius: z.array(z.number()).register(z.globalRegistry, {
      description: 'Camera distance scaling factors for each keyframe.',
    }),
    phi: z.array(z.number()).register(z.globalRegistry, {
      description: 'Vertical rotation angles (degrees) for each keyframe.',
    }),
  })
  .register(z.globalRegistry, {
    description:
      'Camera trajectory parameters for re-camera operations.\n\nEach list represents interpolation values across frames:\n- theta: Horizontal rotation angles (degrees)\n- phi: Vertical rotation angles (degrees)\n- radius: Camera distance scaling factors',
  })

/**
 * LightXRecameraRequest
 *
 * Re-camera-only request (minimal schema).
 */
export const zLightxRecameraInput = z
  .object({
    prompt: z.optional(
      z.string().register(z.globalRegistry, {
        description:
          'Optional text prompt. If omitted, Light-X will auto-caption the video.',
      }),
    ),
    trajectory: z.optional(zTrajectoryParameters),
    video_url: z.string().register(z.globalRegistry, {
      description: 'URL of the input video.',
    }),
    camera: z.optional(
      z.enum(['traj', 'target']).register(z.globalRegistry, {
        description: 'Camera control mode.',
      }),
    ),
    target_pose: z.optional(
      z.array(z.number()).register(z.globalRegistry, {
        description:
          "Target camera pose [theta, phi, radius, x, y] (required when camera='target').",
      }),
    ),
    mode: z.optional(
      z
        .enum(['gradual', 'bullet', 'direct', 'dolly-zoom'])
        .register(z.globalRegistry, {
          description: 'Camera motion mode.',
        }),
    ),
    seed: z.optional(
      z.int().register(z.globalRegistry, {
        description:
          'Random seed for reproducibility. If None, a random seed is chosen.',
      }),
    ),
  })
  .register(z.globalRegistry, {
    description: 'Re-camera-only request (minimal schema).',
  })

/**
 * File
 */
export const zFalAiLightxRecameraFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * LightXOutput
 */
export const zLightxRecameraOutput = z.object({
  viz_video: z.optional(zFalAiLightxRecameraFile),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  input_video: z.optional(zFalAiLightxRecameraFile),
  video: zFalAiLightxRecameraFile,
})

/**
 * MotionControlRequest
 *
 * Request model for motion control video generation.
 */
export const zKlingVideoV26StandardMotionControlInput = z
  .object({
    prompt: z.optional(z.string().max(2500)),
    video_url: z.string().register(z.globalRegistry, {
      description:
        "Reference video URL. The character actions in the generated video will be consistent with this reference video. Should contain a realistic style character with entire body or upper body visible, including head, without obstruction. Duration limit depends on character_orientation: 10s max for 'image', 30s max for 'video'.",
    }),
    character_orientation: z
      .enum(['image', 'video'])
      .register(z.globalRegistry, {
        description:
          "Controls whether the output character's orientation matches the reference image or video. 'video': orientation matches reference video - better for complex motions (max 30s). 'image': orientation matches reference image - better for following camera movements (max 10s).",
      }),
    keep_original_sound: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'Whether to keep the original sound from the reference video.',
        }),
      )
      .default(true),
    image_url: z.string().register(z.globalRegistry, {
      description:
        'Reference image URL. The characters, backgrounds, and other elements in the generated video are based on this reference image. Characters should have clear body proportions, avoid occlusion, and occupy more than 5% of the image area.',
    }),
  })
  .register(z.globalRegistry, {
    description: 'Request model for motion control video generation.',
  })

/**
 * File
 */
export const zFalAiKlingVideoV26StandardMotionControlFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * MotionControlOutput
 *
 * Output model for motion control video generation.
 */
export const zKlingVideoV26StandardMotionControlOutput = z
  .object({
    video: zFalAiKlingVideoV26StandardMotionControlFile,
  })
  .register(z.globalRegistry, {
    description: 'Output model for motion control video generation.',
  })

/**
 * MotionControlRequest
 *
 * Request model for motion control video generation.
 */
export const zKlingVideoV26ProMotionControlInput = z
  .object({
    prompt: z.optional(z.string().max(2500)),
    video_url: z.string().register(z.globalRegistry, {
      description:
        "Reference video URL. The character actions in the generated video will be consistent with this reference video. Should contain a realistic style character with entire body or upper body visible, including head, without obstruction. Duration limit depends on character_orientation: 10s max for 'image', 30s max for 'video'.",
    }),
    character_orientation: z
      .enum(['image', 'video'])
      .register(z.globalRegistry, {
        description:
          "Controls whether the output character's orientation matches the reference image or video. 'video': orientation matches reference video - better for complex motions (max 30s). 'image': orientation matches reference image - better for following camera movements (max 10s).",
      }),
    keep_original_sound: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'Whether to keep the original sound from the reference video.',
        }),
      )
      .default(true),
    image_url: z.string().register(z.globalRegistry, {
      description:
        'Reference image URL. The characters, backgrounds, and other elements in the generated video are based on this reference image. Characters should have clear body proportions, avoid occlusion, and occupy more than 5% of the image area.',
    }),
  })
  .register(z.globalRegistry, {
    description: 'Request model for motion control video generation.',
  })

/**
 * File
 */
export const zFalAiKlingVideoV26ProMotionControlFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * MotionControlOutput
 *
 * Output model for motion control video generation.
 */
export const zKlingVideoV26ProMotionControlOutput = z
  .object({
    video: zFalAiKlingVideoV26ProMotionControlFile,
  })
  .register(z.globalRegistry, {
    description: 'Output model for motion control video generation.',
  })

/**
 * LucyRestyleInput
 */
export const zLucyRestyleInput = z.object({
  sync_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          '\n            If set to true, the function will wait for the video to be generated\n            and uploaded before returning the response. This will increase the\n            latency of the function but it allows you to get the video directly\n            in the response without going through the CDN.\n        ',
      }),
    )
    .default(false),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the video to edit',
  }),
  resolution: z.optional(
    z.enum(['720p']).register(z.globalRegistry, {
      description: 'Resolution of the generated video',
    }),
  ),
  prompt: z.string().max(1500).register(z.globalRegistry, {
    description: 'Text description of the desired video content',
  }),
  seed: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'Seed for video generation',
    }),
  ),
  enhance_prompt: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enhance the prompt for better results.',
      }),
    )
    .default(true),
})

/**
 * File
 */
export const zDecartLucyRestyleFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * LucyRestyleOutput
 */
export const zLucyRestyleOutput = z.object({
  video: zDecartLucyRestyleFile,
})

/**
 * ScailRequest
 */
export const zScailInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt to guide video generation.',
  }),
  video_url: z.string().register(z.globalRegistry, {
    description:
      'The URL of the video to use as a reference for the video generation.',
  }),
  resolution: z.optional(
    z.enum(['512p']).register(z.globalRegistry, {
      description:
        'Output resolution. Outputs 896x512 (landscape) or 512x896 (portrait) based on the input image aspect ratio.',
    }),
  ),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(30).register(z.globalRegistry, {
        description:
          'The number of inference steps to use for the video generation.',
      }),
    )
    .default(28),
  multi_character: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Enable multi-character mode. Use when driving video has multiple people.',
      }),
    )
    .default(false),
  image_url: z.string().register(z.globalRegistry, {
    description:
      'The URL of the image to use as a reference for the video generation.',
  }),
})

/**
 * File
 */
export const zFalAiScailFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * ScailResponse
 */
export const zScailOutput = z.object({
  video: zFalAiScailFile,
})

/**
 * CrystalVideoUpscaleInput
 */
export const zCrystalVideoUpscalerInput = z.object({
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL to the input video.',
  }),
  scale_factor: z
    .optional(
      z.number().gte(1).lte(200).register(z.globalRegistry, {
        description:
          'Scale factor. The scale factor must be chosen such that the upscaled video does not exceed 5K resolution.',
      }),
    )
    .default(2),
})

/**
 * VideoFile
 */
export const zClarityaiCrystalVideoUpscalerVideoFile = z.object({
  height: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The height of the video',
    }),
  ),
  duration: z.optional(
    z.number().register(z.globalRegistry, {
      description: 'The duration of the video',
    }),
  ),
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  fps: z.optional(
    z.number().register(z.globalRegistry, {
      description: 'The FPS of the video',
    }),
  ),
  width: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The width of the video',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  num_frames: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The number of frames in the video',
    }),
  ),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * CrystalVideoUpscaleOutput
 */
export const zCrystalVideoUpscalerOutput = z.object({
  video: zClarityaiCrystalVideoUpscalerVideoFile,
})

/**
 * ReferenceToVideoInput
 *
 * Input for Wan 2.6 reference-to-video generation (R2V)
 */
export const zV26ReferenceToVideoInput = z
  .object({
    prompt: z.string().min(1).register(z.globalRegistry, {
      description:
        "Use @Video1, @Video2, @Video3 to reference subjects from your videos. Works for people, animals, or objects. For multi-shot prompts: '[0-3s] Shot 1. [3-6s] Shot 2.' Max 800 characters.",
    }),
    resolution: z.optional(
      z.enum(['720p', '1080p']).register(z.globalRegistry, {
        description:
          'Video resolution tier. R2V only supports 720p and 1080p (no 480p).',
      }),
    ),
    video_urls: z.array(z.string()).register(z.globalRegistry, {
      description:
        "Reference videos for subject consistency (1-3 videos). Videos' FPS must be at least 16 FPS.Reference in prompt as @Video1, @Video2, @Video3. Works for people, animals, or objects.",
    }),
    aspect_ratio: z.optional(
      z.enum(['16:9', '9:16', '1:1', '4:3', '3:4']).register(z.globalRegistry, {
        description: 'The aspect ratio of the generated video.',
      }),
    ),
    duration: z.optional(
      z.enum(['5', '10']).register(z.globalRegistry, {
        description:
          'Duration of the generated video in seconds. R2V supports only 5 or 10 seconds (no 15s).',
      }),
    ),
    enable_prompt_expansion: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description: 'Whether to enable prompt rewriting using LLM.',
        }),
      )
      .default(true),
    seed: z.optional(
      z.int().register(z.globalRegistry, {
        description:
          'Random seed for reproducibility. If None, a random seed is chosen.',
      }),
    ),
    multi_shots: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'When true (default), enables intelligent multi-shot segmentation for coherent narrative videos with multiple shots. When false, generates single continuous shot. Only active when enable_prompt_expansion is True.',
        }),
      )
      .default(true),
    negative_prompt: z
      .optional(
        z.string().register(z.globalRegistry, {
          description:
            'Negative prompt to describe content to avoid. Max 500 characters.',
        }),
      )
      .default(''),
    enable_safety_checker: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description: 'If set to true, the safety checker will be enabled.',
        }),
      )
      .default(true),
  })
  .register(z.globalRegistry, {
    description: 'Input for Wan 2.6 reference-to-video generation (R2V)',
  })

/**
 * VideoFile
 */
export const zWanV26ReferenceToVideoVideoFile = z.object({
  height: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The height of the video',
    }),
  ),
  duration: z.optional(
    z.number().register(z.globalRegistry, {
      description: 'The duration of the video',
    }),
  ),
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  fps: z.optional(
    z.number().register(z.globalRegistry, {
      description: 'The FPS of the video',
    }),
  ),
  width: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The width of the video',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  num_frames: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The number of frames in the video',
    }),
  ),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * ReferenceToVideoOutput
 *
 * Output for reference-to-video generation
 */
export const zV26ReferenceToVideoOutput = z
  .object({
    actual_prompt: z.optional(
      z.string().register(z.globalRegistry, {
        description: 'The actual prompt used if prompt rewriting was enabled',
      }),
    ),
    seed: z.int().register(z.globalRegistry, {
      description: 'The seed used for generation',
    }),
    video: zWanV26ReferenceToVideoVideoFile,
  })
  .register(z.globalRegistry, {
    description: 'Output for reference-to-video generation',
  })

/**
 * Veo31VideoToVideoInput
 *
 * Input for video extension/video-to-video generation.
 */
export const zVeo31FastExtendVideoInput = z
  .object({
    prompt: z.string().max(20000).register(z.globalRegistry, {
      description:
        'The text prompt describing how the video should be extended',
    }),
    duration: z.optional(
      z.enum(['7s']).register(z.globalRegistry, {
        description: 'The duration of the generated video.',
      }),
    ),
    aspect_ratio: z.optional(
      z.enum(['auto', '16:9', '9:16']).register(z.globalRegistry, {
        description: 'The aspect ratio of the generated video.',
      }),
    ),
    generate_audio: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description: 'Whether to generate audio for the video.',
        }),
      )
      .default(true),
    auto_fix: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.',
        }),
      )
      .default(false),
    video_url: z.string().register(z.globalRegistry, {
      description:
        'URL of the video to extend. The video should be 720p or 1080p resolution in 16:9 or 9:16 aspect ratio.',
    }),
    resolution: z.optional(
      z.enum(['720p']).register(z.globalRegistry, {
        description: 'The resolution of the generated video.',
      }),
    ),
    seed: z.optional(
      z.int().register(z.globalRegistry, {
        description: 'The seed for the random number generator.',
      }),
    ),
    negative_prompt: z.optional(
      z.string().register(z.globalRegistry, {
        description: 'A negative prompt to guide the video generation.',
      }),
    ),
  })
  .register(z.globalRegistry, {
    description: 'Input for video extension/video-to-video generation.',
  })

/**
 * File
 */
export const zFalAiVeo31FastExtendVideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * Veo31VideoToVideoOutput
 */
export const zVeo31FastExtendVideoOutput = z.object({
  video: zFalAiVeo31FastExtendVideoFile,
})

/**
 * Veo31VideoToVideoInput
 *
 * Input for video extension/video-to-video generation.
 */
export const zVeo31ExtendVideoInput = z
  .object({
    prompt: z.string().max(20000).register(z.globalRegistry, {
      description:
        'The text prompt describing how the video should be extended',
    }),
    duration: z.optional(
      z.enum(['7s']).register(z.globalRegistry, {
        description: 'The duration of the generated video.',
      }),
    ),
    aspect_ratio: z.optional(
      z.enum(['auto', '16:9', '9:16']).register(z.globalRegistry, {
        description: 'The aspect ratio of the generated video.',
      }),
    ),
    generate_audio: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description: 'Whether to generate audio for the video.',
        }),
      )
      .default(true),
    auto_fix: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.',
        }),
      )
      .default(false),
    video_url: z.string().register(z.globalRegistry, {
      description:
        'URL of the video to extend. The video should be 720p or 1080p resolution in 16:9 or 9:16 aspect ratio.',
    }),
    resolution: z.optional(
      z.enum(['720p']).register(z.globalRegistry, {
        description: 'The resolution of the generated video.',
      }),
    ),
    seed: z.optional(
      z.int().register(z.globalRegistry, {
        description: 'The seed for the random number generator.',
      }),
    ),
    negative_prompt: z.optional(
      z.string().register(z.globalRegistry, {
        description: 'A negative prompt to guide the video generation.',
      }),
    ),
  })
  .register(z.globalRegistry, {
    description: 'Input for video extension/video-to-video generation.',
  })

/**
 * File
 */
export const zFalAiVeo31ExtendVideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * Veo31VideoToVideoOutput
 */
export const zVeo31ExtendVideoOutput = z.object({
  video: zFalAiVeo31ExtendVideoFile,
})

/**
 * OmniVideoElementInput
 */
export const zOmniVideoElementInput = z.object({
  reference_image_urls: z.optional(
    z.array(z.string()).register(z.globalRegistry, {
      description:
        'Additional reference images from different angles. 1-3 images supported. At least one image is required.',
    }),
  ),
  frontal_image_url: z.string().register(z.globalRegistry, {
    description:
      'The frontal image of the element (main view).\n\nMax file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s',
  }),
})

/**
 * File
 */
export const zFalAiKlingVideoO1StandardVideoToVideoReferenceFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * OmniV2VReferenceOutput
 */
export const zKlingVideoO1StandardVideoToVideoReferenceOutput = z.object({
  video: zFalAiKlingVideoO1StandardVideoToVideoReferenceFile,
})

/**
 * OmniVideoElementInput
 */
export const zFalAiKlingVideoO1StandardVideoToVideoEditOmniVideoElementInput =
  z.object({
    reference_image_urls: z.optional(
      z.array(z.string()).register(z.globalRegistry, {
        description:
          'Additional reference images from different angles. 1-3 images supported. At least one image is required.',
      }),
    ),
    frontal_image_url: z.string().register(z.globalRegistry, {
      description:
        'The frontal image of the element (main view).\n\nMax file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s',
    }),
  })

/**
 * File
 */
export const zFalAiKlingVideoO1StandardVideoToVideoEditFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * OmniV2VEditOutput
 */
export const zKlingVideoO1StandardVideoToVideoEditOutput = z.object({
  video: zFalAiKlingVideoO1StandardVideoToVideoEditFile,
})

/**
 * SteadyDancerRequest
 *
 * Request model for SteadyDancer human animation.
 */
export const zSteadyDancerInput = z
  .object({
    prompt: z
      .optional(
        z.string().register(z.globalRegistry, {
          description: 'Text prompt describing the desired animation.',
        }),
      )
      .default('A person dancing with smooth and natural movements.'),
    video_url: z
      .optional(
        z.string().register(z.globalRegistry, {
          description:
            'URL of the driving pose video. The motion from this video will be transferred to the reference image.',
        }),
      )
      .default(
        'https://v3b.fal.media/files/b/0a84de68/jXDWywjhagRfR-GuZjoRs_video.mp4',
      ),
    acceleration: z.optional(
      z.enum(['light', 'moderate', 'aggressive']).register(z.globalRegistry, {
        description: 'Acceleration levels.',
      }),
    ),
    pose_guidance_scale: z
      .optional(
        z.number().gte(0.5).lte(3).register(z.globalRegistry, {
          description: 'Pose guidance scale for pose control strength.',
        }),
      )
      .default(1),
    shift: z
      .optional(
        z.number().gte(1).lte(10).register(z.globalRegistry, {
          description: 'Shift parameter for video generation.',
        }),
      )
      .default(5),
    pose_guidance_end: z
      .optional(
        z.number().gte(0.2).lte(1).register(z.globalRegistry, {
          description:
            'End ratio for pose guidance. Controls when pose guidance ends.',
        }),
      )
      .default(0.4),
    frames_per_second: z.optional(
      z.int().gte(5).lte(24).register(z.globalRegistry, {
        description:
          'Frames per second of the generated video. Must be between 5 to 24. If not specified, uses the FPS from the input video.',
      }),
    ),
    guidance_scale: z
      .optional(
        z.number().gte(1).lte(6).register(z.globalRegistry, {
          description: 'Classifier-free guidance scale for prompt adherence.',
        }),
      )
      .default(1),
    num_frames: z.optional(
      z.int().gte(5).lte(241).register(z.globalRegistry, {
        description:
          'Number of frames to generate. If not specified, uses the frame count from the input video (capped at 241). Will be adjusted to nearest valid value (must satisfy 4k+1 pattern).',
      }),
    ),
    use_turbo: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized (num_inference_steps=6, guidance_scale=1.0) and uses the LightX2V distillation LoRA.',
        }),
      )
      .default(false),
    negative_prompt: z
      .optional(
        z.string().register(z.globalRegistry, {
          description: 'Negative prompt for video generation.',
        }),
      )
      .default(
        'blurred, distorted face, bad anatomy, extra limbs, poorly drawn hands, poorly drawn feet, disfigured, out of frame, duplicate, watermark, signature, text',
      ),
    enable_safety_checker: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description: 'If set to true, the safety checker will be enabled.',
        }),
      )
      .default(false),
    aspect_ratio: z.optional(
      z.enum(['auto', '16:9', '9:16', '1:1']).register(z.globalRegistry, {
        description:
          "Aspect ratio of the generated video. If 'auto', will be determined from the reference image.",
      }),
    ),
    pose_guidance_start: z
      .optional(
        z.number().gte(0).lte(0.5).register(z.globalRegistry, {
          description:
            'Start ratio for pose guidance. Controls when pose guidance begins.',
        }),
      )
      .default(0.1),
    resolution: z.optional(
      z.enum(['480p', '576p', '720p']).register(z.globalRegistry, {
        description:
          'Resolution of the generated video. 576p is default, 720p for higher quality. 480p is lower quality.',
      }),
    ),
    image_url: z
      .optional(
        z.string().register(z.globalRegistry, {
          description:
            'URL of the reference image to animate. This is the person/character whose appearance will be preserved.',
        }),
      )
      .default(
        'https://v3b.fal.media/files/b/0a85edaa/GDUCMPrdvOMcI5JpEcU7f.png',
      ),
    preserve_audio: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'If enabled, copies audio from the input driving video to the output video.',
        }),
      )
      .default(true),
    seed: z.optional(
      z.int().register(z.globalRegistry, {
        description:
          'Random seed for reproducibility. If None, a random seed is chosen.',
      }),
    ),
    num_inference_steps: z
      .optional(
        z.int().gte(4).lte(50).register(z.globalRegistry, {
          description:
            'Number of inference steps for sampling. Higher values give better quality but take longer.',
        }),
      )
      .default(6),
  })
  .register(z.globalRegistry, {
    description: 'Request model for SteadyDancer human animation.',
  })

/**
 * File
 */
export const zFalAiSteadyDancerFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * SteadyDancerResponse
 *
 * Response model for SteadyDancer.
 */
export const zSteadyDancerOutput = z
  .object({
    num_frames: z.int().register(z.globalRegistry, {
      description:
        'The actual number of frames generated (aligned to 4k+1 pattern).',
    }),
    seed: z.int().register(z.globalRegistry, {
      description: 'The seed used for generation.',
    }),
    video: zFalAiSteadyDancerFile,
  })
  .register(z.globalRegistry, {
    description: 'Response model for SteadyDancer.',
  })

/**
 * OneToALLAnimationRequest
 */
export const zOneToAllAnimation13bInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt to generate the video from.',
  }),
  resolution: z.optional(
    z.enum(['480p', '580p', '720p']).register(z.globalRegistry, {
      description: 'The resolution of the video to generate.',
    }),
  ),
  image_guidance_scale: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description:
          'The image guidance scale to use for the video generation.',
      }),
    )
    .default(2),
  pose_guidance_scale: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description: 'The pose guidance scale to use for the video generation.',
      }),
    )
    .default(1.5),
  video_url: z.string().register(z.globalRegistry, {
    description:
      'The URL of the video to use as a reference for the video generation.',
  }),
  image_url: z.string().register(z.globalRegistry, {
    description:
      'The URL of the image to use as a reference for the video generation.',
  }),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(30).register(z.globalRegistry, {
        description:
          'The number of inference steps to use for the video generation.',
      }),
    )
    .default(30),
  negative_prompt: z.string().register(z.globalRegistry, {
    description: 'The negative prompt to generate the video from.',
  }),
})

/**
 * File
 */
export const zFalAiOneToAllAnimation13bFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * OneToALLAnimationResponse
 */
export const zOneToAllAnimation13bOutput = z.object({
  video: zFalAiOneToAllAnimation13bFile,
})

/**
 * OneToALLAnimationRequest
 */
export const zOneToAllAnimation14bInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt to generate the video from.',
  }),
  resolution: z.optional(
    z.enum(['480p', '580p', '720p']).register(z.globalRegistry, {
      description: 'The resolution of the video to generate.',
    }),
  ),
  image_guidance_scale: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description:
          'The image guidance scale to use for the video generation.',
      }),
    )
    .default(2),
  pose_guidance_scale: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description: 'The pose guidance scale to use for the video generation.',
      }),
    )
    .default(1.5),
  video_url: z.string().register(z.globalRegistry, {
    description:
      'The URL of the video to use as a reference for the video generation.',
  }),
  image_url: z.string().register(z.globalRegistry, {
    description:
      'The URL of the image to use as a reference for the video generation.',
  }),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(30).register(z.globalRegistry, {
        description:
          'The number of inference steps to use for the video generation.',
      }),
    )
    .default(30),
  negative_prompt: z.string().register(z.globalRegistry, {
    description: 'The negative prompt to generate the video from.',
  }),
})

/**
 * File
 */
export const zFalAiOneToAllAnimation14bFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * OneToALLAnimationResponse
 */
export const zOneToAllAnimation14bOutput = z.object({
  video: zFalAiOneToAllAnimation14bFile,
})

/**
 * Input
 *
 * Input parameters for Wan Vision Enhancer (Video-to-Video)
 */
export const zWanVisionEnhancerInput = z
  .object({
    prompt: z.optional(z.union([z.string(), z.unknown()])),
    video_url: z.string().register(z.globalRegistry, {
      description:
        'The URL of the video to enhance with Wan Video. Maximum 200MB file size. Videos longer than 500 frames will have only the first 500 frames processed (~8-21 seconds depending on fps).',
    }),
    seed: z.optional(z.union([z.int(), z.unknown()])),
    target_resolution: z.optional(
      z.enum(['720p', '1080p']).register(z.globalRegistry, {
        description:
          'Target output resolution for the enhanced video. 720p (native, fast) or 1080p (upscaled, slower). Processing is always done at 720p, then upscaled if 1080p selected.',
      }),
    ),
    negative_prompt: z.optional(z.union([z.string(), z.unknown()])),
    creativity: z
      .optional(
        z.int().gte(0).lte(4).register(z.globalRegistry, {
          description:
            'Controls how much the model enhances/changes the video. 0 = Minimal change (preserves original), 1 = Subtle enhancement (default), 2 = Medium enhancement, 3 = Strong enhancement, 4 = Maximum enhancement.',
        }),
      )
      .default(1),
  })
  .register(z.globalRegistry, {
    description: 'Input parameters for Wan Vision Enhancer (Video-to-Video)',
  })

/**
 * File
 */
export const zFalAiWanVisionEnhancerFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * Output
 *
 * Output from Wan Vision Enhancer
 */
export const zWanVisionEnhancerOutput = z
  .object({
    seed: z.int().register(z.globalRegistry, {
      description: 'The seed used for generation.',
    }),
    timings: z.record(z.string(), z.number()).register(z.globalRegistry, {
      description: 'The timings of the different steps in the workflow.',
    }),
    video: zFalAiWanVisionEnhancerFile,
  })
  .register(z.globalRegistry, {
    description: 'Output from Wan Vision Enhancer',
  })

/**
 * React1Input
 */
export const zSyncLipsyncReact1Input = z.object({
  emotion: z
    .enum(['happy', 'angry', 'sad', 'neutral', 'disgusted', 'surprised'])
    .register(z.globalRegistry, {
      description:
        'Emotion prompt for the generation. Currently supports single-word emotions only.',
    }),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL to the input video. Must be **15 seconds or shorter**.',
  }),
  lipsync_mode: z.optional(
    z
      .enum(['cut_off', 'loop', 'bounce', 'silence', 'remap'])
      .register(z.globalRegistry, {
        description:
          'Lipsync mode when audio and video durations are out of sync.',
      }),
  ),
  audio_url: z.string().register(z.globalRegistry, {
    description: 'URL to the input audio. Must be **15 seconds or shorter**.',
  }),
  temperature: z
    .optional(
      z.number().gte(0).lte(1).register(z.globalRegistry, {
        description: 'Controls the expresiveness of the lipsync.',
      }),
    )
    .default(0.5),
  model_mode: z.optional(
    z.enum(['lips', 'face', 'head']).register(z.globalRegistry, {
      description:
        'Controls the edit region and movement scope for the model. Available options:\n- `lips`: Only lipsync using react-1 (minimal facial changes).\n- `face`: Lipsync + facial expressions without head movements.\n- `head`: Lipsync + facial expressions + natural talking head movements.',
    }),
  ),
})

/**
 * VideoFile
 */
export const zFalAiSyncLipsyncReact1VideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  duration: z.optional(
    z.number().register(z.globalRegistry, {
      description: 'The duration of the video',
    }),
  ),
  height: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The height of the video',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  width: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The width of the video',
    }),
  ),
  fps: z.optional(
    z.number().register(z.globalRegistry, {
      description: 'The FPS of the video',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  num_frames: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The number of frames in the video',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * React1Output
 */
export const zSyncLipsyncReact1Output = z.object({
  video: zFalAiSyncLipsyncReact1VideoFile,
})

/**
 * FastGeneralRembgInput
 */
export const zVideoBackgroundRemovalFastInput = z.object({
  video_url: z.url().min(1).max(2083),
  subject_is_person: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Set to False if the subject is not a person.',
      }),
    )
    .default(true),
  output_codec: z.optional(
    z.enum(['vp9', 'h264']).register(z.globalRegistry, {
      description:
        'Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality.',
    }),
  ),
  refine_foreground_edges: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: "Improves the quality of the extracted object's edges.",
      }),
    )
    .default(true),
})

/**
 * File
 */
export const zVeedVideoBackgroundRemovalFastFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * FastGeneralRembgOutput
 */
export const zVideoBackgroundRemovalFastOutput = z.object({
  video: z.array(zVeedVideoBackgroundRemovalFastFile),
})

/**
 * OmniVideoElementInput
 */
export const zFalAiKlingVideoO1VideoToVideoEditOmniVideoElementInput = z.object(
  {
    reference_image_urls: z.optional(
      z.array(z.string()).register(z.globalRegistry, {
        description:
          'Additional reference images from different angles. 1-3 images supported. At least one image is required.',
      }),
    ),
    frontal_image_url: z.string().register(z.globalRegistry, {
      description:
        'The frontal image of the element (main view).\n\nMax file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s',
    }),
  },
)

/**
 * File
 */
export const zFalAiKlingVideoO1VideoToVideoEditFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * OmniV2VEditOutput
 */
export const zKlingVideoO1VideoToVideoEditOutput = z.object({
  video: zFalAiKlingVideoO1VideoToVideoEditFile,
})

/**
 * OmniVideoElementInput
 */
export const zFalAiKlingVideoO1VideoToVideoReferenceOmniVideoElementInput =
  z.object({
    reference_image_urls: z.optional(
      z.array(z.string()).register(z.globalRegistry, {
        description:
          'Additional reference images from different angles. 1-3 images supported. At least one image is required.',
      }),
    ),
    frontal_image_url: z.string().register(z.globalRegistry, {
      description:
        'The frontal image of the element (main view).\n\nMax file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s',
    }),
  })

/**
 * File
 */
export const zFalAiKlingVideoO1VideoToVideoReferenceFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * OmniV2VReferenceOutput
 */
export const zKlingVideoO1VideoToVideoReferenceOutput = z.object({
  video: zFalAiKlingVideoO1VideoToVideoReferenceFile,
})

/**
 * GeneralRembgInput
 */
export const zVeedVideoBackgroundRemovalVideoBackgroundRemovalInput = z.object({
  video_url: z.url().min(1).max(2083),
  subject_is_person: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Set to False if the subject is not a person.',
      }),
    )
    .default(true),
  output_codec: z.optional(
    z.enum(['vp9', 'h264']).register(z.globalRegistry, {
      description:
        'Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality.',
    }),
  ),
  refine_foreground_edges: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: "Improves the quality of the extracted object's edges.",
      }),
    )
    .default(true),
})

/**
 * File
 */
export const zVeedVideoBackgroundRemovalFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * GeneralRembgOutput
 */
export const zVeedVideoBackgroundRemovalVideoBackgroundRemovalOutput = z.object(
  {
    video: z.array(zVeedVideoBackgroundRemovalFile),
  },
)

/**
 * GreenScreenRembgInput
 */
export const zVideoBackgroundRemovalGreenScreenInput = z.object({
  video_url: z.url().min(1).max(2083),
  output_codec: z.optional(
    z.enum(['vp9', 'h264']).register(z.globalRegistry, {
      description:
        'Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality.',
    }),
  ),
  spill_suppression_strength: z.optional(
    z.union([z.number().gte(0).lte(1), z.unknown()]),
  ),
})

/**
 * File
 */
export const zVeedVideoBackgroundRemovalGreenScreenFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * GreenScreenRembgOutput
 */
export const zVideoBackgroundRemovalGreenScreenOutput = z.object({
  video: z.array(zVeedVideoBackgroundRemovalGreenScreenFile),
})

/**
 * LTXRetakeVideoRequest
 */
export const zLtx2RetakeVideoInput = z.object({
  prompt: z.string().min(1).max(5000).register(z.globalRegistry, {
    description: 'The prompt to retake the video with',
  }),
  duration: z
    .optional(
      z.number().gte(2).lte(20).register(z.globalRegistry, {
        description: 'The duration of the video to retake in seconds',
      }),
    )
    .default(5),
  video_url: z.string().register(z.globalRegistry, {
    description: 'The URL of the video to retake',
  }),
  start_time: z
    .optional(
      z.number().gte(0).lte(20).register(z.globalRegistry, {
        description: 'The start time of the video to retake in seconds',
      }),
    )
    .default(0),
  retake_mode: z.optional(
    z
      .enum(['replace_audio', 'replace_video', 'replace_audio_and_video'])
      .register(z.globalRegistry, {
        description: 'The retake mode to use for the retake',
      }),
  ),
})

/**
 * VideoFile
 */
export const zFalAiLtx2RetakeVideoVideoFile = z.object({
  height: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The height of the video',
    }),
  ),
  duration: z.optional(
    z.number().register(z.globalRegistry, {
      description: 'The duration of the video',
    }),
  ),
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  fps: z.optional(
    z.number().register(z.globalRegistry, {
      description: 'The FPS of the video',
    }),
  ),
  width: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The width of the video',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  num_frames: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The number of frames in the video',
    }),
  ),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * LTXRetakeVideoResponse
 */
export const zLtx2RetakeVideoOutput = z.object({
  video: zFalAiLtx2RetakeVideoVideoFile,
})

/**
 * LucyEditFastInput
 */
export const zLucyEditFastInput = z.object({
  sync_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          '\n            If set to true, the function will wait for the video to be generated\n            and uploaded before returning the response. This will increase the\n            latency of the function but it allows you to get the video directly\n            in the response without going through the CDN.\n        ',
      }),
    )
    .default(false),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the video to edit',
  }),
  prompt: z.string().max(1500).register(z.globalRegistry, {
    description: 'Text description of the desired video content',
  }),
  enhance_prompt: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enhance the prompt for better results.',
      }),
    )
    .default(true),
})

/**
 * File
 */
export const zDecartLucyEditFastFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * LucyEditFastOutput
 */
export const zLucyEditFastOutput = z.object({
  video: zDecartLucyEditFastFile,
})

/**
 * BoxPrompt
 */
export const zBoxPrompt = z.object({
  y_min: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'Y Min Coordinate of the box',
    }),
  ),
  object_id: z.optional(
    z.int().register(z.globalRegistry, {
      description:
        'Optional object identifier. Boxes sharing an object id refine the same object.',
    }),
  ),
  frame_index: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The frame index to interact with.',
    }),
  ),
  x_max: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'X Max Coordinate of the box',
    }),
  ),
  x_min: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'X Min Coordinate of the box',
    }),
  ),
  y_max: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'Y Max Coordinate of the box',
    }),
  ),
})

/**
 * PointPrompt
 */
export const zPointPrompt = z.object({
  y: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'Y Coordinate of the prompt',
    }),
  ),
  x: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'X Coordinate of the prompt',
    }),
  ),
  object_id: z.optional(
    z.int().register(z.globalRegistry, {
      description:
        'Optional object identifier. Prompts sharing an object id refine the same object.',
    }),
  ),
  frame_index: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The frame index to interact with.',
    }),
  ),
  label: z.optional(
    z.union([z.literal(0), z.literal(1)]).register(z.globalRegistry, {
      description: '1 for foreground, 0 for background',
    }),
  ),
})

/**
 * SAM3VideoRLEInput
 */
export const zSam3VideoRleInput = z.object({
  prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description:
          "Text prompt for segmentation. Use commas to track multiple objects (e.g., 'person, cloth').",
      }),
    )
    .default(''),
  video_url: z.string().register(z.globalRegistry, {
    description: 'The URL of the video to be segmented.',
  }),
  detection_threshold: z
    .optional(
      z.number().gte(0.01).lte(1).register(z.globalRegistry, {
        description:
          'Detection confidence threshold (0.0-1.0). Lower = more detections but less precise. Defaults: 0.5 for existing, 0.7 for new objects. Try 0.2-0.3 if text prompts fail.',
      }),
    )
    .default(0.5),
  box_prompts: z
    .optional(
      z.array(zBoxPrompt).register(z.globalRegistry, {
        description: 'List of box prompts with optional frame_index.',
      }),
    )
    .default([]),
  boundingbox_zip: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Return per-frame bounding box overlays as a zip archive.',
      }),
    )
    .default(false),
  point_prompts: z
    .optional(
      z.array(zPointPrompt).register(z.globalRegistry, {
        description: 'List of point prompts with frame indices.',
      }),
    )
    .default([]),
  frame_index: z
    .optional(
      z.int().register(z.globalRegistry, {
        description:
          'Frame index used for initial interaction when mask_url is provided.',
      }),
    )
    .default(0),
  mask_url: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The URL of the mask to be applied initially.',
    }),
  ),
  apply_mask: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Apply the mask on the video.',
      }),
    )
    .default(false),
})

/**
 * File
 */
export const zFalAiSam3VideoRleFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * SAM3VideoOutput
 */
export const zSam3VideoRleOutput = z.object({
  boundingbox_frames_zip: z.optional(zFalAiSam3VideoRleFile),
  video: zFalAiSam3VideoRleFile,
})

/**
 * BoxPromptBase
 */
export const zBoxPromptBase = z.object({
  y_min: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'Y Min Coordinate of the box',
    }),
  ),
  object_id: z.optional(
    z.int().register(z.globalRegistry, {
      description:
        'Optional object identifier. Boxes sharing an object id refine the same object.',
    }),
  ),
  x_max: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'X Max Coordinate of the box',
    }),
  ),
  x_min: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'X Min Coordinate of the box',
    }),
  ),
  y_max: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'Y Max Coordinate of the box',
    }),
  ),
})

/**
 * PointPromptBase
 */
export const zPointPromptBase = z.object({
  y: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'Y Coordinate of the prompt',
    }),
  ),
  x: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'X Coordinate of the prompt',
    }),
  ),
  object_id: z.optional(
    z.int().register(z.globalRegistry, {
      description:
        'Optional object identifier. Prompts sharing an object id refine the same object.',
    }),
  ),
  label: z.optional(
    z.union([z.literal(0), z.literal(1)]).register(z.globalRegistry, {
      description: '1 for foreground, 0 for background',
    }),
  ),
})

/**
 * SAM3VideoInput
 */
export const zSam3VideoInput = z.object({
  prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description:
          "Text prompt for segmentation. Use commas to track multiple objects (e.g., 'person, cloth').",
      }),
    )
    .default(''),
  video_url: z.string().register(z.globalRegistry, {
    description: 'The URL of the video to be segmented.',
  }),
  detection_threshold: z
    .optional(
      z.number().gte(0.1).lte(1).register(z.globalRegistry, {
        description:
          'Detection confidence threshold (0.0-1.0). Lower = more detections but less precise. ',
      }),
    )
    .default(0.5),
  box_prompts: z
    .optional(
      z.array(zBoxPromptBase).register(z.globalRegistry, {
        description:
          'List of box prompt coordinates (x_min, y_min, x_max, y_max).',
      }),
    )
    .default([]),
  point_prompts: z
    .optional(
      z.array(zPointPromptBase).register(z.globalRegistry, {
        description: 'List of point prompts',
      }),
    )
    .default([]),
  apply_mask: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Apply the mask on the video.',
      }),
    )
    .default(true),
  text_prompt: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        "[DEPRECATED] Use 'prompt' instead. Kept for backward compatibility.",
    }),
  ),
})

/**
 * File
 */
export const zFalAiSam3VideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * SAM3VideoOutput
 */
export const zSam3VideoOutput = z.object({
  boundingbox_frames_zip: z.optional(zFalAiSam3VideoFile),
  video: zFalAiSam3VideoFile,
})

/**
 * EdittoInput
 */
export const zEdittoInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The text prompt to guide video generation.',
  }),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL to the source video file. Required for inpainting.',
  }),
  acceleration: z.optional(
    z.union([z.enum(['none', 'low', 'regular']), z.unknown()]),
  ),
  num_interpolated_frames: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Number of frames to interpolate between the original frames. A value of 0 means no interpolation.',
      }),
    )
    .default(0),
  temporal_downsample_factor: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.',
      }),
    )
    .default(0),
  shift: z
    .optional(
      z.number().gte(1).lte(15).register(z.globalRegistry, {
        description: 'Shift parameter for video generation.',
      }),
    )
    .default(5),
  frames_per_second: z.optional(z.union([z.int().gte(5).lte(30), z.unknown()])),
  match_input_num_frames: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.',
      }),
    )
    .default(false),
  guidance_scale: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description:
          'Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.',
      }),
    )
    .default(5),
  num_frames: z
    .optional(
      z.int().gte(17).lte(241).register(z.globalRegistry, {
        description:
          'Number of frames to generate. Must be between 81 to 241 (inclusive).',
      }),
    )
    .default(81),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If set to true, the safety checker will be enabled.',
      }),
    )
    .default(false),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'Negative prompt for video generation.',
      }),
    )
    .default(
      'letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards',
    ),
  sampler: z.optional(
    z.enum(['unipc', 'dpm++', 'euler']).register(z.globalRegistry, {
      description: 'Sampler to use for video generation.',
    }),
  ),
  video_write_mode: z.optional(
    z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
      description: 'The write mode of the generated video.',
    }),
  ),
  resolution: z.optional(
    z
      .enum(['auto', '240p', '360p', '480p', '580p', '720p'])
      .register(z.globalRegistry, {
        description: 'Resolution of the generated video.',
      }),
  ),
  aspect_ratio: z.optional(
    z.enum(['auto', '16:9', '1:1', '9:16']).register(z.globalRegistry, {
      description: 'Aspect ratio of the generated video.',
    }),
  ),
  return_frames_zip: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, also return a ZIP file containing all generated frames.',
      }),
    )
    .default(false),
  video_quality: z.optional(
    z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
      description: 'The quality of the generated video.',
    }),
  ),
  match_input_frames_per_second: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.',
      }),
    )
    .default(false),
  sync_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
      }),
    )
    .default(false),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(50).register(z.globalRegistry, {
        description:
          'Number of inference steps for sampling. Higher values give better quality but take longer.',
      }),
    )
    .default(30),
  seed: z.optional(z.union([z.int(), z.unknown()])),
  enable_auto_downsample: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.',
      }),
    )
    .default(false),
})

/**
 * File
 */
export const zFalAiEdittoFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * VideoFile
 */
export const zFalAiEdittoVideoFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  duration: z.optional(z.union([z.number(), z.unknown()])),
  height: z.optional(z.union([z.int(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  width: z.optional(z.union([z.int(), z.unknown()])),
  fps: z.optional(z.union([z.number(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  num_frames: z.optional(z.union([z.int(), z.unknown()])),
})

/**
 * EdittoOutput
 */
export const zEdittoOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for generation.',
  }),
  frames_zip: z.optional(z.union([zFalAiEdittoFile, z.unknown()])),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiEdittoVideoFile,
})

/**
 * FlashVSRPlusVideoInput
 *
 * Input fields common to FlashVSR+ image/video endpoints.
 */
export const zFlashvsrUpscaleVideoInput = z
  .object({
    video_url: z.string().register(z.globalRegistry, {
      description: 'The input video to be upscaled',
    }),
    acceleration: z.optional(
      z.enum(['regular', 'high', 'full']).register(z.globalRegistry, {
        description:
          'Acceleration mode for VAE decoding. Options: regular (best quality), high (balanced), full (fastest). More accerleation means longer duration videos can be processed too.',
      }),
    ),
    quality: z
      .optional(
        z.int().gte(0).lte(100).register(z.globalRegistry, {
          description:
            'Quality level for tile blending (0-100). Controls overlap between tiles to prevent grid artifacts. Higher values provide better quality with more overlap. Recommended: 70-85 for high-res videos, 50-70 for faster processing.',
        }),
      )
      .default(70),
    output_format: z.optional(
      z
        .enum(['X264 (.mp4)', 'VP9 (.webm)', 'PRORES4444 (.mov)', 'GIF (.gif)'])
        .register(z.globalRegistry, {
          description: 'The format of the output video.',
        }),
    ),
    color_fix: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description: 'Color correction enabled.',
        }),
      )
      .default(true),
    output_write_mode: z.optional(
      z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
        description: 'The write mode of the output video.',
      }),
    ),
    sync_mode: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'If `True`, the media will be returned inline and not stored in history.',
        }),
      )
      .default(false),
    output_quality: z.optional(
      z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
        description: 'The quality of the output video.',
      }),
    ),
    upscale_factor: z
      .optional(
        z.number().gte(1).lte(4).register(z.globalRegistry, {
          description: 'Upscaling factor to be used.',
        }),
      )
      .default(2),
    preserve_audio: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'Copy the original audio tracks into the upscaled video using FFmpeg when possible.',
        }),
      )
      .default(false),
    seed: z.optional(
      z.int().register(z.globalRegistry, {
        description: 'The random seed used for the generation process.',
      }),
    ),
  })
  .register(z.globalRegistry, {
    description: 'Input fields common to FlashVSR+ image/video endpoints.',
  })

/**
 * File
 */
export const zFalAiFlashvsrUpscaleVideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * FlashVSRPlusVideoOutput
 */
export const zFlashvsrUpscaleVideoOutput = z.object({
  seed: z.int().register(z.globalRegistry, {
    description: 'The random seed used for the generation process.',
  }),
  video: zFalAiFlashvsrUpscaleVideoFile,
})

/**
 * AutoSubtitleInput
 *
 * Input model for automatic subtitle generation and styling
 */
export const zWorkflowUtilitiesAutoSubtitleInput = z
  .object({
    video_url: z.string().register(z.globalRegistry, {
      description: 'URL of the video file to add automatic subtitles to',
    }),
    font_weight: z.optional(
      z.enum(['normal', 'bold', 'black']).register(z.globalRegistry, {
        description: 'Font weight (TikTok style typically uses bold or black)',
      }),
    ),
    stroke_width: z
      .optional(
        z.int().gte(0).lte(10).register(z.globalRegistry, {
          description: 'Text stroke/outline width in pixels (0 for no stroke)',
        }),
      )
      .default(3),
    font_color: z.optional(
      z
        .enum([
          'white',
          'black',
          'red',
          'green',
          'blue',
          'yellow',
          'orange',
          'purple',
          'pink',
          'brown',
          'gray',
          'cyan',
          'magenta',
        ])
        .register(z.globalRegistry, {
          description: 'Subtitle text color for non-active words',
        }),
    ),
    font_size: z
      .optional(
        z.int().gte(20).lte(150).register(z.globalRegistry, {
          description:
            'Font size for subtitles (TikTok style uses larger text)',
        }),
      )
      .default(100),
    language: z
      .optional(
        z.string().register(z.globalRegistry, {
          description:
            "Language code for transcription (e.g., 'en', 'es', 'fr', 'de', 'it', 'pt', 'nl', 'ja', 'zh', 'ko') or 3-letter ISO code (e.g., 'eng', 'spa', 'fra')",
        }),
      )
      .default('en'),
    highlight_color: z.optional(
      z
        .enum([
          'white',
          'black',
          'red',
          'green',
          'blue',
          'yellow',
          'orange',
          'purple',
          'pink',
          'brown',
          'gray',
          'cyan',
          'magenta',
        ])
        .register(z.globalRegistry, {
          description:
            'Color for the currently speaking word (karaoke-style highlight)',
        }),
    ),
    background_opacity: z
      .optional(
        z.number().gte(0).lte(1).register(z.globalRegistry, {
          description:
            'Background opacity (0.0 = fully transparent, 1.0 = fully opaque)',
        }),
      )
      .default(0),
    stroke_color: z.optional(
      z
        .enum([
          'black',
          'white',
          'red',
          'green',
          'blue',
          'yellow',
          'orange',
          'purple',
          'pink',
          'brown',
          'gray',
          'cyan',
          'magenta',
        ])
        .register(z.globalRegistry, {
          description: 'Text stroke/outline color',
        }),
    ),
    y_offset: z
      .optional(
        z.int().gte(-200).lte(200).register(z.globalRegistry, {
          description:
            'Vertical offset in pixels (positive = move down, negative = move up)',
        }),
      )
      .default(75),
    font_name: z
      .optional(
        z.string().register(z.globalRegistry, {
          description:
            "Any Google Font name from fonts.google.com (e.g., 'Montserrat', 'Poppins', 'BBH Sans Hegarty')",
        }),
      )
      .default('Montserrat'),
    enable_animation: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'Enable animation effects for subtitles (bounce style entrance)',
        }),
      )
      .default(true),
    position: z.optional(
      z.enum(['top', 'center', 'bottom']).register(z.globalRegistry, {
        description: 'Vertical position of subtitles',
      }),
    ),
    words_per_subtitle: z
      .optional(
        z.int().gte(1).lte(12).register(z.globalRegistry, {
          description:
            'Maximum number of words per subtitle segment. Use 1 for single-word display, 2-3 for short phrases, or 8-12 for full sentences.',
        }),
      )
      .default(3),
    background_color: z.optional(
      z
        .enum([
          'black',
          'white',
          'red',
          'green',
          'blue',
          'yellow',
          'orange',
          'purple',
          'pink',
          'brown',
          'gray',
          'cyan',
          'magenta',
          'none',
          'transparent',
        ])
        .register(z.globalRegistry, {
          description:
            "Background color behind text ('none' or 'transparent' for no background)",
        }),
    ),
  })
  .register(z.globalRegistry, {
    description: 'Input model for automatic subtitle generation and styling',
  })

/**
 * File
 */
export const zFalAiWorkflowUtilitiesAutoSubtitleFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * AutoSubtitleOutput
 *
 * Output model for video with automatic subtitles
 */
export const zWorkflowUtilitiesAutoSubtitleOutput = z
  .object({
    transcription: z.string().register(z.globalRegistry, {
      description: 'Full transcription text',
    }),
    subtitle_count: z.int().register(z.globalRegistry, {
      description: 'Number of subtitle segments generated',
    }),
    transcription_metadata: z.optional(
      z.record(z.string(), z.unknown()).register(z.globalRegistry, {
        description:
          'Additional transcription metadata from ElevenLabs (language, segments, etc.)',
      }),
    ),
    words: z.optional(
      z.array(z.record(z.string(), z.unknown())).register(z.globalRegistry, {
        description: 'Word-level timing information from transcription service',
      }),
    ),
    video: zFalAiWorkflowUtilitiesAutoSubtitleFile,
  })
  .register(z.globalRegistry, {
    description: 'Output model for video with automatic subtitles',
  })

/**
 * UpscaleInput
 */
export const zBytedanceUpscalerUpscaleVideoInput = z.object({
  target_fps: z.optional(
    z.enum(['30fps', '60fps']).register(z.globalRegistry, {
      description: 'The target FPS of the video to upscale.',
    }),
  ),
  video_url: z.string().register(z.globalRegistry, {
    description: 'The URL of the video to upscale.',
  }),
  target_resolution: z.optional(
    z.enum(['1080p', '2k', '4k']).register(z.globalRegistry, {
      description: 'The target resolution of the video to upscale.',
    }),
  ),
})

/**
 * File
 */
export const zFalAiBytedanceUpscalerUpscaleVideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * UpscaleOutput
 */
export const zBytedanceUpscalerUpscaleVideoOutput = z.object({
  duration: z.number().register(z.globalRegistry, {
    description: 'Duration of audio input/video output as used for billing.',
  }),
  video: zFalAiBytedanceUpscalerUpscaleVideoFile,
})

/**
 * VideoEffectInputWan
 */
export const zVideoAsPromptInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt to generate an image from.',
  }),
  aspect_ratio: z.optional(
    z.enum(['16:9', '9:16']).register(z.globalRegistry, {
      description: 'Aspect ratio of the generated video.',
    }),
  ),
  resolution: z.optional(
    z.enum(['480p', '580p', '720p']).register(z.globalRegistry, {
      description: 'Resolution of the generated video.',
    }),
  ),
  video_url: z.string().register(z.globalRegistry, {
    description: 'reference video to generate effect video from.',
  }),
  image_url: z.string().register(z.globalRegistry, {
    description: 'Input image to generate the effect video for.',
  }),
  fps: z
    .optional(
      z.int().gte(1).lte(60).register(z.globalRegistry, {
        description:
          "Frames per second for the output video. Only applicable if output_type is 'video'.",
      }),
    )
    .default(16),
  video_description: z.string().register(z.globalRegistry, {
    description: 'A brief description of the input video content.',
  }),
  seed: z.optional(z.union([z.int(), z.unknown()])),
  guidance_scale: z
    .optional(
      z.number().gte(1).lte(20).register(z.globalRegistry, {
        description: 'Guidance scale for generation.',
      }),
    )
    .default(5),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If set to true, the safety checker will be enabled.',
      }),
    )
    .default(true),
  num_frames: z
    .optional(
      z.int().gte(1).lte(100).register(z.globalRegistry, {
        description: 'The number of frames to generate.',
      }),
    )
    .default(49),
})

/**
 * File
 */
export const zFalAiVideoAsPromptFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * VideoEffectOutput
 */
export const zVideoAsPromptOutput = z.object({
  video: zFalAiVideoAsPromptFile,
})

/**
 * VideoInputV2
 */
export const zBirefnetV2VideoInput = z.object({
  video_write_mode: z.optional(
    z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
      description: 'The write mode of the generated video.',
    }),
  ),
  video_output_type: z.optional(
    z
      .enum(['X264 (.mp4)', 'VP9 (.webm)', 'PRORES4444 (.mov)', 'GIF (.gif)'])
      .register(z.globalRegistry, {
        description: 'The output type of the generated video.',
      }),
  ),
  operating_resolution: z.optional(
    z.enum(['1024x1024', '2048x2048', '2304x2304']).register(z.globalRegistry, {
      description:
        "The resolution to operate on. The higher the resolution, the more accurate the output will be for high res input images. The '2304x2304' option is only available for the 'General Use (Dynamic)' model.",
    }),
  ),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the video to remove background from',
  }),
  model: z.optional(
    z
      .enum([
        'General Use (Light)',
        'General Use (Light 2K)',
        'General Use (Heavy)',
        'Matting',
        'Portrait',
        'General Use (Dynamic)',
      ])
      .register(z.globalRegistry, {
        description:
          "\n            Model to use for background removal.\n            The 'General Use (Light)' model is the original model used in the BiRefNet repository.\n            The 'General Use (Light 2K)' model is the original model used in the BiRefNet repository but trained with 2K images.\n            The 'General Use (Heavy)' model is a slower but more accurate model.\n            The 'Matting' model is a model trained specifically for matting images.\n            The 'Portrait' model is a model trained specifically for portrait images.\n            The 'General Use (Dynamic)' model supports dynamic resolutions from 256x256 to 2304x2304.\n            The 'General Use (Light)' model is recommended for most use cases.\n\n            The corresponding models are as follows:\n            - 'General Use (Light)': BiRefNet\n            - 'General Use (Light 2K)': BiRefNet_lite-2K\n            - 'General Use (Heavy)': BiRefNet_lite\n            - 'Matting': BiRefNet-matting\n            - 'Portrait': BiRefNet-portrait\n            - 'General Use (Dynamic)': BiRefNet_dynamic\n        ",
      }),
  ),
  video_quality: z.optional(
    z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
      description: 'The quality of the generated video.',
    }),
  ),
  sync_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
      }),
    )
    .default(false),
  output_mask: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to output the mask used to remove the background',
      }),
    )
    .default(false),
  refine_foreground: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to refine the foreground using the estimated mask',
      }),
    )
    .default(true),
})

/**
 * VideoFile
 */
export const zFalAiBirefnetV2VideoVideoFile = z.object({
  height: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The height of the video',
    }),
  ),
  duration: z.optional(
    z.number().register(z.globalRegistry, {
      description: 'The duration of the video',
    }),
  ),
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  fps: z.optional(
    z.number().register(z.globalRegistry, {
      description: 'The FPS of the video',
    }),
  ),
  width: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The width of the video',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  num_frames: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The number of frames in the video',
    }),
  ),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * VideoOutput
 */
export const zBirefnetV2VideoOutput = z.object({
  video: zFalAiBirefnetV2VideoVideoFile,
  mask_video: z.optional(zFalAiBirefnetV2VideoVideoFile),
})

/**
 * Q2VideoExtensionRequest
 */
export const zViduQ2VideoExtensionProInput = z.object({
  prompt: z.optional(
    z.string().max(3000).register(z.globalRegistry, {
      description: 'text prompt to guide the video extension',
    }),
  ),
  duration: z.optional(
    z
      .union([
        z.literal(2),
        z.literal(3),
        z.literal(4),
        z.literal(5),
        z.literal(6),
        z.literal(7),
      ])
      .register(z.globalRegistry, {
        description: 'Duration of the extension in seconds',
      }),
  ),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the video to extend',
  }),
  resolution: z.optional(
    z.enum(['720p', '1080p']).register(z.globalRegistry, {
      description: 'Output video resolution',
    }),
  ),
  seed: z.optional(
    z.int().register(z.globalRegistry, {
      description:
        'Random seed for reproducibility. If None, a random seed is chosen.',
    }),
  ),
})

/**
 * File
 */
export const zFalAiViduQ2VideoExtensionProFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * Q2VideoExtensionOutput
 */
export const zViduQ2VideoExtensionProOutput = z.object({
  video: zFalAiViduQ2VideoExtensionProFile,
})

/**
 * Input
 */
export const zSfxV15VideoToVideoInput = z.object({
  num_samples: z.optional(z.union([z.int().gte(2).lte(8), z.unknown()])),
  duration: z.optional(z.union([z.number().gte(1).lte(10), z.unknown()])),
  start_offset: z.optional(z.union([z.number().gte(0), z.unknown()])),
  video_url: z.url().min(1).max(2083).register(z.globalRegistry, {
    description:
      'A video url that can accessed from the API to process and add sound effects',
  }),
  seed: z.optional(z.union([z.int().gte(1), z.unknown()])),
  text_prompt: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * Video
 */
export const zVideoOutput = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * VideoToVideoInput
 */
export const zKreaWan14bVideoToVideoInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'Prompt for the video-to-video generation.',
  }),
  video_url: z.string().register(z.globalRegistry, {
    description:
      'URL of the input video. Currently, only outputs of 16:9 aspect ratio and 480p resolution are supported. Video duration should be less than 1000 frames at 16fps, and output frames will be 6 plus a multiple of 12, for example 18, 30, 42, etc.',
  }),
  strength: z
    .optional(
      z.number().gte(0.01).lte(1).register(z.globalRegistry, {
        description:
          'Denoising strength for the video-to-video generation. 0.0 preserves the original, 1.0 completely remakes the video.',
      }),
    )
    .default(0.85),
  enable_prompt_expansion: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.',
      }),
    )
    .default(false),
  seed: z.optional(z.union([z.int(), z.unknown()])),
})

/**
 * File
 */
export const zFalAiKreaWan14bVideoToVideoFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * VideoToVideoOutput
 */
export const zKreaWan14bVideoToVideoOutput = z.object({
  video: zFalAiKreaWan14bVideoToVideoFile,
})

/**
 * RemixInput
 */
export const zSora2VideoToVideoRemixInput = z.object({
  prompt: z.string().min(1).max(5000).register(z.globalRegistry, {
    description: 'Updated text prompt that directs the remix generation',
  }),
  video_id: z.string().register(z.globalRegistry, {
    description:
      'The video_id from a previous Sora 2 generation. Note: You can only remix videos that were generated by Sora (via text-to-video or image-to-video endpoints), not arbitrary uploaded videos.',
  }),
  delete_video: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted.',
      }),
    )
    .default(true),
})

/**
 * ImageFile
 */
export const zImageFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  height: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The height of the image',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  width: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The width of the image',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * VideoFile
 */
export const zFalAiSora2VideoToVideoRemixVideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  duration: z.optional(
    z.number().register(z.globalRegistry, {
      description: 'The duration of the video',
    }),
  ),
  height: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The height of the video',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  fps: z.optional(
    z.number().register(z.globalRegistry, {
      description: 'The FPS of the video',
    }),
  ),
  width: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The width of the video',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  num_frames: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The number of frames in the video',
    }),
  ),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * RemixOutput
 */
export const zSora2VideoToVideoRemixOutput = z.object({
  spritesheet: z.optional(zImageFile),
  thumbnail: z.optional(zImageFile),
  video_id: z.string().register(z.globalRegistry, {
    description: 'The ID of the generated video',
  }),
  video: zFalAiSora2VideoToVideoRemixVideoFile,
})

/**
 * LongWanVACEReframeRequest
 */
export const zWanVaceAppsLongReframeInput = z.object({
  prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description:
          'The text prompt to guide video generation. Optional for reframing.',
      }),
    )
    .default(''),
  video_url: z.string().register(z.globalRegistry, {
    description:
      'URL to the source video file. This video will be used as a reference for the reframe task.',
  }),
  acceleration: z.optional(
    z.enum(['none', 'low', 'regular']).register(z.globalRegistry, {
      description:
        "Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.",
    }),
  ),
  paste_back: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to paste back the reframed scene to the original video.',
      }),
    )
    .default(true),
  zoom_factor: z
    .optional(
      z.number().gte(0).lte(0.9).register(z.globalRegistry, {
        description:
          'Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom.',
      }),
    )
    .default(0),
  shift: z
    .optional(
      z.number().gte(1).lte(15).register(z.globalRegistry, {
        description: 'Shift parameter for video generation.',
      }),
    )
    .default(5),
  scene_threshold: z
    .optional(
      z.number().gte(0).lte(100).register(z.globalRegistry, {
        description:
          'Threshold for scene detection sensitivity (0-100). Lower values detect more scenes.',
      }),
    )
    .default(30),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If set to true, the safety checker will be enabled.',
      }),
    )
    .default(false),
  guidance_scale: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description:
          'Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.',
      }),
    )
    .default(5),
  auto_downsample_min_fps: z
    .optional(
      z.number().gte(1).lte(60).register(z.globalRegistry, {
        description: 'Minimum FPS for auto downsample.',
      }),
    )
    .default(6),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'Negative prompt for video generation.',
      }),
    )
    .default(
      'letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards',
    ),
  sampler: z.optional(
    z.enum(['unipc', 'dpm++', 'euler']).register(z.globalRegistry, {
      description: 'Sampler to use for video generation.',
    }),
  ),
  video_write_mode: z.optional(
    z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
      description: 'The write mode of the generated video.',
    }),
  ),
  resolution: z.optional(
    z
      .enum(['auto', '240p', '360p', '480p', '580p', '720p'])
      .register(z.globalRegistry, {
        description: 'Resolution of the generated video.',
      }),
  ),
  aspect_ratio: z.optional(
    z.enum(['auto', '16:9', '1:1', '9:16']).register(z.globalRegistry, {
      description: 'Aspect ratio of the generated video.',
    }),
  ),
  return_frames_zip: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, also return a ZIP file containing all generated frames.',
      }),
    )
    .default(false),
  trim_borders: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to trim borders from the video.',
      }),
    )
    .default(true),
  transparency_mode: z.optional(
    z.enum(['content_aware', 'white', 'black']).register(z.globalRegistry, {
      description:
        'The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.',
    }),
  ),
  video_quality: z.optional(
    z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
      description: 'The quality of the generated video.',
    }),
  ),
  sync_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
      }),
    )
    .default(false),
  seed: z.optional(
    z.int().register(z.globalRegistry, {
      description:
        'Random seed for reproducibility. If None, a random seed is chosen.',
    }),
  ),
  interpolator_model: z.optional(
    z.enum(['rife', 'film']).register(z.globalRegistry, {
      description:
        "The model to use for frame interpolation. Options are 'rife' or 'film'.",
    }),
  ),
  enable_auto_downsample: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enable auto downsample.',
      }),
    )
    .default(true),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(50).register(z.globalRegistry, {
        description:
          'Number of inference steps for sampling. Higher values give better quality but take longer.',
      }),
    )
    .default(30),
})

/**
 * VideoFile
 */
export const zFalAiWanVaceAppsLongReframeVideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  duration: z.optional(
    z.number().register(z.globalRegistry, {
      description: 'The duration of the video',
    }),
  ),
  height: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The height of the video',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  width: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The width of the video',
    }),
  ),
  fps: z.optional(
    z.number().register(z.globalRegistry, {
      description: 'The FPS of the video',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  num_frames: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The number of frames in the video',
    }),
  ),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * LongWanVACEReframeResponse
 */
export const zWanVaceAppsLongReframeOutput = z.object({
  video: zFalAiWanVaceAppsLongReframeVideoFile,
})

/**
 * InfiniTalkVid2VidAudioRequest
 */
export const zInfinitalkVideoToVideoInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The text prompt to guide video generation.',
  }),
  resolution: z.optional(
    z.enum(['480p', '720p']).register(z.globalRegistry, {
      description:
        'Resolution of the video to generate. Must be either 480p or 720p.',
    }),
  ),
  acceleration: z.optional(
    z.enum(['none', 'regular', 'high']).register(z.globalRegistry, {
      description: 'The acceleration level to use for generation.',
    }),
  ),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the input video.',
  }),
  audio_url: z.string().register(z.globalRegistry, {
    description: 'The URL of the audio file.',
  }),
  num_frames: z
    .optional(
      z.int().gte(41).lte(241).register(z.globalRegistry, {
        description:
          'Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.',
      }),
    )
    .default(145),
  seed: z
    .optional(
      z.int().register(z.globalRegistry, {
        description:
          'Random seed for reproducibility. If None, a random seed is chosen.',
      }),
    )
    .default(42),
})

/**
 * File
 */
export const zFalAiInfinitalkVideoToVideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * InfinitalkVid2VidResponse
 */
export const zInfinitalkVideoToVideoOutput = z.object({
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiInfinitalkVideoToVideoFile,
})

/**
 * SeedVRVideoInput
 */
export const zSeedvrUpscaleVideoInput = z.object({
  video_url: z.string().register(z.globalRegistry, {
    description: 'The input video to be processed',
  }),
  upscale_mode: z.optional(
    z.enum(['target', 'factor']).register(z.globalRegistry, {
      description:
        "The mode to use for the upscale. If 'target', the upscale factor will be calculated based on the target resolution. If 'factor', the upscale factor will be used directly.",
    }),
  ),
  noise_scale: z
    .optional(
      z.number().gte(0).lte(1).register(z.globalRegistry, {
        description: 'The noise scale to use for the generation process.',
      }),
    )
    .default(0.1),
  target_resolution: z.optional(
    z.enum(['720p', '1080p', '1440p', '2160p']).register(z.globalRegistry, {
      description:
        'The target resolution to upscale to when `upscale_mode` is `target`.',
    }),
  ),
  output_write_mode: z.optional(
    z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
      description: 'The write mode of the output video.',
    }),
  ),
  output_format: z.optional(
    z
      .enum(['X264 (.mp4)', 'VP9 (.webm)', 'PRORES4444 (.mov)', 'GIF (.gif)'])
      .register(z.globalRegistry, {
        description: 'The format of the output video.',
      }),
  ),
  output_quality: z.optional(
    z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
      description: 'The quality of the output video.',
    }),
  ),
  sync_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
      }),
    )
    .default(false),
  upscale_factor: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description:
          'Upscaling factor to be used. Will multiply the dimensions with this factor when `upscale_mode` is `factor`.',
      }),
    )
    .default(2),
  seed: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The random seed used for the generation process.',
    }),
  ),
})

/**
 * File
 */
export const zFalAiSeedvrUpscaleVideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * SeedVRVideoOutput
 */
export const zSeedvrUpscaleVideoOutput = z.object({
  seed: z.int().register(z.globalRegistry, {
    description: 'The random seed used for the generation process.',
  }),
  video: zFalAiSeedvrUpscaleVideoFile,
})

/**
 * WanVACEVideoEditRequest
 */
export const zWanVaceAppsVideoEditInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'Prompt to edit the video.',
  }),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the input video.',
  }),
  acceleration: z.optional(
    z.enum(['none', 'low', 'regular']).register(z.globalRegistry, {
      description:
        "Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.",
    }),
  ),
  resolution: z.optional(
    z
      .enum(['auto', '240p', '360p', '480p', '580p', '720p'])
      .register(z.globalRegistry, {
        description: 'Resolution of the edited video.',
      }),
  ),
  aspect_ratio: z.optional(
    z.enum(['auto', '16:9', '9:16', '1:1']).register(z.globalRegistry, {
      description: 'Aspect ratio of the edited video.',
    }),
  ),
  return_frames_zip: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to include a ZIP archive containing all generated frames.',
      }),
    )
    .default(false),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enable the safety checker.',
      }),
    )
    .default(true),
  video_type: z.optional(
    z.enum(['auto', 'general', 'human']).register(z.globalRegistry, {
      description:
        "The type of video you're editing. Use 'general' for most videos, and 'human' for videos emphasizing human subjects and motions. The default value 'auto' means the model will guess based on the first frame of the video.",
    }),
  ),
  image_urls: z
    .optional(
      z.array(z.string()).register(z.globalRegistry, {
        description:
          'URLs of the input images to use as a reference for the generation.',
      }),
    )
    .default([]),
  enable_auto_downsample: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to enable automatic downsampling. If your video has a high frame rate or is long, enabling longer sequences to be generated. The video will be interpolated back to the original frame rate after generation.',
      }),
    )
    .default(true),
  auto_downsample_min_fps: z
    .optional(
      z.number().gte(1).lte(60).register(z.globalRegistry, {
        description:
          'The minimum frames per second to downsample the video to.',
      }),
    )
    .default(15),
})

/**
 * File
 */
export const zFalAiWanVaceAppsVideoEditFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * VideoFile
 */
export const zFalAiWanVaceAppsVideoEditVideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  duration: z.optional(
    z.number().register(z.globalRegistry, {
      description: 'The duration of the video',
    }),
  ),
  height: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The height of the video',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  width: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The width of the video',
    }),
  ),
  fps: z.optional(
    z.number().register(z.globalRegistry, {
      description: 'The FPS of the video',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  num_frames: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The number of frames in the video',
    }),
  ),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * WanVACEVideoEditResponse
 */
export const zWanVaceAppsVideoEditOutput = z.object({
  frames_zip: z.optional(zFalAiWanVaceAppsVideoEditFile),
  video: zFalAiWanVaceAppsVideoEditVideoFile,
})

/**
 * WanAnimateMoveRequest
 */
export const zWanV2214bAnimateReplaceInput = z.object({
  shift: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description: 'Shift value for the video. Must be between 1.0 and 10.0.',
      }),
    )
    .default(5),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the input video.',
  }),
  resolution: z.optional(
    z.enum(['480p', '580p', '720p']).register(z.globalRegistry, {
      description: 'Resolution of the generated video (480p, 580p, or 720p).',
    }),
  ),
  return_frames_zip: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, also return a ZIP archive containing per-frame images generated on GPU (lossless).',
      }),
    )
    .default(false),
  video_write_mode: z.optional(
    z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
      description:
        'The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.',
    }),
  ),
  enable_output_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If set to true, output video will be checked for safety after generation.',
      }),
    )
    .default(false),
  image_url: z.string().register(z.globalRegistry, {
    description:
      'URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.',
  }),
  video_quality: z.optional(
    z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
      description:
        'The quality of the output video. Higher quality means better visual quality but larger file size.',
    }),
  ),
  guidance_scale: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description:
          'Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.',
      }),
    )
    .default(1),
  seed: z.optional(z.union([z.int(), z.unknown()])),
  use_turbo: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized for best results.',
      }),
    )
    .default(false),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(40).register(z.globalRegistry, {
        description:
          'Number of inference steps for sampling. Higher values give better quality but take longer.',
      }),
    )
    .default(20),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If set to true, input data will be checked for safety before processing.',
      }),
    )
    .default(false),
})

/**
 * File
 */
export const zFalAiWanV2214bAnimateReplaceFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * WanAnimateReplaceResponse
 */
export const zWanV2214bAnimateReplaceOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for generation (auto-generated by the model)',
  }),
  frames_zip: z.optional(
    z.union([zFalAiWanV2214bAnimateReplaceFile, z.unknown()]),
  ),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation',
  }),
  video: zFalAiWanV2214bAnimateReplaceFile,
})

/**
 * WanAnimateMoveRequest
 */
export const zWanV2214bAnimateMoveInput = z.object({
  shift: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description: 'Shift value for the video. Must be between 1.0 and 10.0.',
      }),
    )
    .default(5),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the input video.',
  }),
  resolution: z.optional(
    z.enum(['480p', '580p', '720p']).register(z.globalRegistry, {
      description: 'Resolution of the generated video (480p, 580p, or 720p).',
    }),
  ),
  return_frames_zip: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, also return a ZIP archive containing per-frame images generated on GPU (lossless).',
      }),
    )
    .default(false),
  video_write_mode: z.optional(
    z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
      description:
        'The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.',
    }),
  ),
  enable_output_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If set to true, output video will be checked for safety after generation.',
      }),
    )
    .default(false),
  image_url: z.string().register(z.globalRegistry, {
    description:
      'URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.',
  }),
  video_quality: z.optional(
    z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
      description:
        'The quality of the output video. Higher quality means better visual quality but larger file size.',
    }),
  ),
  guidance_scale: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description:
          'Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.',
      }),
    )
    .default(1),
  seed: z.optional(z.union([z.int(), z.unknown()])),
  use_turbo: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized for best results.',
      }),
    )
    .default(false),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(40).register(z.globalRegistry, {
        description:
          'Number of inference steps for sampling. Higher values give better quality but take longer.',
      }),
    )
    .default(20),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If set to true, input data will be checked for safety before processing.',
      }),
    )
    .default(false),
})

/**
 * File
 */
export const zFalAiWanV2214bAnimateMoveFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * WanAnimateMoveResponse
 */
export const zWanV2214bAnimateMoveOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for generation (auto-generated by the model)',
  }),
  frames_zip: z.optional(
    z.union([zFalAiWanV2214bAnimateMoveFile, z.unknown()]),
  ),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation',
  }),
  video: zFalAiWanV2214bAnimateMoveFile,
})

/**
 * LucyEditProInput
 */
export const zLucyEditProInput = z.object({
  sync_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          '\n            If set to true, the function will wait for the video to be generated\n            and uploaded before returning the response. This will increase the\n            latency of the function but it allows you to get the video directly\n            in the response without going through the CDN.\n        ',
      }),
    )
    .default(true),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the video to edit',
  }),
  prompt: z.string().max(1500).register(z.globalRegistry, {
    description: 'Text description of the desired video content',
  }),
  resolution: z.optional(
    z.enum(['720p']).register(z.globalRegistry, {
      description: 'Resolution of the generated video',
    }),
  ),
  enhance_prompt: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enhance the prompt for better results.',
      }),
    )
    .default(true),
})

/**
 * File
 */
export const zDecartLucyEditProFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * LucyEditProOutput
 */
export const zLucyEditProOutput = z.object({
  video: zDecartLucyEditProFile,
})

/**
 * LucyEditDevInput
 */
export const zLucyEditDevInput = z.object({
  sync_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          '\n            If set to true, the function will wait for the video to be generated\n            and uploaded before returning the response. This will increase the\n            latency of the function but it allows you to get the video directly\n            in the response without going through the CDN.\n        ',
      }),
    )
    .default(true),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the video to edit',
  }),
  prompt: z.string().max(1500).register(z.globalRegistry, {
    description: 'Text description of the desired video content',
  }),
  enhance_prompt: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enhance the prompt for better results.',
      }),
    )
    .default(true),
})

/**
 * File
 */
export const zDecartLucyEditDevFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * LucyEditDevOutput
 */
export const zLucyEditDevOutput = z.object({
  video: zDecartLucyEditDevFile,
})

/**
 * WanVACEReframeRequest
 */
export const zWan22VaceFunA14bReframeInput = z.object({
  prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description:
          'The text prompt to guide video generation. Optional for reframing.',
      }),
    )
    .default(''),
  video_url: z.string().register(z.globalRegistry, {
    description:
      'URL to the source video file. This video will be used as a reference for the reframe task.',
  }),
  num_interpolated_frames: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Number of frames to interpolate between the original frames. A value of 0 means no interpolation.',
      }),
    )
    .default(0),
  temporal_downsample_factor: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.',
      }),
    )
    .default(0),
  first_frame_url: z.optional(z.union([z.string(), z.unknown()])),
  guidance_scale: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description:
          'Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.',
      }),
    )
    .default(5),
  num_frames: z
    .optional(
      z.int().gte(17).lte(241).register(z.globalRegistry, {
        description:
          'Number of frames to generate. Must be between 81 to 241 (inclusive).',
      }),
    )
    .default(81),
  auto_downsample_min_fps: z
    .optional(
      z.number().gte(1).lte(60).register(z.globalRegistry, {
        description:
          'The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.',
      }),
    )
    .default(15),
  trim_borders: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to trim borders from the video.',
      }),
    )
    .default(true),
  sampler: z.optional(
    z.enum(['unipc', 'dpm++', 'euler']).register(z.globalRegistry, {
      description: 'Sampler to use for video generation.',
    }),
  ),
  transparency_mode: z.optional(
    z.enum(['content_aware', 'white', 'black']).register(z.globalRegistry, {
      description:
        'The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.',
    }),
  ),
  sync_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
      }),
    )
    .default(false),
  video_quality: z.optional(
    z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
      description: 'The quality of the generated video.',
    }),
  ),
  enable_prompt_expansion: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enable prompt expansion.',
      }),
    )
    .default(false),
  seed: z.optional(z.union([z.int(), z.unknown()])),
  interpolator_model: z.optional(
    z.enum(['rife', 'film']).register(z.globalRegistry, {
      description:
        "The model to use for frame interpolation. Options are 'rife' or 'film'.",
    }),
  ),
  enable_auto_downsample: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.',
      }),
    )
    .default(false),
  shift: z
    .optional(
      z.number().gte(1).lte(15).register(z.globalRegistry, {
        description: 'Shift parameter for video generation.',
      }),
    )
    .default(5),
  acceleration: z.optional(
    z.union([z.enum(['none', 'low', 'regular']), z.unknown()]),
  ),
  zoom_factor: z
    .optional(
      z.number().gte(0).lte(0.9).register(z.globalRegistry, {
        description:
          'Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom.',
      }),
    )
    .default(0),
  frames_per_second: z.optional(z.union([z.int().gte(5).lte(30), z.unknown()])),
  match_input_num_frames: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.',
      }),
    )
    .default(true),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If set to true, the safety checker will be enabled.',
      }),
    )
    .default(false),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'Negative prompt for video generation.',
      }),
    )
    .default(
      'letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards',
    ),
  video_write_mode: z.optional(
    z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
      description: 'The write mode of the generated video.',
    }),
  ),
  resolution: z.optional(
    z
      .enum(['auto', '240p', '360p', '480p', '580p', '720p'])
      .register(z.globalRegistry, {
        description: 'Resolution of the generated video.',
      }),
  ),
  return_frames_zip: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, also return a ZIP file containing all generated frames.',
      }),
    )
    .default(false),
  aspect_ratio: z.optional(
    z.enum(['auto', '16:9', '1:1', '9:16']).register(z.globalRegistry, {
      description: 'Aspect ratio of the generated video.',
    }),
  ),
  match_input_frames_per_second: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.',
      }),
    )
    .default(true),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(50).register(z.globalRegistry, {
        description:
          'Number of inference steps for sampling. Higher values give better quality but take longer.',
      }),
    )
    .default(30),
  last_frame_url: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * File
 */
export const zFalAiWan22VaceFunA14bReframeFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * VideoFile
 */
export const zFalAiWan22VaceFunA14bReframeVideoFile = z.object({
  height: z.optional(z.union([z.int(), z.unknown()])),
  duration: z.optional(z.union([z.number(), z.unknown()])),
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  fps: z.optional(z.union([z.number(), z.unknown()])),
  width: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  num_frames: z.optional(z.union([z.int(), z.unknown()])),
})

/**
 * WanVACEReframeResponse
 */
export const zWan22VaceFunA14bReframeOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for generation.',
  }),
  frames_zip: z.optional(
    z.union([zFalAiWan22VaceFunA14bReframeFile, z.unknown()]),
  ),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiWan22VaceFunA14bReframeVideoFile,
})

/**
 * WanVACEOutpaintingRequest
 */
export const zWan22VaceFunA14bOutpaintingInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The text prompt to guide video generation.',
  }),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL to the source video file. Required for outpainting.',
  }),
  num_interpolated_frames: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Number of frames to interpolate between the original frames. A value of 0 means no interpolation.',
      }),
    )
    .default(0),
  temporal_downsample_factor: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.',
      }),
    )
    .default(0),
  first_frame_url: z.optional(z.union([z.string(), z.unknown()])),
  ref_image_urls: z.optional(
    z.array(z.string()).register(z.globalRegistry, {
      description:
        'URLs to source reference image. If provided, the model will use this image as reference.',
    }),
  ),
  expand_ratio: z
    .optional(
      z.number().gte(0).lte(1).register(z.globalRegistry, {
        description:
          'Amount of expansion. This is a float value between 0 and 1, where 0.25 adds 25% to the original video size on the specified sides.',
      }),
    )
    .default(0.25),
  transparency_mode: z.optional(
    z.enum(['content_aware', 'white', 'black']).register(z.globalRegistry, {
      description:
        'The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.',
    }),
  ),
  num_frames: z
    .optional(
      z.int().gte(17).lte(241).register(z.globalRegistry, {
        description:
          'Number of frames to generate. Must be between 81 to 241 (inclusive).',
      }),
    )
    .default(81),
  auto_downsample_min_fps: z
    .optional(
      z.number().gte(1).lte(60).register(z.globalRegistry, {
        description:
          'The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.',
      }),
    )
    .default(15),
  guidance_scale: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description:
          'Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.',
      }),
    )
    .default(5),
  sampler: z.optional(
    z.enum(['unipc', 'dpm++', 'euler']).register(z.globalRegistry, {
      description: 'Sampler to use for video generation.',
    }),
  ),
  expand_bottom: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to expand the video to the bottom.',
      }),
    )
    .default(false),
  sync_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
      }),
    )
    .default(false),
  video_quality: z.optional(
    z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
      description: 'The quality of the generated video.',
    }),
  ),
  enable_prompt_expansion: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enable prompt expansion.',
      }),
    )
    .default(false),
  expand_left: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to expand the video to the left.',
      }),
    )
    .default(false),
  interpolator_model: z.optional(
    z.enum(['rife', 'film']).register(z.globalRegistry, {
      description:
        "The model to use for frame interpolation. Options are 'rife' or 'film'.",
    }),
  ),
  enable_auto_downsample: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.',
      }),
    )
    .default(false),
  expand_top: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to expand the video to the top.',
      }),
    )
    .default(false),
  shift: z
    .optional(
      z.number().gte(1).lte(15).register(z.globalRegistry, {
        description: 'Shift parameter for video generation.',
      }),
    )
    .default(5),
  seed: z.optional(z.union([z.int(), z.unknown()])),
  acceleration: z.optional(
    z.union([z.enum(['none', 'low', 'regular']), z.unknown()]),
  ),
  frames_per_second: z.optional(z.union([z.int().gte(5).lte(30), z.unknown()])),
  match_input_num_frames: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.',
      }),
    )
    .default(false),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If set to true, the safety checker will be enabled.',
      }),
    )
    .default(false),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'Negative prompt for video generation.',
      }),
    )
    .default(
      'letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards',
    ),
  video_write_mode: z.optional(
    z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
      description: 'The write mode of the generated video.',
    }),
  ),
  expand_right: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to expand the video to the right.',
      }),
    )
    .default(false),
  resolution: z.optional(
    z
      .enum(['auto', '240p', '360p', '480p', '580p', '720p'])
      .register(z.globalRegistry, {
        description: 'Resolution of the generated video.',
      }),
  ),
  return_frames_zip: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, also return a ZIP file containing all generated frames.',
      }),
    )
    .default(false),
  aspect_ratio: z.optional(
    z.enum(['auto', '16:9', '1:1', '9:16']).register(z.globalRegistry, {
      description: 'Aspect ratio of the generated video.',
    }),
  ),
  match_input_frames_per_second: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.',
      }),
    )
    .default(false),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(50).register(z.globalRegistry, {
        description:
          'Number of inference steps for sampling. Higher values give better quality but take longer.',
      }),
    )
    .default(30),
  last_frame_url: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * File
 */
export const zFalAiWan22VaceFunA14bOutpaintingFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * VideoFile
 */
export const zFalAiWan22VaceFunA14bOutpaintingVideoFile = z.object({
  height: z.optional(z.union([z.int(), z.unknown()])),
  duration: z.optional(z.union([z.number(), z.unknown()])),
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  fps: z.optional(z.union([z.number(), z.unknown()])),
  width: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  num_frames: z.optional(z.union([z.int(), z.unknown()])),
})

/**
 * WanVACEOutpaintingResponse
 */
export const zWan22VaceFunA14bOutpaintingOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for generation.',
  }),
  frames_zip: z.optional(
    z.union([zFalAiWan22VaceFunA14bOutpaintingFile, z.unknown()]),
  ),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiWan22VaceFunA14bOutpaintingVideoFile,
})

/**
 * WanVACEInpaintingRequest
 */
export const zWan22VaceFunA14bInpaintingInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The text prompt to guide video generation.',
  }),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL to the source video file. Required for inpainting.',
  }),
  num_interpolated_frames: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Number of frames to interpolate between the original frames. A value of 0 means no interpolation.',
      }),
    )
    .default(0),
  temporal_downsample_factor: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.',
      }),
    )
    .default(0),
  first_frame_url: z.optional(z.union([z.string(), z.unknown()])),
  ref_image_urls: z.optional(
    z.array(z.string()).register(z.globalRegistry, {
      description:
        'Urls to source reference image. If provided, the model will use this image as reference.',
    }),
  ),
  guidance_scale: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description:
          'Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.',
      }),
    )
    .default(5),
  num_frames: z
    .optional(
      z.int().gte(17).lte(241).register(z.globalRegistry, {
        description:
          'Number of frames to generate. Must be between 81 to 241 (inclusive).',
      }),
    )
    .default(81),
  auto_downsample_min_fps: z
    .optional(
      z.number().gte(1).lte(60).register(z.globalRegistry, {
        description:
          'The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.',
      }),
    )
    .default(15),
  transparency_mode: z.optional(
    z.enum(['content_aware', 'white', 'black']).register(z.globalRegistry, {
      description:
        'The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.',
    }),
  ),
  sampler: z.optional(
    z.enum(['unipc', 'dpm++', 'euler']).register(z.globalRegistry, {
      description: 'Sampler to use for video generation.',
    }),
  ),
  sync_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
      }),
    )
    .default(false),
  video_quality: z.optional(
    z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
      description: 'The quality of the generated video.',
    }),
  ),
  mask_video_url: z.union([z.string(), z.unknown()]),
  seed: z.optional(z.union([z.int(), z.unknown()])),
  interpolator_model: z.optional(
    z.enum(['rife', 'film']).register(z.globalRegistry, {
      description:
        "The model to use for frame interpolation. Options are 'rife' or 'film'.",
    }),
  ),
  enable_auto_downsample: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.',
      }),
    )
    .default(false),
  enable_prompt_expansion: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enable prompt expansion.',
      }),
    )
    .default(false),
  shift: z
    .optional(
      z.number().gte(1).lte(15).register(z.globalRegistry, {
        description: 'Shift parameter for video generation.',
      }),
    )
    .default(5),
  preprocess: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to preprocess the input video.',
      }),
    )
    .default(false),
  acceleration: z.optional(
    z.union([z.enum(['none', 'low', 'regular']), z.unknown()]),
  ),
  mask_image_url: z.optional(z.union([z.string(), z.unknown()])),
  frames_per_second: z.optional(z.union([z.int().gte(5).lte(30), z.unknown()])),
  match_input_num_frames: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.',
      }),
    )
    .default(false),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If set to true, the safety checker will be enabled.',
      }),
    )
    .default(false),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'Negative prompt for video generation.',
      }),
    )
    .default(
      'letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards',
    ),
  video_write_mode: z.optional(
    z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
      description: 'The write mode of the generated video.',
    }),
  ),
  resolution: z.optional(
    z
      .enum(['auto', '240p', '360p', '480p', '580p', '720p'])
      .register(z.globalRegistry, {
        description: 'Resolution of the generated video.',
      }),
  ),
  return_frames_zip: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, also return a ZIP file containing all generated frames.',
      }),
    )
    .default(false),
  aspect_ratio: z.optional(
    z.enum(['auto', '16:9', '1:1', '9:16']).register(z.globalRegistry, {
      description: 'Aspect ratio of the generated video.',
    }),
  ),
  match_input_frames_per_second: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.',
      }),
    )
    .default(false),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(50).register(z.globalRegistry, {
        description:
          'Number of inference steps for sampling. Higher values give better quality but take longer.',
      }),
    )
    .default(30),
  last_frame_url: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * File
 */
export const zFalAiWan22VaceFunA14bInpaintingFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * VideoFile
 */
export const zFalAiWan22VaceFunA14bInpaintingVideoFile = z.object({
  height: z.optional(z.union([z.int(), z.unknown()])),
  duration: z.optional(z.union([z.number(), z.unknown()])),
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  fps: z.optional(z.union([z.number(), z.unknown()])),
  width: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  num_frames: z.optional(z.union([z.int(), z.unknown()])),
})

/**
 * WanVACEInpaintingResponse
 */
export const zWan22VaceFunA14bInpaintingOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for generation.',
  }),
  frames_zip: z.optional(
    z.union([zFalAiWan22VaceFunA14bInpaintingFile, z.unknown()]),
  ),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiWan22VaceFunA14bInpaintingVideoFile,
})

/**
 * WanVACEDepthRequest
 */
export const zWan22VaceFunA14bDepthInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The text prompt to guide video generation.',
  }),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL to the source video file. Required for depth task.',
  }),
  num_interpolated_frames: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Number of frames to interpolate between the original frames. A value of 0 means no interpolation.',
      }),
    )
    .default(0),
  temporal_downsample_factor: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.',
      }),
    )
    .default(0),
  first_frame_url: z.optional(z.union([z.string(), z.unknown()])),
  ref_image_urls: z.optional(
    z.array(z.string()).register(z.globalRegistry, {
      description:
        'URLs to source reference image. If provided, the model will use this image as reference.',
    }),
  ),
  guidance_scale: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description:
          'Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.',
      }),
    )
    .default(5),
  num_frames: z
    .optional(
      z.int().gte(17).lte(241).register(z.globalRegistry, {
        description:
          'Number of frames to generate. Must be between 81 to 241 (inclusive).',
      }),
    )
    .default(81),
  auto_downsample_min_fps: z
    .optional(
      z.number().gte(1).lte(60).register(z.globalRegistry, {
        description:
          'The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.',
      }),
    )
    .default(15),
  transparency_mode: z.optional(
    z.enum(['content_aware', 'white', 'black']).register(z.globalRegistry, {
      description:
        'The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.',
    }),
  ),
  sampler: z.optional(
    z.enum(['unipc', 'dpm++', 'euler']).register(z.globalRegistry, {
      description: 'Sampler to use for video generation.',
    }),
  ),
  sync_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
      }),
    )
    .default(false),
  video_quality: z.optional(
    z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
      description: 'The quality of the generated video.',
    }),
  ),
  enable_prompt_expansion: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enable prompt expansion.',
      }),
    )
    .default(false),
  seed: z.optional(z.union([z.int(), z.unknown()])),
  interpolator_model: z.optional(
    z.enum(['rife', 'film']).register(z.globalRegistry, {
      description:
        "The model to use for frame interpolation. Options are 'rife' or 'film'.",
    }),
  ),
  enable_auto_downsample: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.',
      }),
    )
    .default(false),
  preprocess: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to preprocess the input video.',
      }),
    )
    .default(false),
  shift: z
    .optional(
      z.number().gte(1).lte(15).register(z.globalRegistry, {
        description: 'Shift parameter for video generation.',
      }),
    )
    .default(5),
  acceleration: z.optional(
    z.union([z.enum(['none', 'low', 'regular']), z.unknown()]),
  ),
  frames_per_second: z.optional(z.union([z.int().gte(5).lte(30), z.unknown()])),
  match_input_num_frames: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.',
      }),
    )
    .default(false),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If set to true, the safety checker will be enabled.',
      }),
    )
    .default(false),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'Negative prompt for video generation.',
      }),
    )
    .default(
      'letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards',
    ),
  video_write_mode: z.optional(
    z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
      description: 'The write mode of the generated video.',
    }),
  ),
  resolution: z.optional(
    z
      .enum(['auto', '240p', '360p', '480p', '580p', '720p'])
      .register(z.globalRegistry, {
        description: 'Resolution of the generated video.',
      }),
  ),
  return_frames_zip: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, also return a ZIP file containing all generated frames.',
      }),
    )
    .default(false),
  aspect_ratio: z.optional(
    z.enum(['auto', '16:9', '1:1', '9:16']).register(z.globalRegistry, {
      description: 'Aspect ratio of the generated video.',
    }),
  ),
  match_input_frames_per_second: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.',
      }),
    )
    .default(false),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(50).register(z.globalRegistry, {
        description:
          'Number of inference steps for sampling. Higher values give better quality but take longer.',
      }),
    )
    .default(30),
  last_frame_url: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * File
 */
export const zFalAiWan22VaceFunA14bDepthFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * VideoFile
 */
export const zFalAiWan22VaceFunA14bDepthVideoFile = z.object({
  height: z.optional(z.union([z.int(), z.unknown()])),
  duration: z.optional(z.union([z.number(), z.unknown()])),
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  fps: z.optional(z.union([z.number(), z.unknown()])),
  width: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  num_frames: z.optional(z.union([z.int(), z.unknown()])),
})

/**
 * WanVACEDepthResponse
 */
export const zWan22VaceFunA14bDepthOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for generation.',
  }),
  frames_zip: z.optional(
    z.union([zFalAiWan22VaceFunA14bDepthFile, z.unknown()]),
  ),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiWan22VaceFunA14bDepthVideoFile,
})

/**
 * WanVACEPoseRequest
 */
export const zWan22VaceFunA14bPoseInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description:
      'The text prompt to guide video generation. For pose task, the prompt should describe the desired pose and action of the subject in the video.',
  }),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL to the source video file. Required for pose task.',
  }),
  num_interpolated_frames: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Number of frames to interpolate between the original frames. A value of 0 means no interpolation.',
      }),
    )
    .default(0),
  temporal_downsample_factor: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.',
      }),
    )
    .default(0),
  first_frame_url: z.optional(z.union([z.string(), z.unknown()])),
  ref_image_urls: z.optional(
    z.array(z.string()).register(z.globalRegistry, {
      description:
        'URLs to source reference image. If provided, the model will use this image as reference.',
    }),
  ),
  guidance_scale: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description:
          'Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.',
      }),
    )
    .default(5),
  num_frames: z
    .optional(
      z.int().gte(17).lte(241).register(z.globalRegistry, {
        description:
          'Number of frames to generate. Must be between 81 to 241 (inclusive).',
      }),
    )
    .default(81),
  auto_downsample_min_fps: z
    .optional(
      z.number().gte(1).lte(60).register(z.globalRegistry, {
        description:
          'The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.',
      }),
    )
    .default(15),
  transparency_mode: z.optional(
    z.enum(['content_aware', 'white', 'black']).register(z.globalRegistry, {
      description:
        'The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.',
    }),
  ),
  sampler: z.optional(
    z.enum(['unipc', 'dpm++', 'euler']).register(z.globalRegistry, {
      description: 'Sampler to use for video generation.',
    }),
  ),
  sync_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
      }),
    )
    .default(false),
  video_quality: z.optional(
    z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
      description: 'The quality of the generated video.',
    }),
  ),
  enable_prompt_expansion: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enable prompt expansion.',
      }),
    )
    .default(false),
  seed: z.optional(z.union([z.int(), z.unknown()])),
  interpolator_model: z.optional(
    z.enum(['rife', 'film']).register(z.globalRegistry, {
      description:
        "The model to use for frame interpolation. Options are 'rife' or 'film'.",
    }),
  ),
  enable_auto_downsample: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.',
      }),
    )
    .default(false),
  preprocess: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to preprocess the input video.',
      }),
    )
    .default(false),
  shift: z
    .optional(
      z.number().gte(1).lte(15).register(z.globalRegistry, {
        description: 'Shift parameter for video generation.',
      }),
    )
    .default(5),
  acceleration: z.optional(
    z.union([z.enum(['none', 'low', 'regular']), z.unknown()]),
  ),
  frames_per_second: z.optional(z.union([z.int().gte(5).lte(30), z.unknown()])),
  match_input_num_frames: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.',
      }),
    )
    .default(false),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If set to true, the safety checker will be enabled.',
      }),
    )
    .default(false),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'Negative prompt for video generation.',
      }),
    )
    .default(
      'letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards',
    ),
  video_write_mode: z.optional(
    z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
      description: 'The write mode of the generated video.',
    }),
  ),
  resolution: z.optional(
    z
      .enum(['auto', '240p', '360p', '480p', '580p', '720p'])
      .register(z.globalRegistry, {
        description: 'Resolution of the generated video.',
      }),
  ),
  return_frames_zip: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, also return a ZIP file containing all generated frames.',
      }),
    )
    .default(false),
  aspect_ratio: z.optional(
    z.enum(['auto', '16:9', '1:1', '9:16']).register(z.globalRegistry, {
      description: 'Aspect ratio of the generated video.',
    }),
  ),
  match_input_frames_per_second: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.',
      }),
    )
    .default(false),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(50).register(z.globalRegistry, {
        description:
          'Number of inference steps for sampling. Higher values give better quality but take longer.',
      }),
    )
    .default(30),
  last_frame_url: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * File
 */
export const zFalAiWan22VaceFunA14bPoseFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * VideoFile
 */
export const zFalAiWan22VaceFunA14bPoseVideoFile = z.object({
  height: z.optional(z.union([z.int(), z.unknown()])),
  duration: z.optional(z.union([z.number(), z.unknown()])),
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  fps: z.optional(z.union([z.number(), z.unknown()])),
  width: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  num_frames: z.optional(z.union([z.int(), z.unknown()])),
})

/**
 * WanVACEPoseResponse
 */
export const zWan22VaceFunA14bPoseOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for generation.',
  }),
  frames_zip: z.optional(
    z.union([zFalAiWan22VaceFunA14bPoseFile, z.unknown()]),
  ),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiWan22VaceFunA14bPoseVideoFile,
})

/**
 * HunyuanFoleyRequest
 */
export const zHunyuanVideoFoleyInput = z.object({
  video_url: z.string().register(z.globalRegistry, {
    description: 'The URL of the video to generate audio for.',
  }),
  guidance_scale: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description: 'Guidance scale for audio generation.',
      }),
    )
    .default(4.5),
  num_inference_steps: z
    .optional(
      z.int().gte(10).lte(100).register(z.globalRegistry, {
        description: 'Number of inference steps for generation.',
      }),
    )
    .default(50),
  seed: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'Random seed for reproducible generation.',
    }),
  ),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'Negative prompt to avoid certain audio characteristics.',
      }),
    )
    .default('noisy, harsh'),
  text_prompt: z.string().register(z.globalRegistry, {
    description: 'Text description of the desired audio (optional).',
  }),
})

/**
 * File
 */
export const zFalAiHunyuanVideoFoleyFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * HunyuanFoleyResponse
 */
export const zHunyuanVideoFoleyOutput = z.object({
  video: zFalAiHunyuanVideoFoleyFile,
})

/**
 * LipSyncV2ProInput
 */
export const zSyncLipsyncV2ProInput = z.object({
  sync_mode: z.optional(
    z
      .enum(['cut_off', 'loop', 'bounce', 'silence', 'remap'])
      .register(z.globalRegistry, {
        description:
          'Lipsync mode when audio and video durations are out of sync.',
      }),
  ),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the input video',
  }),
  audio_url: z.string().register(z.globalRegistry, {
    description: 'URL of the input audio',
  }),
})

/**
 * File
 */
export const zFalAiSyncLipsyncV2ProFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * LipSyncV2ProOutput
 */
export const zSyncLipsyncV2ProOutput = z.object({
  video: zFalAiSyncLipsyncV2ProFile,
})

/**
 * WanFunControlRequest
 */
export const zWanFunControlInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt to generate the video.',
  }),
  shift: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description: 'The shift for the scheduler.',
      }),
    )
    .default(5),
  preprocess_video: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to preprocess the video. If True, the video will be preprocessed to depth or pose.',
      }),
    )
    .default(false),
  reference_image_url: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The URL of the reference image to use as a reference for the video generation.',
    }),
  ),
  fps: z
    .optional(
      z.int().gte(4).lte(60).register(z.globalRegistry, {
        description:
          'The fps to generate. Only used when match_input_fps is False.',
      }),
    )
    .default(16),
  match_input_num_frames: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to match the number of frames in the input video.',
      }),
    )
    .default(true),
  guidance_scale: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description: 'The guidance scale.',
      }),
    )
    .default(6),
  preprocess_type: z.optional(
    z.enum(['depth', 'pose']).register(z.globalRegistry, {
      description:
        'The type of preprocess to apply to the video. Only used when preprocess_video is True.',
    }),
  ),
  control_video_url: z.string().register(z.globalRegistry, {
    description:
      'The URL of the control video to use as a reference for the video generation.',
  }),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'The negative prompt to generate the video.',
      }),
    )
    .default(''),
  num_frames: z
    .optional(
      z.int().gte(49).lte(121).register(z.globalRegistry, {
        description:
          'The number of frames to generate. Only used when match_input_num_frames is False.',
      }),
    )
    .default(81),
  seed: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The seed for the random number generator.',
    }),
  ),
  num_inference_steps: z
    .optional(
      z.int().gte(4).lte(50).register(z.globalRegistry, {
        description: 'The number of inference steps.',
      }),
    )
    .default(27),
  match_input_fps: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to match the fps in the input video.',
      }),
    )
    .default(true),
})

/**
 * File
 */
export const zFalAiWanFunControlFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * WanFunControlResponse
 */
export const zWanFunControlOutput = z.object({
  video: zFalAiWanFunControlFile,
})

/**
 * InputIncreaseResolutionModel
 */
export const zVideoIncreaseResolutionInput = z.object({
  video_url: z.string().register(z.globalRegistry, {
    description:
      'Input video to increase resolution. Size should be less than 14142x14142 and duration less than 30s.',
  }),
  output_container_and_codec: z.optional(
    z
      .enum([
        'mp4_h265',
        'mp4_h264',
        'webm_vp9',
        'mov_h265',
        'mov_proresks',
        'mkv_h265',
        'mkv_h264',
        'mkv_vp9',
        'gif',
      ])
      .register(z.globalRegistry, {
        description:
          'Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, mov_h265, mov_proresks, mkv_h265, mkv_h264, mkv_vp9, gif.',
      }),
  ),
  desired_increase: z.optional(
    z.enum(['2', '4']).register(z.globalRegistry, {
      description: 'desired_increase factor. Options: 2x, 4x.',
    }),
  ),
})

/**
 * Video
 */
export const zBriaVideoIncreaseResolutionVideo = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * File
 */
export const zBriaVideoIncreaseResolutionFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * OutputIncreaseResolutionModel
 */
export const zVideoIncreaseResolutionOutput = z.object({
  video: z.union([
    zBriaVideoIncreaseResolutionVideo,
    zBriaVideoIncreaseResolutionFile,
  ]),
})

/**
 * InfiniTalkSingleAudioRequest
 */
export const zInfinitalkInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The text prompt to guide video generation.',
  }),
  resolution: z.optional(
    z.enum(['480p', '720p']).register(z.globalRegistry, {
      description:
        'Resolution of the video to generate. Must be either 480p or 720p.',
    }),
  ),
  acceleration: z.optional(
    z.enum(['none', 'regular', 'high']).register(z.globalRegistry, {
      description: 'The acceleration level to use for generation.',
    }),
  ),
  image_url: z.string().register(z.globalRegistry, {
    description:
      'URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.',
  }),
  audio_url: z.string().register(z.globalRegistry, {
    description: 'The URL of the audio file.',
  }),
  num_frames: z
    .optional(
      z.int().gte(41).lte(721).register(z.globalRegistry, {
        description: 'Number of frames to generate. Must be between 41 to 721.',
      }),
    )
    .default(145),
  seed: z
    .optional(
      z.int().register(z.globalRegistry, {
        description:
          'Random seed for reproducibility. If None, a random seed is chosen.',
      }),
    )
    .default(42),
})

/**
 * File
 */
export const zFalAiInfinitalkFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * AvatarSingleAudioResponse
 */
export const zInfinitalkOutput = z.object({
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiInfinitalkFile,
})

/**
 * Input
 */
export const zSfxV1VideoToVideoInput = z.object({
  num_samples: z.optional(z.union([z.int().gte(2).lte(8), z.unknown()])),
  video_url: z.url().min(1).max(2083).register(z.globalRegistry, {
    description:
      'A video url that can accessed from the API to process and add sound effects',
  }),
  duration: z.optional(z.union([z.number().gte(1).lte(10), z.unknown()])),
  seed: z.optional(z.union([z.int().gte(1), z.unknown()])),
  text_prompt: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * Video
 */
export const zMireloAiSfxV1VideoToVideoVideo = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * VideoOutput
 */
export const zSfxV1VideoToVideoOutput = z.object({
  video: z.array(zMireloAiSfxV1VideoToVideoVideo).register(z.globalRegistry, {
    description: 'The processed video with sound effects',
  }),
})

/**
 * MareyInputPoseTransfer
 */
export const zMareyPoseTransferInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt to generate a video from',
  }),
  video_url: z.string().register(z.globalRegistry, {
    description: 'The URL of the video to use as the control video.',
  }),
  seed: z.optional(z.union([z.int(), z.unknown()])),
  reference_image_url: z.optional(z.union([z.string(), z.unknown()])),
  negative_prompt: z.optional(z.union([z.string(), z.unknown()])),
  first_frame_image_url: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * File
 */
export const zMoonvalleyMareyPoseTransferFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * MareyOutput
 */
export const zMareyPoseTransferOutput = z.object({
  video: zMoonvalleyMareyPoseTransferFile,
})

/**
 * MareyInputMotionTransfer
 */
export const zMareyMotionTransferInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt to generate a video from',
  }),
  video_url: z.string().register(z.globalRegistry, {
    description: 'The URL of the video to use as the control video.',
  }),
  seed: z.optional(z.union([z.int(), z.unknown()])),
  reference_image_url: z.optional(z.union([z.string(), z.unknown()])),
  negative_prompt: z.optional(z.union([z.string(), z.unknown()])),
  first_frame_image_url: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * File
 */
export const zMoonvalleyMareyMotionTransferFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * MareyOutput
 */
export const zMareyMotionTransferOutput = z.object({
  video: zMoonvalleyMareyMotionTransferFile,
})

/**
 * ImageSize
 */
export const zFalAiFfmpegApiMergeVideosImageSize = z.object({
  height: z
    .optional(
      z.int().lte(14142).register(z.globalRegistry, {
        description: 'The height of the generated image.',
      }),
    )
    .default(512),
  width: z
    .optional(
      z.int().lte(14142).register(z.globalRegistry, {
        description: 'The width of the generated image.',
      }),
    )
    .default(512),
})

/**
 * MergeVideosInput
 */
export const zFfmpegApiMergeVideosInput = z.object({
  target_fps: z.optional(z.union([z.number().gte(1).lte(60), z.unknown()])),
  video_urls: z.array(z.string()).min(2).register(z.globalRegistry, {
    description: 'List of video URLs to merge in order',
  }),
  resolution: z.optional(
    z.union([
      zFalAiFfmpegApiMergeVideosImageSize,
      z.enum([
        'square_hd',
        'square',
        'portrait_4_3',
        'portrait_16_9',
        'landscape_4_3',
        'landscape_16_9',
      ]),
      z.unknown(),
    ]),
  ),
})

/**
 * File
 */
export const zFalAiFfmpegApiMergeVideosFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * MergeVideosOutput
 */
export const zFfmpegApiMergeVideosOutput = z.object({
  metadata: z.record(z.string(), z.unknown()).register(z.globalRegistry, {
    description:
      'Metadata about the merged video including original video info',
  }),
  video: zFalAiFfmpegApiMergeVideosFile,
})

/**
 * WanV2VRequest
 */
export const zWanV22A14bVideoToVideoInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The text prompt to guide video generation.',
  }),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the input video.',
  }),
  acceleration: z.optional(
    z.enum(['none', 'regular']).register(z.globalRegistry, {
      description:
        "Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.",
    }),
  ),
  num_interpolated_frames: z
    .optional(
      z.int().gte(0).lte(4).register(z.globalRegistry, {
        description:
          'Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.',
      }),
    )
    .default(1),
  shift: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description: 'Shift value for the video. Must be between 1.0 and 10.0.',
      }),
    )
    .default(5),
  resample_fps: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the video will be resampled to the passed frames per second. If false, the video will not be resampled.',
      }),
    )
    .default(false),
  frames_per_second: z.optional(z.union([z.int().gte(4).lte(60), z.unknown()])),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If set to true, input data will be checked for safety before processing.',
      }),
    )
    .default(false),
  num_frames: z
    .optional(
      z.int().gte(17).lte(161).register(z.globalRegistry, {
        description:
          'Number of frames to generate. Must be between 17 to 161 (inclusive).',
      }),
    )
    .default(81),
  guidance_scale: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description:
          'Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.',
      }),
    )
    .default(3.5),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'Negative prompt for video generation.',
      }),
    )
    .default(''),
  video_write_mode: z.optional(
    z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
      description:
        'The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.',
    }),
  ),
  resolution: z.optional(
    z.enum(['480p', '580p', '720p']).register(z.globalRegistry, {
      description: 'Resolution of the generated video (480p, 580p, or 720p).',
    }),
  ),
  aspect_ratio: z.optional(
    z.enum(['auto', '16:9', '9:16', '1:1']).register(z.globalRegistry, {
      description:
        "Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input video.",
    }),
  ),
  enable_output_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If set to true, output video will be checked for safety after generation.',
      }),
    )
    .default(false),
  guidance_scale_2: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description:
          'Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model.',
      }),
    )
    .default(4),
  strength: z
    .optional(
      z.number().gte(0).lte(1).register(z.globalRegistry, {
        description:
          'Strength of the video transformation. A value of 1.0 means the output will be completely based on the prompt, while a value of 0.0 means the output will be identical to the input video.',
      }),
    )
    .default(0.9),
  video_quality: z.optional(
    z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
      description:
        'The quality of the output video. Higher quality means better visual quality but larger file size.',
    }),
  ),
  enable_prompt_expansion: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.',
      }),
    )
    .default(false),
  seed: z.optional(z.union([z.int(), z.unknown()])),
  interpolator_model: z.optional(
    z.enum(['none', 'film', 'rife']).register(z.globalRegistry, {
      description:
        'The model to use for frame interpolation. If None, no interpolation is applied.',
    }),
  ),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(40).register(z.globalRegistry, {
        description:
          'Number of inference steps for sampling. Higher values give better quality but take longer.',
      }),
    )
    .default(27),
  adjust_fps_for_interpolation: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.',
      }),
    )
    .default(true),
})

/**
 * File
 */
export const zFalAiWanV22A14bVideoToVideoFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * WanV2VResponse
 */
export const zWanV22A14bVideoToVideoOutput = z.object({
  prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'The text prompt used for video generation.',
      }),
    )
    .default(''),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiWanV22A14bVideoToVideoFile,
})

/**
 * ExtendVideoConditioningInput
 */
export const zExtendVideoConditioningInput = z.object({
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of video to use as conditioning',
  }),
  start_frame_num: z
    .optional(
      z.int().gte(0).lte(1440).register(z.globalRegistry, {
        description:
          'Frame number of the video from which the conditioning starts. Must be a multiple of 8.',
      }),
    )
    .default(0),
  reverse_video: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to reverse the video. This is useful for tasks where the video conditioning should be applied in reverse order.',
      }),
    )
    .default(false),
  limit_num_frames: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to limit the number of frames used from the video. If True, the `max_num_frames` parameter will be used to limit the number of frames.',
      }),
    )
    .default(false),
  resample_fps: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to resample the video to a specific FPS. If True, the `target_fps` parameter will be used to resample the video.',
      }),
    )
    .default(false),
  strength: z
    .optional(
      z.number().gte(0).lte(1).register(z.globalRegistry, {
        description:
          'Strength of the conditioning. 0.0 means no conditioning, 1.0 means full conditioning.',
      }),
    )
    .default(1),
  target_fps: z
    .optional(
      z.int().gte(1).lte(60).register(z.globalRegistry, {
        description:
          'Target FPS to resample the video to. Only relevant if `resample_fps` is True.',
      }),
    )
    .default(24),
  max_num_frames: z
    .optional(
      z.int().gte(1).lte(1441).register(z.globalRegistry, {
        description:
          'Maximum number of frames to use from the video. If None, all frames will be used.',
      }),
    )
    .default(1441),
})

/**
 * File
 */
export const zFalAiLtxv13B098DistilledExtendFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * ExtendVideoOutput
 */
export const zLtxv13B098DistilledExtendOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for generation.',
  }),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiLtxv13B098DistilledExtendFile,
})

/**
 * RIFEVideoInput
 */
export const zRifeVideoInput = z.object({
  video_url: z.string().register(z.globalRegistry, {
    description: 'The URL of the video to use for interpolation.',
  }),
  use_scene_detection: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If True, the input video will be split into scenes before interpolation. This removes smear frames between scenes, but can result in false positives if the scene detection is not accurate. If False, the entire video will be treated as a single scene.',
      }),
    )
    .default(false),
  loop: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If True, the final frame will be looped back to the first frame to create a seamless loop. If False, the final frame will not loop back.',
      }),
    )
    .default(false),
  num_frames: z
    .optional(
      z.int().gte(1).lte(4).register(z.globalRegistry, {
        description:
          'The number of frames to generate between the input video frames.',
      }),
    )
    .default(1),
  use_calculated_fps: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If True, the function will use the calculated FPS of the input video multiplied by the number of frames to determine the output FPS. If False, the passed FPS will be used.',
      }),
    )
    .default(true),
  fps: z
    .optional(
      z.int().gte(1).lte(60).register(z.globalRegistry, {
        description:
          'Frames per second for the output video. Only applicable if use_calculated_fps is False.',
      }),
    )
    .default(8),
})

/**
 * File
 */
export const zFalAiRifeVideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * RIFEVideoOutput
 */
export const zRifeVideoOutput = z.object({
  video: zFalAiRifeVideoFile,
})

/**
 * FILMVideoInput
 */
export const zFilmVideoInput = z.object({
  video_write_mode: z.optional(
    z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
      description:
        "The write mode of the output video. Only applicable if output_type is 'video'.",
    }),
  ),
  video_url: z.string().register(z.globalRegistry, {
    description: 'The URL of the video to use for interpolation.',
  }),
  use_calculated_fps: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If True, the function will use the calculated FPS of the input video multiplied by the number of frames to determine the output FPS. If False, the passed FPS will be used.',
      }),
    )
    .default(true),
  loop: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If True, the final frame will be looped back to the first frame to create a seamless loop. If False, the final frame will not loop back.',
      }),
    )
    .default(false),
  fps: z
    .optional(
      z.int().gte(1).lte(60).register(z.globalRegistry, {
        description:
          'Frames per second for the output video. Only applicable if use_calculated_fps is False.',
      }),
    )
    .default(8),
  sync_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
      }),
    )
    .default(false),
  video_quality: z.optional(
    z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
      description:
        "The quality of the output video. Only applicable if output_type is 'video'.",
    }),
  ),
  use_scene_detection: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If True, the input video will be split into scenes before interpolation. This removes smear frames between scenes, but can result in false positives if the scene detection is not accurate. If False, the entire video will be treated as a single scene.',
      }),
    )
    .default(false),
  num_frames: z
    .optional(
      z.int().gte(1).lte(4).register(z.globalRegistry, {
        description:
          'The number of frames to generate between the input video frames.',
      }),
    )
    .default(1),
})

/**
 * VideoFile
 */
export const zFalAiFilmVideoVideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  duration: z.optional(
    z.number().register(z.globalRegistry, {
      description: 'The duration of the video',
    }),
  ),
  height: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The height of the video',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  width: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The width of the video',
    }),
  ),
  fps: z.optional(
    z.number().register(z.globalRegistry, {
      description: 'The FPS of the video',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  num_frames: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The number of frames in the video',
    }),
  ),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * FILMVideoOutput
 */
export const zFilmVideoOutput = z.object({
  video: zFalAiFilmVideoVideoFile,
})

/**
 * ModifyVideoRequest
 */
export const zLumaDreamMachineRay2FlashModifyInput = z.object({
  prompt: z.optional(
    z.string().min(3).max(5000).register(z.globalRegistry, {
      description: 'Instruction for modifying the video',
    }),
  ),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the input video to modify',
  }),
  mode: z.optional(
    z
      .enum([
        'adhere_1',
        'adhere_2',
        'adhere_3',
        'flex_1',
        'flex_2',
        'flex_3',
        'reimagine_1',
        'reimagine_2',
        'reimagine_3',
      ])
      .register(z.globalRegistry, {
        description:
          'Amount of modification to apply to the video, adhere_1 is the least amount of modification, reimagine_3 is the most',
      }),
  ),
  image_url: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'Optional URL of the first frame image for modification',
    }),
  ),
})

/**
 * File
 */
export const zFalAiLumaDreamMachineRay2FlashModifyFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * ModifyOutput
 */
export const zLumaDreamMachineRay2FlashModifyOutput = z.object({
  video: zFalAiLumaDreamMachineRay2FlashModifyFile,
})

/**
 * VideoConditioningInput
 */
export const zVideoConditioningInput = z.object({
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of video to use as conditioning',
  }),
  start_frame_num: z
    .optional(
      z.int().gte(0).lte(1440).register(z.globalRegistry, {
        description:
          'Frame number of the video from which the conditioning starts. Must be a multiple of 8.',
      }),
    )
    .default(0),
  reverse_video: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to reverse the video. This is useful for tasks where the video conditioning should be applied in reverse order.',
      }),
    )
    .default(false),
  limit_num_frames: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to limit the number of frames used from the video. If True, the `max_num_frames` parameter will be used to limit the number of frames.',
      }),
    )
    .default(false),
  resample_fps: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to resample the video to a specific FPS. If True, the `target_fps` parameter will be used to resample the video.',
      }),
    )
    .default(false),
  strength: z
    .optional(
      z.number().gte(0).lte(1).register(z.globalRegistry, {
        description:
          'Strength of the conditioning. 0.0 means no conditioning, 1.0 means full conditioning.',
      }),
    )
    .default(1),
  target_fps: z
    .optional(
      z.int().gte(1).lte(60).register(z.globalRegistry, {
        description:
          'Target FPS to resample the video to. Only relevant if `resample_fps` is True.',
      }),
    )
    .default(24),
  max_num_frames: z
    .optional(
      z.int().gte(1).lte(1441).register(z.globalRegistry, {
        description:
          'Maximum number of frames to use from the video. If None, all frames will be used.',
      }),
    )
    .default(1441),
  conditioning_type: z.optional(
    z.enum(['rgb', 'depth', 'pose', 'canny']).register(z.globalRegistry, {
      description:
        'Type of conditioning this video provides. This is relevant to ensure in-context LoRA weights are applied correctly, as well as selecting the correct preprocessing pipeline, when enabled.',
    }),
  ),
  preprocess: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to preprocess the video. If True, the video will be preprocessed to match the conditioning type. This is a no-op for RGB conditioning.',
      }),
    )
    .default(false),
})

/**
 * File
 */
export const zFalAiLtxv13B098DistilledMulticonditioningFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * MultiConditioningVideoOutput
 */
export const zLtxv13B098DistilledMulticonditioningOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for generation.',
  }),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiLtxv13B098DistilledMulticonditioningFile,
})

/**
 * SoundEffectRequest
 */
export const zPixverseSoundEffectsInput = z.object({
  prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description:
          'Description of the sound effect to generate. If empty, a random sound effect will be generated',
      }),
    )
    .default(''),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the input video to add sound effects to',
  }),
  original_sound_switch: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to keep the original audio from the video',
      }),
    )
    .default(false),
})

/**
 * File
 */
export const zFalAiPixverseSoundEffectsFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * SoundEffectOutput
 */
export const zPixverseSoundEffectsOutput = z.object({
  video: zFalAiPixverseSoundEffectsFile,
})

/**
 * Input
 */
export const zThinksoundAudioInput = z.object({
  prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description:
          'A prompt to guide the audio generation. If not provided, it will be extracted from the video.',
      }),
    )
    .default(''),
  video_url: z.string().register(z.globalRegistry, {
    description: 'The URL of the video to generate the audio for.',
  }),
  seed: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The seed for the random number generator',
    }),
  ),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(100).register(z.globalRegistry, {
        description: 'The number of inference steps for audio generation.',
      }),
    )
    .default(24),
  cfg_scale: z
    .optional(
      z.number().gte(1).lte(20).register(z.globalRegistry, {
        description: 'The classifier-free guidance scale for audio generation.',
      }),
    )
    .default(5),
})

/**
 * File
 */
export const zFalAiThinksoundAudioFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * AudioOutput
 */
export const zThinksoundAudioOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used to generate the audio.',
  }),
  audio: zFalAiThinksoundAudioFile,
})

/**
 * Input
 */
export const zThinksoundInput = z.object({
  prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description:
          'A prompt to guide the audio generation. If not provided, it will be extracted from the video.',
      }),
    )
    .default(''),
  video_url: z.string().register(z.globalRegistry, {
    description: 'The URL of the video to generate the audio for.',
  }),
  seed: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The seed for the random number generator',
    }),
  ),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(100).register(z.globalRegistry, {
        description: 'The number of inference steps for audio generation.',
      }),
    )
    .default(24),
  cfg_scale: z
    .optional(
      z.number().gte(1).lte(20).register(z.globalRegistry, {
        description: 'The classifier-free guidance scale for audio generation.',
      }),
    )
    .default(5),
})

/**
 * File
 */
export const zFalAiThinksoundFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * Output
 */
export const zThinksoundOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used to generate the audio.',
  }),
  video: zFalAiThinksoundFile,
})

/**
 * FastExtendRequest
 */
export const zPixverseExtendFastInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'Prompt describing how to extend the video',
  }),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the input video to extend',
  }),
  resolution: z.optional(
    z.enum(['360p', '540p', '720p']).register(z.globalRegistry, {
      description:
        "The resolution of the generated video. Fast mode doesn't support 1080p",
    }),
  ),
  style: z.optional(
    z
      .enum(['anime', '3d_animation', 'clay', 'comic', 'cyberpunk'])
      .register(z.globalRegistry, {
        description: 'The style of the extended video',
      }),
  ),
  model: z.optional(
    z.enum(['v3.5', 'v4', 'v4.5', 'v5', 'v5.5']).register(z.globalRegistry, {
      description: 'The model version to use for generation',
    }),
  ),
  seed: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'Random seed for generation',
    }),
  ),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'Negative prompt to be used for the generation',
      }),
    )
    .default(''),
})

/**
 * File
 */
export const zFalAiPixverseExtendFastFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * ExtendOutput
 */
export const zPixverseExtendFastOutput = z.object({
  video: zFalAiPixverseExtendFastFile,
})

/**
 * ExtendRequest
 */
export const zPixverseExtendInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'Prompt describing how to extend the video',
  }),
  duration: z.optional(
    z.enum(['5', '8']).register(z.globalRegistry, {
      description:
        'The duration of the generated video in seconds. 1080p videos are limited to 5 seconds',
    }),
  ),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the input video to extend',
  }),
  style: z.optional(
    z
      .enum(['anime', '3d_animation', 'clay', 'comic', 'cyberpunk'])
      .register(z.globalRegistry, {
        description: 'The style of the extended video',
      }),
  ),
  resolution: z.optional(
    z.enum(['360p', '540p', '720p', '1080p']).register(z.globalRegistry, {
      description: 'The resolution of the generated video',
    }),
  ),
  model: z.optional(
    z.enum(['v3.5', 'v4', 'v4.5', 'v5', 'v5.5']).register(z.globalRegistry, {
      description: 'The model version to use for generation',
    }),
  ),
  seed: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'Random seed for generation',
    }),
  ),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'Negative prompt to be used for the generation',
      }),
    )
    .default(''),
})

/**
 * File
 */
export const zFalAiPixverseExtendFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * ExtendOutput
 */
export const zPixverseExtendOutput = z.object({
  video: zFalAiPixverseExtendFile,
})

/**
 * LipsyncRequest
 */
export const zPixverseLipsyncInput = z.object({
  text: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'Text content for TTS when audio_url is not provided',
    }),
  ),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the input video',
  }),
  audio_url: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'URL of the input audio. If not provided, TTS will be used.',
    }),
  ),
  voice_id: z.optional(
    z
      .enum([
        'Emily',
        'James',
        'Isabella',
        'Liam',
        'Chloe',
        'Adrian',
        'Harper',
        'Ava',
        'Sophia',
        'Julia',
        'Mason',
        'Jack',
        'Oliver',
        'Ethan',
        'Auto',
      ])
      .register(z.globalRegistry, {
        description: 'Voice to use for TTS when audio_url is not provided',
      }),
  ),
})

/**
 * File
 */
export const zFalAiPixverseLipsyncFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * LipsyncOutput
 */
export const zPixverseLipsyncOutput = z.object({
  video: zFalAiPixverseLipsyncFile,
})

/**
 * ModifyVideoRequest
 */
export const zLumaDreamMachineRay2ModifyInput = z.object({
  prompt: z.optional(
    z.string().min(3).max(5000).register(z.globalRegistry, {
      description: 'Instruction for modifying the video',
    }),
  ),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the input video to modify',
  }),
  mode: z.optional(
    z
      .enum([
        'adhere_1',
        'adhere_2',
        'adhere_3',
        'flex_1',
        'flex_2',
        'flex_3',
        'reimagine_1',
        'reimagine_2',
        'reimagine_3',
      ])
      .register(z.globalRegistry, {
        description:
          'Amount of modification to apply to the video, adhere_1 is the least amount of modification, reimagine_3 is the most',
      }),
  ),
  image_url: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'Optional URL of the first frame image for modification',
    }),
  ),
})

/**
 * File
 */
export const zFalAiLumaDreamMachineRay2ModifyFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * ModifyOutput
 */
export const zLumaDreamMachineRay2ModifyOutput = z.object({
  video: zFalAiLumaDreamMachineRay2ModifyFile,
})

/**
 * WanVACEReframeRequest
 */
export const zWanVace14bReframeInput = z.object({
  prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description:
          'The text prompt to guide video generation. Optional for reframing.',
      }),
    )
    .default(''),
  video_url: z.string().register(z.globalRegistry, {
    description:
      'URL to the source video file. This video will be used as a reference for the reframe task.',
  }),
  num_interpolated_frames: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Number of frames to interpolate between the original frames. A value of 0 means no interpolation.',
      }),
    )
    .default(0),
  temporal_downsample_factor: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.',
      }),
    )
    .default(0),
  first_frame_url: z.optional(z.union([z.string(), z.unknown()])),
  transparency_mode: z.optional(
    z.enum(['content_aware', 'white', 'black']).register(z.globalRegistry, {
      description:
        'The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.',
    }),
  ),
  num_frames: z
    .optional(
      z.int().gte(17).lte(241).register(z.globalRegistry, {
        description:
          'Number of frames to generate. Must be between 81 to 241 (inclusive).',
      }),
    )
    .default(81),
  trim_borders: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to trim borders from the video.',
      }),
    )
    .default(true),
  auto_downsample_min_fps: z
    .optional(
      z.number().gte(1).lte(60).register(z.globalRegistry, {
        description:
          'The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.',
      }),
    )
    .default(15),
  sampler: z.optional(
    z.enum(['unipc', 'dpm++', 'euler']).register(z.globalRegistry, {
      description: 'Sampler to use for video generation.',
    }),
  ),
  guidance_scale: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description:
          'Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.',
      }),
    )
    .default(5),
  video_quality: z.optional(
    z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
      description: 'The quality of the generated video.',
    }),
  ),
  sync_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
      }),
    )
    .default(false),
  enable_prompt_expansion: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enable prompt expansion.',
      }),
    )
    .default(false),
  seed: z.optional(z.union([z.int(), z.unknown()])),
  interpolator_model: z.optional(
    z.enum(['rife', 'film']).register(z.globalRegistry, {
      description:
        "The model to use for frame interpolation. Options are 'rife' or 'film'.",
    }),
  ),
  enable_auto_downsample: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.',
      }),
    )
    .default(false),
  shift: z
    .optional(
      z.number().gte(1).lte(15).register(z.globalRegistry, {
        description: 'Shift parameter for video generation.',
      }),
    )
    .default(5),
  acceleration: z.optional(
    z.union([z.enum(['none', 'low', 'regular']), z.unknown()]),
  ),
  zoom_factor: z
    .optional(
      z.number().gte(0).lte(0.9).register(z.globalRegistry, {
        description:
          'Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom.',
      }),
    )
    .default(0),
  match_input_num_frames: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.',
      }),
    )
    .default(true),
  frames_per_second: z.optional(z.union([z.int().gte(5).lte(30), z.unknown()])),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If set to true, the safety checker will be enabled.',
      }),
    )
    .default(false),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'Negative prompt for video generation.',
      }),
    )
    .default(
      'letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards',
    ),
  video_write_mode: z.optional(
    z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
      description: 'The write mode of the generated video.',
    }),
  ),
  return_frames_zip: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, also return a ZIP file containing all generated frames.',
      }),
    )
    .default(false),
  resolution: z.optional(
    z
      .enum(['auto', '240p', '360p', '480p', '580p', '720p'])
      .register(z.globalRegistry, {
        description: 'Resolution of the generated video.',
      }),
  ),
  aspect_ratio: z.optional(
    z.enum(['auto', '16:9', '1:1', '9:16']).register(z.globalRegistry, {
      description: 'Aspect ratio of the generated video.',
    }),
  ),
  match_input_frames_per_second: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.',
      }),
    )
    .default(true),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(50).register(z.globalRegistry, {
        description:
          'Number of inference steps for sampling. Higher values give better quality but take longer.',
      }),
    )
    .default(30),
  last_frame_url: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * File
 */
export const zFalAiWanVace14bReframeFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * VideoFile
 */
export const zFalAiWanVace14bReframeVideoFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  duration: z.optional(z.union([z.number(), z.unknown()])),
  height: z.optional(z.union([z.int(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  width: z.optional(z.union([z.int(), z.unknown()])),
  fps: z.optional(z.union([z.number(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  num_frames: z.optional(z.union([z.int(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * WanVACEReframeResponse
 */
export const zWanVace14bReframeOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for generation.',
  }),
  frames_zip: z.optional(z.union([zFalAiWanVace14bReframeFile, z.unknown()])),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiWanVace14bReframeVideoFile,
})

/**
 * WanVACEOutpaintingRequest
 */
export const zWanVace14bOutpaintingInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The text prompt to guide video generation.',
  }),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL to the source video file. Required for outpainting.',
  }),
  num_interpolated_frames: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Number of frames to interpolate between the original frames. A value of 0 means no interpolation.',
      }),
    )
    .default(0),
  temporal_downsample_factor: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.',
      }),
    )
    .default(0),
  first_frame_url: z.optional(z.union([z.string(), z.unknown()])),
  ref_image_urls: z.optional(
    z.array(z.string()).register(z.globalRegistry, {
      description:
        'URLs to source reference image. If provided, the model will use this image as reference.',
    }),
  ),
  expand_ratio: z
    .optional(
      z.number().gte(0).lte(1).register(z.globalRegistry, {
        description:
          'Amount of expansion. This is a float value between 0 and 1, where 0.25 adds 25% to the original video size on the specified sides.',
      }),
    )
    .default(0.25),
  transparency_mode: z.optional(
    z.enum(['content_aware', 'white', 'black']).register(z.globalRegistry, {
      description:
        'The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.',
    }),
  ),
  num_frames: z
    .optional(
      z.int().gte(17).lte(241).register(z.globalRegistry, {
        description:
          'Number of frames to generate. Must be between 81 to 241 (inclusive).',
      }),
    )
    .default(81),
  auto_downsample_min_fps: z
    .optional(
      z.number().gte(1).lte(60).register(z.globalRegistry, {
        description:
          'The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.',
      }),
    )
    .default(15),
  expand_bottom: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to expand the video to the bottom.',
      }),
    )
    .default(false),
  sampler: z.optional(
    z.enum(['unipc', 'dpm++', 'euler']).register(z.globalRegistry, {
      description: 'Sampler to use for video generation.',
    }),
  ),
  guidance_scale: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description:
          'Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.',
      }),
    )
    .default(5),
  video_quality: z.optional(
    z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
      description: 'The quality of the generated video.',
    }),
  ),
  sync_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
      }),
    )
    .default(false),
  enable_prompt_expansion: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enable prompt expansion.',
      }),
    )
    .default(false),
  seed: z.optional(z.union([z.int(), z.unknown()])),
  interpolator_model: z.optional(
    z.enum(['rife', 'film']).register(z.globalRegistry, {
      description:
        "The model to use for frame interpolation. Options are 'rife' or 'film'.",
    }),
  ),
  enable_auto_downsample: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.',
      }),
    )
    .default(false),
  expand_top: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to expand the video to the top.',
      }),
    )
    .default(false),
  shift: z
    .optional(
      z.number().gte(1).lte(15).register(z.globalRegistry, {
        description: 'Shift parameter for video generation.',
      }),
    )
    .default(5),
  expand_left: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to expand the video to the left.',
      }),
    )
    .default(false),
  acceleration: z.optional(
    z.union([z.enum(['none', 'low', 'regular']), z.unknown()]),
  ),
  match_input_num_frames: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.',
      }),
    )
    .default(false),
  frames_per_second: z.optional(z.union([z.int().gte(5).lte(30), z.unknown()])),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If set to true, the safety checker will be enabled.',
      }),
    )
    .default(false),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'Negative prompt for video generation.',
      }),
    )
    .default(
      'letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards',
    ),
  video_write_mode: z.optional(
    z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
      description: 'The write mode of the generated video.',
    }),
  ),
  return_frames_zip: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, also return a ZIP file containing all generated frames.',
      }),
    )
    .default(false),
  resolution: z.optional(
    z
      .enum(['auto', '240p', '360p', '480p', '580p', '720p'])
      .register(z.globalRegistry, {
        description: 'Resolution of the generated video.',
      }),
  ),
  expand_right: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to expand the video to the right.',
      }),
    )
    .default(false),
  aspect_ratio: z.optional(
    z.enum(['auto', '16:9', '1:1', '9:16']).register(z.globalRegistry, {
      description: 'Aspect ratio of the generated video.',
    }),
  ),
  match_input_frames_per_second: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.',
      }),
    )
    .default(false),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(50).register(z.globalRegistry, {
        description:
          'Number of inference steps for sampling. Higher values give better quality but take longer.',
      }),
    )
    .default(30),
  last_frame_url: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * File
 */
export const zFalAiWanVace14bOutpaintingFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * VideoFile
 */
export const zFalAiWanVace14bOutpaintingVideoFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  duration: z.optional(z.union([z.number(), z.unknown()])),
  height: z.optional(z.union([z.int(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  width: z.optional(z.union([z.int(), z.unknown()])),
  fps: z.optional(z.union([z.number(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  num_frames: z.optional(z.union([z.int(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * WanVACEOutpaintingResponse
 */
export const zWanVace14bOutpaintingOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for generation.',
  }),
  frames_zip: z.optional(
    z.union([zFalAiWanVace14bOutpaintingFile, z.unknown()]),
  ),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiWanVace14bOutpaintingVideoFile,
})

/**
 * WanVACEInpaintingRequest
 */
export const zWanVace14bInpaintingInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The text prompt to guide video generation.',
  }),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL to the source video file. Required for inpainting.',
  }),
  num_interpolated_frames: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Number of frames to interpolate between the original frames. A value of 0 means no interpolation.',
      }),
    )
    .default(0),
  temporal_downsample_factor: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.',
      }),
    )
    .default(0),
  first_frame_url: z.optional(z.union([z.string(), z.unknown()])),
  ref_image_urls: z.optional(
    z.array(z.string()).register(z.globalRegistry, {
      description:
        'Urls to source reference image. If provided, the model will use this image as reference.',
    }),
  ),
  transparency_mode: z.optional(
    z.enum(['content_aware', 'white', 'black']).register(z.globalRegistry, {
      description:
        'The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.',
    }),
  ),
  num_frames: z
    .optional(
      z.int().gte(17).lte(241).register(z.globalRegistry, {
        description:
          'Number of frames to generate. Must be between 81 to 241 (inclusive).',
      }),
    )
    .default(81),
  auto_downsample_min_fps: z
    .optional(
      z.number().gte(1).lte(60).register(z.globalRegistry, {
        description:
          'The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.',
      }),
    )
    .default(15),
  guidance_scale: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description:
          'Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.',
      }),
    )
    .default(5),
  sampler: z.optional(
    z.enum(['unipc', 'dpm++', 'euler']).register(z.globalRegistry, {
      description: 'Sampler to use for video generation.',
    }),
  ),
  video_quality: z.optional(
    z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
      description: 'The quality of the generated video.',
    }),
  ),
  sync_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
      }),
    )
    .default(false),
  mask_video_url: z.union([z.string(), z.unknown()]),
  seed: z.optional(z.union([z.int(), z.unknown()])),
  interpolator_model: z.optional(
    z.enum(['rife', 'film']).register(z.globalRegistry, {
      description:
        "The model to use for frame interpolation. Options are 'rife' or 'film'.",
    }),
  ),
  enable_auto_downsample: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.',
      }),
    )
    .default(false),
  enable_prompt_expansion: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enable prompt expansion.',
      }),
    )
    .default(false),
  shift: z
    .optional(
      z.number().gte(1).lte(15).register(z.globalRegistry, {
        description: 'Shift parameter for video generation.',
      }),
    )
    .default(5),
  preprocess: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to preprocess the input video.',
      }),
    )
    .default(false),
  acceleration: z.optional(
    z.union([z.enum(['none', 'low', 'regular']), z.unknown()]),
  ),
  mask_image_url: z.optional(z.union([z.string(), z.unknown()])),
  match_input_num_frames: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.',
      }),
    )
    .default(false),
  frames_per_second: z.optional(z.union([z.int().gte(5).lte(30), z.unknown()])),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If set to true, the safety checker will be enabled.',
      }),
    )
    .default(false),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'Negative prompt for video generation.',
      }),
    )
    .default(
      'letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards',
    ),
  video_write_mode: z.optional(
    z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
      description: 'The write mode of the generated video.',
    }),
  ),
  return_frames_zip: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, also return a ZIP file containing all generated frames.',
      }),
    )
    .default(false),
  resolution: z.optional(
    z
      .enum(['auto', '240p', '360p', '480p', '580p', '720p'])
      .register(z.globalRegistry, {
        description: 'Resolution of the generated video.',
      }),
  ),
  aspect_ratio: z.optional(
    z.enum(['auto', '16:9', '1:1', '9:16']).register(z.globalRegistry, {
      description: 'Aspect ratio of the generated video.',
    }),
  ),
  match_input_frames_per_second: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.',
      }),
    )
    .default(false),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(50).register(z.globalRegistry, {
        description:
          'Number of inference steps for sampling. Higher values give better quality but take longer.',
      }),
    )
    .default(30),
  last_frame_url: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * File
 */
export const zFalAiWanVace14bInpaintingFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * VideoFile
 */
export const zFalAiWanVace14bInpaintingVideoFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  duration: z.optional(z.union([z.number(), z.unknown()])),
  height: z.optional(z.union([z.int(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  width: z.optional(z.union([z.int(), z.unknown()])),
  fps: z.optional(z.union([z.number(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  num_frames: z.optional(z.union([z.int(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * WanVACEInpaintingResponse
 */
export const zWanVace14bInpaintingOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for generation.',
  }),
  frames_zip: z.optional(
    z.union([zFalAiWanVace14bInpaintingFile, z.unknown()]),
  ),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiWanVace14bInpaintingVideoFile,
})

/**
 * WanVACEPoseRequest
 */
export const zWanVace14bPoseInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description:
      'The text prompt to guide video generation. For pose task, the prompt should describe the desired pose and action of the subject in the video.',
  }),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL to the source video file. Required for pose task.',
  }),
  num_interpolated_frames: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Number of frames to interpolate between the original frames. A value of 0 means no interpolation.',
      }),
    )
    .default(0),
  temporal_downsample_factor: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.',
      }),
    )
    .default(0),
  first_frame_url: z.optional(z.union([z.string(), z.unknown()])),
  ref_image_urls: z.optional(
    z.array(z.string()).register(z.globalRegistry, {
      description:
        'URLs to source reference image. If provided, the model will use this image as reference.',
    }),
  ),
  transparency_mode: z.optional(
    z.enum(['content_aware', 'white', 'black']).register(z.globalRegistry, {
      description:
        'The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.',
    }),
  ),
  num_frames: z
    .optional(
      z.int().gte(17).lte(241).register(z.globalRegistry, {
        description:
          'Number of frames to generate. Must be between 81 to 241 (inclusive).',
      }),
    )
    .default(81),
  auto_downsample_min_fps: z
    .optional(
      z.number().gte(1).lte(60).register(z.globalRegistry, {
        description:
          'The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.',
      }),
    )
    .default(15),
  guidance_scale: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description:
          'Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.',
      }),
    )
    .default(5),
  sampler: z.optional(
    z.enum(['unipc', 'dpm++', 'euler']).register(z.globalRegistry, {
      description: 'Sampler to use for video generation.',
    }),
  ),
  video_quality: z.optional(
    z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
      description: 'The quality of the generated video.',
    }),
  ),
  sync_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
      }),
    )
    .default(false),
  enable_prompt_expansion: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enable prompt expansion.',
      }),
    )
    .default(false),
  seed: z.optional(z.union([z.int(), z.unknown()])),
  interpolator_model: z.optional(
    z.enum(['rife', 'film']).register(z.globalRegistry, {
      description:
        "The model to use for frame interpolation. Options are 'rife' or 'film'.",
    }),
  ),
  enable_auto_downsample: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.',
      }),
    )
    .default(false),
  preprocess: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to preprocess the input video.',
      }),
    )
    .default(false),
  shift: z
    .optional(
      z.number().gte(1).lte(15).register(z.globalRegistry, {
        description: 'Shift parameter for video generation.',
      }),
    )
    .default(5),
  acceleration: z.optional(
    z.union([z.enum(['none', 'low', 'regular']), z.unknown()]),
  ),
  match_input_num_frames: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.',
      }),
    )
    .default(false),
  frames_per_second: z.optional(z.union([z.int().gte(5).lte(30), z.unknown()])),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If set to true, the safety checker will be enabled.',
      }),
    )
    .default(false),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'Negative prompt for video generation.',
      }),
    )
    .default(
      'letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards',
    ),
  video_write_mode: z.optional(
    z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
      description: 'The write mode of the generated video.',
    }),
  ),
  return_frames_zip: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, also return a ZIP file containing all generated frames.',
      }),
    )
    .default(false),
  resolution: z.optional(
    z
      .enum(['auto', '240p', '360p', '480p', '580p', '720p'])
      .register(z.globalRegistry, {
        description: 'Resolution of the generated video.',
      }),
  ),
  aspect_ratio: z.optional(
    z.enum(['auto', '16:9', '1:1', '9:16']).register(z.globalRegistry, {
      description: 'Aspect ratio of the generated video.',
    }),
  ),
  match_input_frames_per_second: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.',
      }),
    )
    .default(false),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(50).register(z.globalRegistry, {
        description:
          'Number of inference steps for sampling. Higher values give better quality but take longer.',
      }),
    )
    .default(30),
  last_frame_url: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * File
 */
export const zFalAiWanVace14bPoseFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * VideoFile
 */
export const zFalAiWanVace14bPoseVideoFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  duration: z.optional(z.union([z.number(), z.unknown()])),
  height: z.optional(z.union([z.int(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  width: z.optional(z.union([z.int(), z.unknown()])),
  fps: z.optional(z.union([z.number(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  num_frames: z.optional(z.union([z.int(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * WanVACEPoseResponse
 */
export const zWanVace14bPoseOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for generation.',
  }),
  frames_zip: z.optional(z.union([zFalAiWanVace14bPoseFile, z.unknown()])),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiWanVace14bPoseVideoFile,
})

/**
 * WanVACEDepthRequest
 */
export const zWanVace14bDepthInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The text prompt to guide video generation.',
  }),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL to the source video file. Required for depth task.',
  }),
  num_interpolated_frames: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Number of frames to interpolate between the original frames. A value of 0 means no interpolation.',
      }),
    )
    .default(0),
  temporal_downsample_factor: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.',
      }),
    )
    .default(0),
  first_frame_url: z.optional(z.union([z.string(), z.unknown()])),
  ref_image_urls: z.optional(
    z.array(z.string()).register(z.globalRegistry, {
      description:
        'URLs to source reference image. If provided, the model will use this image as reference.',
    }),
  ),
  transparency_mode: z.optional(
    z.enum(['content_aware', 'white', 'black']).register(z.globalRegistry, {
      description:
        'The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.',
    }),
  ),
  num_frames: z
    .optional(
      z.int().gte(17).lte(241).register(z.globalRegistry, {
        description:
          'Number of frames to generate. Must be between 81 to 241 (inclusive).',
      }),
    )
    .default(81),
  auto_downsample_min_fps: z
    .optional(
      z.number().gte(1).lte(60).register(z.globalRegistry, {
        description:
          'The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.',
      }),
    )
    .default(15),
  guidance_scale: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description:
          'Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.',
      }),
    )
    .default(5),
  sampler: z.optional(
    z.enum(['unipc', 'dpm++', 'euler']).register(z.globalRegistry, {
      description: 'Sampler to use for video generation.',
    }),
  ),
  video_quality: z.optional(
    z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
      description: 'The quality of the generated video.',
    }),
  ),
  sync_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
      }),
    )
    .default(false),
  enable_prompt_expansion: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enable prompt expansion.',
      }),
    )
    .default(false),
  seed: z.optional(z.union([z.int(), z.unknown()])),
  interpolator_model: z.optional(
    z.enum(['rife', 'film']).register(z.globalRegistry, {
      description:
        "The model to use for frame interpolation. Options are 'rife' or 'film'.",
    }),
  ),
  enable_auto_downsample: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.',
      }),
    )
    .default(false),
  preprocess: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to preprocess the input video.',
      }),
    )
    .default(false),
  shift: z
    .optional(
      z.number().gte(1).lte(15).register(z.globalRegistry, {
        description: 'Shift parameter for video generation.',
      }),
    )
    .default(5),
  acceleration: z.optional(
    z.union([z.enum(['none', 'low', 'regular']), z.unknown()]),
  ),
  match_input_num_frames: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.',
      }),
    )
    .default(false),
  frames_per_second: z.optional(z.union([z.int().gte(5).lte(30), z.unknown()])),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If set to true, the safety checker will be enabled.',
      }),
    )
    .default(false),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'Negative prompt for video generation.',
      }),
    )
    .default(
      'letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards',
    ),
  video_write_mode: z.optional(
    z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
      description: 'The write mode of the generated video.',
    }),
  ),
  return_frames_zip: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, also return a ZIP file containing all generated frames.',
      }),
    )
    .default(false),
  resolution: z.optional(
    z
      .enum(['auto', '240p', '360p', '480p', '580p', '720p'])
      .register(z.globalRegistry, {
        description: 'Resolution of the generated video.',
      }),
  ),
  aspect_ratio: z.optional(
    z.enum(['auto', '16:9', '1:1', '9:16']).register(z.globalRegistry, {
      description: 'Aspect ratio of the generated video.',
    }),
  ),
  match_input_frames_per_second: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.',
      }),
    )
    .default(false),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(50).register(z.globalRegistry, {
        description:
          'Number of inference steps for sampling. Higher values give better quality but take longer.',
      }),
    )
    .default(30),
  last_frame_url: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * File
 */
export const zFalAiWanVace14bDepthFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * VideoFile
 */
export const zFalAiWanVace14bDepthVideoFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  duration: z.optional(z.union([z.number(), z.unknown()])),
  height: z.optional(z.union([z.int(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  width: z.optional(z.union([z.int(), z.unknown()])),
  fps: z.optional(z.union([z.number(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  num_frames: z.optional(z.union([z.int(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * WanVACEDepthResponse
 */
export const zWanVace14bDepthOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for generation.',
  }),
  frames_zip: z.optional(z.union([zFalAiWanVace14bDepthFile, z.unknown()])),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiWanVace14bDepthVideoFile,
})

/**
 * DWPoseVideoInput
 */
export const zDwposeVideoInput = z.object({
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of video to be used for pose estimation',
  }),
  draw_mode: z.optional(
    z
      .enum([
        'full-pose',
        'body-pose',
        'face-pose',
        'hand-pose',
        'face-hand-mask',
        'face-mask',
        'hand-mask',
      ])
      .register(z.globalRegistry, {
        description:
          "Mode of drawing the pose on the video. Options are: 'full-pose', 'body-pose', 'face-pose', 'hand-pose', 'face-hand-mask', 'face-mask', 'hand-mask'.",
      }),
  ),
})

/**
 * File
 */
export const zFalAiDwposeVideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * DWPoseVideoOutput
 */
export const zDwposeVideoOutput = z.object({
  video: zFalAiDwposeVideoFile,
})

/**
 * CombineInput
 */
export const zFfmpegApiMergeAudioVideoInput = z.object({
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the video file to use as the video track',
  }),
  start_offset: z
    .optional(
      z.number().gte(0).register(z.globalRegistry, {
        description:
          'Offset in seconds for when the audio should start relative to the video',
      }),
    )
    .default(0),
  audio_url: z.string().register(z.globalRegistry, {
    description: 'URL of the audio file to use as the audio track',
  }),
})

/**
 * File
 */
export const zFalAiFfmpegApiMergeAudioVideoFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * CombineOutput
 */
export const zFfmpegApiMergeAudioVideoOutput = z.object({
  video: zFalAiFfmpegApiMergeAudioVideoFile,
})

/**
 * WanT2VRequest
 */
export const zWanVace13bInput = z.object({
  shift: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description: 'Shift parameter for video generation.',
      }),
    )
    .default(5),
  video_url: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'URL to the source video file. If provided, the model will use this video as a reference.',
    }),
  ),
  prompt: z.string().register(z.globalRegistry, {
    description: 'The text prompt to guide video generation.',
  }),
  mask_image_url: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored.',
    }),
  ),
  task: z.optional(
    z.enum(['depth', 'inpainting', 'pose']).register(z.globalRegistry, {
      description: 'Task type for the model.',
    }),
  ),
  frames_per_second: z
    .optional(
      z.int().gte(5).lte(24).register(z.globalRegistry, {
        description:
          'Frames per second of the generated video. Must be between 5 to 24.',
      }),
    )
    .default(16),
  ref_image_urls: z.optional(
    z.array(z.string()).register(z.globalRegistry, {
      description:
        'Urls to source reference image. If provided, the model will use this image as reference.',
    }),
  ),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If set to true, the safety checker will be enabled.',
      }),
    )
    .default(false),
  num_frames: z
    .optional(
      z.int().gte(81).lte(240).register(z.globalRegistry, {
        description:
          'Number of frames to generate. Must be between 81 to 100 (inclusive). Works only with only reference images as input if source video or mask video is provided output len would be same as source up to 241 frames',
      }),
    )
    .default(81),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'Negative prompt for video generation.',
      }),
    )
    .default(
      'bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards',
    ),
  resolution: z.optional(
    z.enum(['480p', '580p', '720p']).register(z.globalRegistry, {
      description: 'Resolution of the generated video (480p,580p, or 720p).',
    }),
  ),
  aspect_ratio: z.optional(
    z.enum(['auto', '9:16', '16:9']).register(z.globalRegistry, {
      description: 'Aspect ratio of the generated video (16:9 or 9:16).',
    }),
  ),
  mask_video_url: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'URL to the source mask file. If provided, the model will use this mask as a reference.',
    }),
  ),
  seed: z.optional(
    z.int().register(z.globalRegistry, {
      description:
        'Random seed for reproducibility. If None, a random seed is chosen.',
    }),
  ),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(40).register(z.globalRegistry, {
        description:
          'Number of inference steps for sampling. Higher values give better quality but take longer.',
      }),
    )
    .default(30),
  preprocess: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to preprocess the input video.',
      }),
    )
    .default(false),
  enable_prompt_expansion: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enable prompt expansion.',
      }),
    )
    .default(false),
})

/**
 * File
 */
export const zFalAiWanVace13bFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * WanT2VResponse
 */
export const zWanVace13bOutput = z.object({
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiWanVace13bFile,
})

/**
 * ReframeVideoRequest
 */
export const zLumaDreamMachineRay2FlashReframeInput = z.object({
  prompt: z.optional(
    z.string().min(1).max(5000).register(z.globalRegistry, {
      description: 'Optional prompt for reframing',
    }),
  ),
  aspect_ratio: z
    .enum(['1:1', '16:9', '9:16', '4:3', '3:4', '21:9', '9:21'])
    .register(z.globalRegistry, {
      description: 'The aspect ratio of the reframed video',
    }),
  y_start: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'Start Y coordinate for reframing',
    }),
  ),
  x_end: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'End X coordinate for reframing',
    }),
  ),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the input video to reframe',
  }),
  y_end: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'End Y coordinate for reframing',
    }),
  ),
  x_start: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'Start X coordinate for reframing',
    }),
  ),
  grid_position_y: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'Y position of the grid for reframing',
    }),
  ),
  grid_position_x: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'X position of the grid for reframing',
    }),
  ),
  image_url: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'Optional URL of the first frame image for reframing',
    }),
  ),
})

/**
 * File
 */
export const zFalAiLumaDreamMachineRay2FlashReframeFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * ReframeOutput
 */
export const zLumaDreamMachineRay2FlashReframeOutput = z.object({
  video: zFalAiLumaDreamMachineRay2FlashReframeFile,
})

/**
 * ReframeVideoRequest
 */
export const zLumaDreamMachineRay2ReframeInput = z.object({
  prompt: z.optional(
    z.string().min(1).max(5000).register(z.globalRegistry, {
      description: 'Optional prompt for reframing',
    }),
  ),
  aspect_ratio: z
    .enum(['1:1', '16:9', '9:16', '4:3', '3:4', '21:9', '9:21'])
    .register(z.globalRegistry, {
      description: 'The aspect ratio of the reframed video',
    }),
  y_start: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'Start Y coordinate for reframing',
    }),
  ),
  x_end: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'End X coordinate for reframing',
    }),
  ),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the input video to reframe',
  }),
  y_end: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'End Y coordinate for reframing',
    }),
  ),
  x_start: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'Start X coordinate for reframing',
    }),
  ),
  grid_position_y: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'Y position of the grid for reframing',
    }),
  ),
  grid_position_x: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'X position of the grid for reframing',
    }),
  ),
  image_url: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'Optional URL of the first frame image for reframing',
    }),
  ),
})

/**
 * File
 */
export const zFalAiLumaDreamMachineRay2ReframeFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * ReframeOutput
 */
export const zLumaDreamMachineRay2ReframeOutput = z.object({
  video: zFalAiLumaDreamMachineRay2ReframeFile,
})

/**
 * LipsyncInput
 */
export const zLipsyncInput = z.object({
  video_url: z.url().min(1).max(2083),
  audio_url: z.url().min(1).max(2083),
})

/**
 * File
 */
export const zVeedLipsyncFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * LipsyncAppOutput
 */
export const zLipsyncOutput = z.object({
  video: zVeedLipsyncFile,
})

/**
 * WanVACERequest
 */
export const zWanVace14bInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The text prompt to guide video generation.',
  }),
  video_url: z.optional(z.union([z.string(), z.unknown()])),
  num_interpolated_frames: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Number of frames to interpolate between the original frames. A value of 0 means no interpolation.',
      }),
    )
    .default(0),
  temporal_downsample_factor: z
    .optional(
      z.int().gte(0).lte(5).register(z.globalRegistry, {
        description:
          'Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.',
      }),
    )
    .default(0),
  first_frame_url: z.optional(z.union([z.string(), z.unknown()])),
  ref_image_urls: z.optional(
    z.array(z.string()).register(z.globalRegistry, {
      description:
        'URLs to source reference image. If provided, the model will use this image as reference.',
    }),
  ),
  transparency_mode: z.optional(
    z.enum(['content_aware', 'white', 'black']).register(z.globalRegistry, {
      description:
        'The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.',
    }),
  ),
  num_frames: z
    .optional(
      z.int().gte(17).lte(241).register(z.globalRegistry, {
        description:
          'Number of frames to generate. Must be between 81 to 241 (inclusive).',
      }),
    )
    .default(81),
  auto_downsample_min_fps: z
    .optional(
      z.number().gte(1).lte(60).register(z.globalRegistry, {
        description:
          'The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.',
      }),
    )
    .default(15),
  guidance_scale: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description:
          'Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.',
      }),
    )
    .default(5),
  sampler: z.optional(
    z.enum(['unipc', 'dpm++', 'euler']).register(z.globalRegistry, {
      description: 'Sampler to use for video generation.',
    }),
  ),
  video_quality: z.optional(
    z.enum(['low', 'medium', 'high', 'maximum']).register(z.globalRegistry, {
      description: 'The quality of the generated video.',
    }),
  ),
  sync_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
      }),
    )
    .default(false),
  mask_video_url: z.optional(z.union([z.string(), z.unknown()])),
  seed: z.optional(z.union([z.int(), z.unknown()])),
  interpolator_model: z.optional(
    z.enum(['rife', 'film']).register(z.globalRegistry, {
      description:
        "The model to use for frame interpolation. Options are 'rife' or 'film'.",
    }),
  ),
  enable_auto_downsample: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.',
      }),
    )
    .default(false),
  preprocess: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to preprocess the input video.',
      }),
    )
    .default(false),
  shift: z
    .optional(
      z.number().gte(1).lte(15).register(z.globalRegistry, {
        description: 'Shift parameter for video generation.',
      }),
    )
    .default(5),
  enable_prompt_expansion: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enable prompt expansion.',
      }),
    )
    .default(false),
  acceleration: z.optional(
    z.union([z.enum(['none', 'low', 'regular']), z.unknown()]),
  ),
  mask_image_url: z.optional(z.union([z.string(), z.unknown()])),
  task: z.optional(
    z
      .enum(['depth', 'pose', 'inpainting', 'outpainting', 'reframe'])
      .register(z.globalRegistry, {
        description: 'Task type for the model.',
      }),
  ),
  match_input_num_frames: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.',
      }),
    )
    .default(false),
  frames_per_second: z.optional(z.union([z.int().gte(5).lte(30), z.unknown()])),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If set to true, the safety checker will be enabled.',
      }),
    )
    .default(false),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'Negative prompt for video generation.',
      }),
    )
    .default(
      'letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards',
    ),
  video_write_mode: z.optional(
    z.enum(['fast', 'balanced', 'small']).register(z.globalRegistry, {
      description: 'The write mode of the generated video.',
    }),
  ),
  return_frames_zip: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, also return a ZIP file containing all generated frames.',
      }),
    )
    .default(false),
  resolution: z.optional(
    z
      .enum(['auto', '240p', '360p', '480p', '580p', '720p'])
      .register(z.globalRegistry, {
        description: 'Resolution of the generated video.',
      }),
  ),
  aspect_ratio: z.optional(
    z.enum(['auto', '16:9', '1:1', '9:16']).register(z.globalRegistry, {
      description: 'Aspect ratio of the generated video.',
    }),
  ),
  match_input_frames_per_second: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.',
      }),
    )
    .default(false),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(50).register(z.globalRegistry, {
        description:
          'Number of inference steps for sampling. Higher values give better quality but take longer.',
      }),
    )
    .default(30),
  last_frame_url: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * File
 */
export const zFalAiWanVace14bFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * VideoFile
 */
export const zFalAiWanVace14bVideoFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  duration: z.optional(z.union([z.number(), z.unknown()])),
  height: z.optional(z.union([z.int(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  width: z.optional(z.union([z.int(), z.unknown()])),
  fps: z.optional(z.union([z.number(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  num_frames: z.optional(z.union([z.int(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * WanVACEResponse
 */
export const zWanVace14bOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for generation.',
  }),
  frames_zip: z.optional(z.union([zFalAiWanVace14bFile, z.unknown()])),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiWanVace14bVideoFile,
})

/**
 * VideoConditioningInput
 */
export const zFalAiLtxVideo13bDistilledExtendVideoConditioningInput = z.object({
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of video to use as conditioning',
  }),
  reverse_video: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to reverse the video. This is useful for tasks where the video conditioning should be applied in reverse order.',
      }),
    )
    .default(false),
  start_frame_num: z
    .optional(
      z.int().gte(0).lte(160).register(z.globalRegistry, {
        description:
          'Frame number of the video from which the conditioning starts. Must be a multiple of 8.',
      }),
    )
    .default(0),
  limit_num_frames: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to limit the number of frames used from the video. If True, the `max_num_frames` parameter will be used to limit the number of frames.',
      }),
    )
    .default(false),
  resample_fps: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to resample the video to a specific FPS. If True, the `target_fps` parameter will be used to resample the video.',
      }),
    )
    .default(false),
  strength: z
    .optional(
      z.number().gte(0).lte(1).register(z.globalRegistry, {
        description:
          'Strength of the conditioning. 0.0 means no conditioning, 1.0 means full conditioning.',
      }),
    )
    .default(1),
  target_fps: z
    .optional(
      z.int().gte(1).lte(60).register(z.globalRegistry, {
        description:
          'Target FPS to resample the video to. Only relevant if `resample_fps` is True.',
      }),
    )
    .default(30),
  max_num_frames: z
    .optional(
      z.int().gte(1).lte(161).register(z.globalRegistry, {
        description:
          'Maximum number of frames to use from the video. If None, all frames will be used.',
      }),
    )
    .default(121),
  conditioning_type: z.optional(
    z.enum(['rgb', 'depth', 'pose', 'canny']).register(z.globalRegistry, {
      description:
        'Type of conditioning this video provides. This is relevant to ensure in-context LoRA weights are applied correctly, as well as selecting the correct preprocessing pipeline, when enabled.',
    }),
  ),
  preprocess: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to preprocess the video. If True, the video will be preprocessed to match the conditioning type. This is a no-op for RGB conditioning.',
      }),
    )
    .default(false),
})

/**
 * File
 */
export const zFalAiLtxVideo13bDistilledExtendFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * ExtendVideoOutput
 */
export const zLtxVideo13bDistilledExtendOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for generation.',
  }),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiLtxVideo13bDistilledExtendFile,
})

/**
 * VideoConditioningInput
 */
export const zFalAiLtxVideo13bDistilledMulticonditioningVideoConditioningInput =
  z.object({
    video_url: z.string().register(z.globalRegistry, {
      description: 'URL of video to use as conditioning',
    }),
    reverse_video: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'Whether to reverse the video. This is useful for tasks where the video conditioning should be applied in reverse order.',
        }),
      )
      .default(false),
    start_frame_num: z
      .optional(
        z.int().gte(0).lte(160).register(z.globalRegistry, {
          description:
            'Frame number of the video from which the conditioning starts. Must be a multiple of 8.',
        }),
      )
      .default(0),
    limit_num_frames: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'Whether to limit the number of frames used from the video. If True, the `max_num_frames` parameter will be used to limit the number of frames.',
        }),
      )
      .default(false),
    resample_fps: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'Whether to resample the video to a specific FPS. If True, the `target_fps` parameter will be used to resample the video.',
        }),
      )
      .default(false),
    strength: z
      .optional(
        z.number().gte(0).lte(1).register(z.globalRegistry, {
          description:
            'Strength of the conditioning. 0.0 means no conditioning, 1.0 means full conditioning.',
        }),
      )
      .default(1),
    target_fps: z
      .optional(
        z.int().gte(1).lte(60).register(z.globalRegistry, {
          description:
            'Target FPS to resample the video to. Only relevant if `resample_fps` is True.',
        }),
      )
      .default(30),
    max_num_frames: z
      .optional(
        z.int().gte(1).lte(161).register(z.globalRegistry, {
          description:
            'Maximum number of frames to use from the video. If None, all frames will be used.',
        }),
      )
      .default(121),
    conditioning_type: z.optional(
      z.enum(['rgb', 'depth', 'pose', 'canny']).register(z.globalRegistry, {
        description:
          'Type of conditioning this video provides. This is relevant to ensure in-context LoRA weights are applied correctly, as well as selecting the correct preprocessing pipeline, when enabled.',
      }),
    ),
    preprocess: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'Whether to preprocess the video. If True, the video will be preprocessed to match the conditioning type. This is a no-op for RGB conditioning.',
        }),
      )
      .default(false),
  })

/**
 * File
 */
export const zFalAiLtxVideo13bDistilledMulticonditioningFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * MultiConditioningVideoOutput
 */
export const zLtxVideo13bDistilledMulticonditioningOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for generation.',
  }),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiLtxVideo13bDistilledMulticonditioningFile,
})

/**
 * VideoConditioningInput
 */
export const zFalAiLtxVideo13bDevMulticonditioningVideoConditioningInput =
  z.object({
    video_url: z.string().register(z.globalRegistry, {
      description: 'URL of video to use as conditioning',
    }),
    start_frame_num: z
      .optional(
        z.int().gte(0).lte(160).register(z.globalRegistry, {
          description:
            'Frame number of the video from which the conditioning starts. Must be a multiple of 8.',
        }),
      )
      .default(0),
    reverse_video: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'Whether to reverse the video. This is useful for tasks where the video conditioning should be applied in reverse order.',
        }),
      )
      .default(false),
    limit_num_frames: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'Whether to limit the number of frames used from the video. If True, the `max_num_frames` parameter will be used to limit the number of frames.',
        }),
      )
      .default(false),
    resample_fps: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'Whether to resample the video to a specific FPS. If True, the `target_fps` parameter will be used to resample the video.',
        }),
      )
      .default(false),
    strength: z
      .optional(
        z.number().gte(0).lte(1).register(z.globalRegistry, {
          description:
            'Strength of the conditioning. 0.0 means no conditioning, 1.0 means full conditioning.',
        }),
      )
      .default(1),
    target_fps: z
      .optional(
        z.int().gte(1).lte(60).register(z.globalRegistry, {
          description:
            'Target FPS to resample the video to. Only relevant if `resample_fps` is True.',
        }),
      )
      .default(30),
    max_num_frames: z
      .optional(
        z.int().gte(1).lte(161).register(z.globalRegistry, {
          description:
            'Maximum number of frames to use from the video. If None, all frames will be used.',
        }),
      )
      .default(121),
    conditioning_type: z.optional(
      z.enum(['rgb', 'depth', 'pose', 'canny']).register(z.globalRegistry, {
        description:
          'Type of conditioning this video provides. This is relevant to ensure in-context LoRA weights are applied correctly, as well as selecting the correct preprocessing pipeline, when enabled.',
      }),
    ),
    preprocess: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'Whether to preprocess the video. If True, the video will be preprocessed to match the conditioning type. This is a no-op for RGB conditioning.',
        }),
      )
      .default(false),
  })

/**
 * File
 */
export const zFalAiLtxVideo13bDevMulticonditioningFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * MultiConditioningVideoOutput
 */
export const zLtxVideo13bDevMulticonditioningOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for generation.',
  }),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiLtxVideo13bDevMulticonditioningFile,
})

/**
 * VideoConditioningInput
 */
export const zFalAiLtxVideo13bDevExtendVideoConditioningInput = z.object({
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of video to use as conditioning',
  }),
  start_frame_num: z
    .optional(
      z.int().gte(0).lte(160).register(z.globalRegistry, {
        description:
          'Frame number of the video from which the conditioning starts. Must be a multiple of 8.',
      }),
    )
    .default(0),
  reverse_video: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to reverse the video. This is useful for tasks where the video conditioning should be applied in reverse order.',
      }),
    )
    .default(false),
  limit_num_frames: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to limit the number of frames used from the video. If True, the `max_num_frames` parameter will be used to limit the number of frames.',
      }),
    )
    .default(false),
  resample_fps: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to resample the video to a specific FPS. If True, the `target_fps` parameter will be used to resample the video.',
      }),
    )
    .default(false),
  strength: z
    .optional(
      z.number().gte(0).lte(1).register(z.globalRegistry, {
        description:
          'Strength of the conditioning. 0.0 means no conditioning, 1.0 means full conditioning.',
      }),
    )
    .default(1),
  target_fps: z
    .optional(
      z.int().gte(1).lte(60).register(z.globalRegistry, {
        description:
          'Target FPS to resample the video to. Only relevant if `resample_fps` is True.',
      }),
    )
    .default(30),
  max_num_frames: z
    .optional(
      z.int().gte(1).lte(161).register(z.globalRegistry, {
        description:
          'Maximum number of frames to use from the video. If None, all frames will be used.',
      }),
    )
    .default(121),
  conditioning_type: z.optional(
    z.enum(['rgb', 'depth', 'pose', 'canny']).register(z.globalRegistry, {
      description:
        'Type of conditioning this video provides. This is relevant to ensure in-context LoRA weights are applied correctly, as well as selecting the correct preprocessing pipeline, when enabled.',
    }),
  ),
  preprocess: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to preprocess the video. If True, the video will be preprocessed to match the conditioning type. This is a no-op for RGB conditioning.',
      }),
    )
    .default(false),
})

/**
 * File
 */
export const zFalAiLtxVideo13bDevExtendFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * ExtendVideoOutput
 */
export const zLtxVideo13bDevExtendOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for generation.',
  }),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiLtxVideo13bDevExtendFile,
})

/**
 * LoRAWeight
 *
 * LoRA weight to use for generation.
 */
export const zLoRaWeight = z
  .object({
    path: z.string().register(z.globalRegistry, {
      description: 'URL or path to the LoRA weights.',
    }),
    scale: z
      .optional(
        z.number().gte(0).lte(4).register(z.globalRegistry, {
          description:
            'Scale of the LoRA weight. This is a multiplier applied to the LoRA weight when loading it.',
        }),
      )
      .default(1),
    weight_name: z.optional(
      z.string().register(z.globalRegistry, {
        description:
          'Name of the LoRA weight. Only used if `path` is a HuggingFace repository, and is only required when the repository contains multiple LoRA weights.',
      }),
    ),
  })
  .register(z.globalRegistry, {
    description: 'LoRA weight to use for generation.',
  })

/**
 * ImageCondition
 *
 * Image condition to use for generation.
 */
export const zImageCondition = z
  .object({
    strength: z
      .optional(
        z.number().gte(0).lte(1).register(z.globalRegistry, {
          description: 'The strength of the condition.',
        }),
      )
      .default(1),
    start_frame_number: z
      .optional(
        z.int().gte(0).lte(160).register(z.globalRegistry, {
          description: 'The frame number to start the condition on.',
        }),
      )
      .default(0),
    image_url: z.string().register(z.globalRegistry, {
      description: 'The URL of the image to use as input.',
    }),
  })
  .register(z.globalRegistry, {
    description: 'Image condition to use for generation.',
  })

/**
 * VideoCondition
 *
 * Video condition to use for generation.
 */
export const zVideoCondition = z
  .object({
    strength: z
      .optional(
        z.number().gte(0).lte(1).register(z.globalRegistry, {
          description: 'The strength of the condition.',
        }),
      )
      .default(1),
    start_frame_number: z
      .optional(
        z.int().gte(0).lte(160).register(z.globalRegistry, {
          description: 'The frame number to start the condition on.',
        }),
      )
      .default(0),
    video_url: z.string().register(z.globalRegistry, {
      description: 'The URL of the video to use as input.',
    }),
  })
  .register(z.globalRegistry, {
    description: 'Video condition to use for generation.',
  })

/**
 * MulticonditioningVideoInput
 *
 * Request model for text-to-video generation with multiple conditions.
 */
export const zLtxVideoLoraMulticonditioningInput = z
  .object({
    number_of_steps: z
      .optional(
        z.int().gte(1).lte(50).register(z.globalRegistry, {
          description: 'The number of inference steps to use.',
        }),
      )
      .default(30),
    prompt: z.string().register(z.globalRegistry, {
      description: 'The prompt to generate the video from.',
    }),
    reverse_video: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description: 'Whether to reverse the video.',
        }),
      )
      .default(false),
    frame_rate: z
      .optional(
        z.int().gte(1).lte(60).register(z.globalRegistry, {
          description: 'The frame rate of the video.',
        }),
      )
      .default(25),
    expand_prompt: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description: 'Whether to expand the prompt using the LLM.',
        }),
      )
      .default(false),
    number_of_frames: z
      .optional(
        z.int().gte(9).lte(161).register(z.globalRegistry, {
          description: 'The number of frames in the video.',
        }),
      )
      .default(89),
    loras: z
      .optional(
        z.array(zLoRaWeight).register(z.globalRegistry, {
          description: 'The LoRA weights to use for generation.',
        }),
      )
      .default([]),
    images: z
      .optional(
        z.array(zImageCondition).register(z.globalRegistry, {
          description: 'The image conditions to use for generation.',
        }),
      )
      .default([]),
    enable_safety_checker: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description: 'Whether to enable the safety checker.',
        }),
      )
      .default(true),
    negative_prompt: z
      .optional(
        z.string().register(z.globalRegistry, {
          description: 'The negative prompt to use.',
        }),
      )
      .default(
        'blurry, low quality, low resolution, inconsistent motion, jittery, distorted',
      ),
    aspect_ratio: z.optional(
      z.enum(['16:9', '1:1', '9:16', 'auto']).register(z.globalRegistry, {
        description: 'The aspect ratio of the video.',
      }),
    ),
    resolution: z.optional(
      z.enum(['480p', '720p']).register(z.globalRegistry, {
        description: 'The resolution of the video.',
      }),
    ),
    videos: z
      .optional(
        z.array(zVideoCondition).register(z.globalRegistry, {
          description: 'The video conditions to use for generation.',
        }),
      )
      .default([]),
    seed: z.optional(
      z.int().register(z.globalRegistry, {
        description: 'The seed to use for generation.',
      }),
    ),
  })
  .register(z.globalRegistry, {
    description:
      'Request model for text-to-video generation with multiple conditions.',
  })

/**
 * File
 */
export const zFalAiLtxVideoLoraMulticonditioningFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * MulticonditioningVideoOutput
 */
export const zLtxVideoLoraMulticonditioningOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for generation.',
  }),
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiLtxVideoLoraMulticonditioningFile,
})

/**
 * MagiVideoExtensionRequest
 */
export const zMagiExtendVideoInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The text prompt to guide video generation.',
  }),
  resolution: z.optional(
    z.enum(['480p', '720p']).register(z.globalRegistry, {
      description:
        'Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.',
    }),
  ),
  aspect_ratio: z.optional(
    z.enum(['auto', '16:9', '9:16', '1:1']).register(z.globalRegistry, {
      description:
        "Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.",
    }),
  ),
  video_url: z.string().register(z.globalRegistry, {
    description:
      'URL of the input video to represent the beginning of the video. If the input video does not match the chosen aspect ratio, it is resized and center cropped.',
  }),
  start_frame: z.optional(
    z.int().gte(0).register(z.globalRegistry, {
      description:
        'The frame to begin the generation from, with the remaining frames will be treated as the prefix video. The final video will contain the frames up until this number unchanged, followed by the generated frames. The default start frame is 32 frames before the end of the video, which gives optimal results.',
    }),
  ),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If set to true, the safety checker will be enabled.',
      }),
    )
    .default(true),
  num_inference_steps: z.optional(
    z
      .union([
        z.literal(4),
        z.literal(8),
        z.literal(16),
        z.literal(32),
        z.literal(64),
      ])
      .register(z.globalRegistry, {
        description:
          'Number of inference steps for sampling. Higher values give better quality but take longer.',
      }),
  ),
  seed: z.optional(
    z.int().register(z.globalRegistry, {
      description:
        'Random seed for reproducibility. If None, a random seed is chosen.',
    }),
  ),
  num_frames: z
    .optional(
      z.int().gte(96).lte(192).register(z.globalRegistry, {
        description:
          'Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit.',
      }),
    )
    .default(96),
})

/**
 * File
 */
export const zFalAiMagiExtendVideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * MagiVideoExtensionResponse
 */
export const zMagiExtendVideoOutput = z.object({
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiMagiExtendVideoFile,
})

/**
 * MagiVideoExtensionRequest
 */
export const zMagiDistilledExtendVideoInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The text prompt to guide video generation.',
  }),
  resolution: z.optional(
    z.enum(['480p', '720p']).register(z.globalRegistry, {
      description:
        'Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.',
    }),
  ),
  aspect_ratio: z.optional(
    z.enum(['auto', '16:9', '9:16', '1:1']).register(z.globalRegistry, {
      description:
        "Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.",
    }),
  ),
  video_url: z.string().register(z.globalRegistry, {
    description:
      'URL of the input video to represent the beginning of the video. If the input video does not match the chosen aspect ratio, it is resized and center cropped.',
  }),
  start_frame: z.optional(
    z.int().gte(0).register(z.globalRegistry, {
      description:
        'The frame to begin the generation from, with the remaining frames will be treated as the prefix video. The final video will contain the frames up until this number unchanged, followed by the generated frames. The default start frame is 32 frames before the end of the video, which gives optimal results.',
    }),
  ),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If set to true, the safety checker will be enabled.',
      }),
    )
    .default(true),
  num_inference_steps: z.optional(
    z
      .union([z.literal(4), z.literal(8), z.literal(16), z.literal(32)])
      .register(z.globalRegistry, {
        description:
          'Number of inference steps for sampling. Higher values give better quality but take longer.',
      }),
  ),
  seed: z.optional(
    z.int().register(z.globalRegistry, {
      description:
        'Random seed for reproducibility. If None, a random seed is chosen.',
    }),
  ),
  num_frames: z
    .optional(
      z.int().gte(96).lte(192).register(z.globalRegistry, {
        description:
          'Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit.',
      }),
    )
    .default(96),
})

/**
 * File
 */
export const zFalAiMagiDistilledExtendVideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * MagiVideoExtensionResponse
 */
export const zMagiDistilledExtendVideoOutput = z.object({
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiMagiDistilledExtendVideoFile,
})

/**
 * WanT2VRequest
 */
export const zWanVaceInput = z.object({
  shift: z
    .optional(
      z.number().gte(1).lte(10).register(z.globalRegistry, {
        description: 'Shift parameter for video generation.',
      }),
    )
    .default(5),
  video_url: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'URL to the source video file. If provided, the model will use this video as a reference.',
    }),
  ),
  prompt: z.string().register(z.globalRegistry, {
    description: 'The text prompt to guide video generation.',
  }),
  ref_image_urls: z.optional(
    z.array(z.string()).register(z.globalRegistry, {
      description:
        'Urls to source reference image. If provided, the model will use this image as reference.',
    }),
  ),
  task: z.optional(
    z.enum(['depth', 'inpainting']).register(z.globalRegistry, {
      description: 'Task type for the model.',
    }),
  ),
  frames_per_second: z
    .optional(
      z.int().gte(5).lte(24).register(z.globalRegistry, {
        description:
          'Frames per second of the generated video. Must be between 5 to 24.',
      }),
    )
    .default(16),
  mask_image_url: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored.',
    }),
  ),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If set to true, the safety checker will be enabled.',
      }),
    )
    .default(false),
  num_frames: z
    .optional(
      z.int().gte(81).lte(240).register(z.globalRegistry, {
        description:
          'Number of frames to generate. Must be between 81 to 100 (inclusive). Works only with only reference images as input if source video or mask video is provided output len would be same as source up to 241 frames',
      }),
    )
    .default(81),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'Negative prompt for video generation.',
      }),
    )
    .default(
      'bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards',
    ),
  aspect_ratio: z.optional(
    z.enum(['auto', '9:16', '16:9']).register(z.globalRegistry, {
      description: 'Aspect ratio of the generated video (16:9 or 9:16).',
    }),
  ),
  resolution: z.optional(
    z.enum(['480p', '580p', '720p']).register(z.globalRegistry, {
      description: 'Resolution of the generated video (480p,580p, or 720p).',
    }),
  ),
  mask_video_url: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'URL to the source mask file. If provided, the model will use this mask as a reference.',
    }),
  ),
  seed: z.optional(
    z.int().register(z.globalRegistry, {
      description:
        'Random seed for reproducibility. If None, a random seed is chosen.',
    }),
  ),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(40).register(z.globalRegistry, {
        description:
          'Number of inference steps for sampling. Higher values give better quality but take longer.',
      }),
    )
    .default(30),
  preprocess: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to preprocess the input video.',
      }),
    )
    .default(false),
  enable_prompt_expansion: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to enable prompt expansion.',
      }),
    )
    .default(false),
})

/**
 * File
 */
export const zFalAiWanVaceFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * WanT2VResponse
 */
export const zWanVaceOutput = z.object({
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiWanVaceFile,
})

/**
 * Video
 *
 * Represents a video file.
 */
export const zCassetteaiVideoSoundEffectsGeneratorVideo = z
  .object({
    file_size: z.optional(z.union([z.int(), z.unknown()])),
    file_name: z.optional(z.union([z.string(), z.unknown()])),
    content_type: z.optional(z.union([z.string(), z.unknown()])),
    url: z.string().register(z.globalRegistry, {
      description: 'The URL where the file can be downloaded from.',
    }),
    file_data: z.optional(z.union([z.string(), z.unknown()])),
  })
  .register(z.globalRegistry, {
    description: 'Represents a video file.',
  })

/**
 * VideoInput
 *
 * Pydantic model for receiving a video file to analyze and re-sound.
 */
export const zVideoSoundEffectsGeneratorInput = z
  .object({
    video_url: zCassetteaiVideoSoundEffectsGeneratorVideo,
  })
  .register(z.globalRegistry, {
    description:
      'Pydantic model for receiving a video file to analyze and re-sound.',
  })

/**
 * File
 */
export const zCassetteaiVideoSoundEffectsGeneratorFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * VideoOutput
 *
 * Pydantic model for returning the re-sounded video back to the client.
 */
export const zVideoSoundEffectsGeneratorOutput = z
  .object({
    video: zCassetteaiVideoSoundEffectsGeneratorFile,
  })
  .register(z.globalRegistry, {
    description:
      'Pydantic model for returning the re-sounded video back to the client.',
  })

/**
 * LipSyncV2Input
 */
export const zSyncLipsyncV2Input = z.object({
  model: z.optional(
    z.enum(['lipsync-2', 'lipsync-2-pro']).register(z.globalRegistry, {
      description:
        'The model to use for lipsyncing. `lipsync-2-pro` will cost roughly 1.67 times as much as `lipsync-2` for the same duration.',
    }),
  ),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the input video',
  }),
  sync_mode: z.optional(
    z
      .enum(['cut_off', 'loop', 'bounce', 'silence', 'remap'])
      .register(z.globalRegistry, {
        description:
          'Lipsync mode when audio and video durations are out of sync.',
      }),
  ),
  audio_url: z.string().register(z.globalRegistry, {
    description: 'URL of the input audio',
  }),
})

/**
 * File
 */
export const zFalAiSyncLipsyncV2File = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * LipSyncV2Output
 */
export const zSyncLipsyncV2Output = z.object({
  video: zFalAiSyncLipsyncV2File,
})

/**
 * Input
 */
export const zLatentsyncInput = z.object({
  video_url: z.string().register(z.globalRegistry, {
    description: 'The URL of the video to generate the lip sync for.',
  }),
  guidance_scale: z
    .optional(
      z.number().gte(1).lte(2).register(z.globalRegistry, {
        description: 'Guidance scale for the model inference',
      }),
    )
    .default(1),
  seed: z.optional(
    z.int().register(z.globalRegistry, {
      description:
        'Random seed for generation. If None, a random seed will be used.',
    }),
  ),
  audio_url: z.string().register(z.globalRegistry, {
    description: 'The URL of the audio to generate the lip sync for.',
  }),
  loop_mode: z.optional(
    z.enum(['pingpong', 'loop']).register(z.globalRegistry, {
      description:
        'Video loop mode when audio is longer than video. Options: pingpong, loop',
    }),
  ),
})

/**
 * File
 */
export const zFalAiLatentsyncFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * Output
 */
export const zLatentsyncOutput = z.object({
  video: zFalAiLatentsyncFile,
})

/**
 * PikadditionsRequest
 *
 * Request model for Pikadditions endpoint
 */
export const zPikaV2PikadditionsInput = z
  .object({
    prompt: z.optional(
      z.string().register(z.globalRegistry, {
        description: 'Text prompt describing what to add',
      }),
    ),
    video_url: z.string().register(z.globalRegistry, {
      description: 'URL of the input video',
    }),
    seed: z.optional(
      z.int().register(z.globalRegistry, {
        description: 'The seed for the random number generator',
      }),
    ),
    negative_prompt: z.optional(
      z.string().register(z.globalRegistry, {
        description: 'Negative prompt to guide the model',
      }),
    ),
    image_url: z.string().register(z.globalRegistry, {
      description: 'URL of the image to add',
    }),
  })
  .register(z.globalRegistry, {
    description: 'Request model for Pikadditions endpoint',
  })

/**
 * File
 */
export const zFalAiPikaV2PikadditionsFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * PikadditionsOutput
 *
 * Output from Pikadditions generation
 */
export const zPikaV2PikadditionsOutput = z
  .object({
    video: zFalAiPikaV2PikadditionsFile,
  })
  .register(z.globalRegistry, {
    description: 'Output from Pikadditions generation',
  })

/**
 * VideoConditioningInput
 */
export const zFalAiLtxVideoV095MulticonditioningVideoConditioningInput =
  z.object({
    video_url: z.string().register(z.globalRegistry, {
      description: 'URL of video to be extended',
    }),
    start_frame_num: z.int().gte(0).lte(120).register(z.globalRegistry, {
      description:
        'Frame number of the video from which the conditioning starts. Must be a multiple of 8.',
    }),
  })

/**
 * File
 */
export const zFalAiLtxVideoV095MulticonditioningFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * MulticonditioningVideoOutput
 */
export const zLtxVideoV095MulticonditioningOutput = z.object({
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiLtxVideoV095MulticonditioningFile,
})

/**
 * VideoConditioningInput
 */
export const zFalAiLtxVideoV095ExtendVideoConditioningInput = z.object({
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of video to be extended',
  }),
  start_frame_num: z.int().gte(0).lte(120).register(z.globalRegistry, {
    description:
      'Frame number of the video from which the conditioning starts. Must be a multiple of 8.',
  }),
})

/**
 * File
 */
export const zFalAiLtxVideoV095ExtendFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * ExtendVideoOutput
 */
export const zLtxVideoV095ExtendOutput = z.object({
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generation.',
  }),
  video: zFalAiLtxVideoV095ExtendFile,
})

/**
 * VideoUpscaleRequest
 */
export const zTopazUpscaleVideoInput = z.object({
  H264_output: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to use H264 codec for output video. Default is H265.',
      }),
    )
    .default(false),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the video to upscale',
  }),
  upscale_factor: z
    .optional(
      z.number().gte(1).lte(4).register(z.globalRegistry, {
        description:
          'Factor to upscale the video by (e.g. 2.0 doubles width and height)',
      }),
    )
    .default(2),
  target_fps: z.optional(
    z.int().gte(16).lte(60).register(z.globalRegistry, {
      description:
        'Target FPS for frame interpolation. If set, frame interpolation will be enabled.',
    }),
  ),
})

/**
 * File
 */
export const zFalAiTopazUpscaleVideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * VideoUpscaleOutput
 */
export const zTopazUpscaleVideoOutput = z.object({
  video: zFalAiTopazUpscaleVideoFile,
})

/**
 * Ben2InputVideo
 */
export const zBenV2VideoInput = z.object({
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of video to be used for background removal.',
  }),
  seed: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'Random seed for reproducible generation.',
    }),
  ),
  background_color: z.optional(
    z
      .tuple([z.unknown(), z.unknown(), z.unknown()])
      .register(z.globalRegistry, {
        description:
          'Optional RGB values (0-255) for the background color. If not provided, the background will be transparent. For ex: [0, 0, 0]',
      }),
  ),
})

/**
 * File
 */
export const zFalAiBenV2VideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * Ben2OutputVideo
 */
export const zBenV2VideoOutput = z.object({
  seed: z.int().register(z.globalRegistry, {
    description:
      '\n            Seed of the generated Image. It will be the same value of the one passed in the\n            input or the randomly generated that was used in case none was passed.\n        ',
  }),
  video: zFalAiBenV2VideoFile,
})

/**
 * HunyuanV2VRequest
 */
export const zHunyuanVideoVideoToVideoInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt to generate the video from.',
  }),
  aspect_ratio: z.optional(
    z.enum(['16:9', '9:16']).register(z.globalRegistry, {
      description: 'The aspect ratio of the video to generate.',
    }),
  ),
  resolution: z.optional(
    z.enum(['480p', '580p', '720p']).register(z.globalRegistry, {
      description: 'The resolution of the video to generate.',
    }),
  ),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the video input.',
  }),
  strength: z
    .optional(
      z.number().gte(0.01).lte(1).register(z.globalRegistry, {
        description: 'Strength for Video-to-Video',
      }),
    )
    .default(0.85),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If set to true, the safety checker will be enabled.',
      }),
    )
    .default(false),
  num_inference_steps: z
    .optional(
      z.int().gte(2).lte(30).register(z.globalRegistry, {
        description:
          'The number of inference steps to run. Lower gets faster results, higher gets better results.',
      }),
    )
    .default(30),
  seed: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The seed to use for generating the video.',
    }),
  ),
  num_frames: z.optional(
    z.enum(['129', '85']).register(z.globalRegistry, {
      description: 'The number of frames to generate.',
    }),
  ),
  pro_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.',
      }),
    )
    .default(false),
})

/**
 * File
 */
export const zFalAiHunyuanVideoVideoToVideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * HunyuanT2VResponse
 */
export const zHunyuanVideoVideoToVideoOutput = z.object({
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generating the video.',
  }),
  video: zFalAiHunyuanVideoVideoToVideoFile,
})

/**
 * LoraWeight
 */
export const zLoraWeight = z.object({
  path: z.string().register(z.globalRegistry, {
    description: 'URL or the path to the LoRA weights.',
  }),
  scale: z
    .optional(
      z.number().gte(0).lte(4).register(z.globalRegistry, {
        description:
          '\n            The scale of the LoRA weight. This is used to scale the LoRA weight\n            before merging it with the base model.\n        ',
      }),
    )
    .default(1),
})

/**
 * HunyuanV2VRequest
 */
export const zHunyuanVideoLoraVideoToVideoInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt to generate the video from.',
  }),
  aspect_ratio: z.optional(
    z.enum(['16:9', '9:16']).register(z.globalRegistry, {
      description: 'The aspect ratio of the video to generate.',
    }),
  ),
  resolution: z.optional(
    z.enum(['480p', '580p', '720p']).register(z.globalRegistry, {
      description: 'The resolution of the video to generate.',
    }),
  ),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the video',
  }),
  loras: z
    .optional(
      z.array(zLoraWeight).register(z.globalRegistry, {
        description:
          '\n            The LoRAs to use for the image generation. You can use any number of LoRAs\n            and they will be merged together to generate the final image.\n        ',
      }),
    )
    .default([]),
  strength: z
    .optional(
      z.number().gte(0.01).lte(1).register(z.globalRegistry, {
        description: 'Strength of video-to-video',
      }),
    )
    .default(0.75),
  enable_safety_checker: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If set to true, the safety checker will be enabled.',
      }),
    )
    .default(false),
  seed: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The seed to use for generating the video.',
    }),
  ),
  num_frames: z.optional(
    z.enum(['129', '85']).register(z.globalRegistry, {
      description: 'The number of frames to generate.',
    }),
  ),
  pro_mode: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.',
      }),
    )
    .default(false),
})

/**
 * File
 */
export const zFalAiHunyuanVideoLoraVideoToVideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * HunyuanV2VResponse
 */
export const zHunyuanVideoLoraVideoToVideoOutput = z.object({
  seed: z.int().register(z.globalRegistry, {
    description: 'The seed used for generating the video.',
  }),
  video: zFalAiHunyuanVideoLoraVideoToVideoFile,
})

/**
 * Keyframe
 */
export const zKeyframe = z.object({
  duration: z.number().register(z.globalRegistry, {
    description: 'The duration in milliseconds of this keyframe',
  }),
  timestamp: z.number().register(z.globalRegistry, {
    description: 'The timestamp in milliseconds where this keyframe starts',
  }),
  url: z.string().register(z.globalRegistry, {
    description: "The URL where this keyframe's media file can be accessed",
  }),
})

/**
 * Track
 */
export const zTrack = z.object({
  type: z.string().register(z.globalRegistry, {
    description: "Type of track ('video' or 'audio')",
  }),
  id: z.string().register(z.globalRegistry, {
    description: 'Unique identifier for the track',
  }),
  keyframes: z.array(zKeyframe).register(z.globalRegistry, {
    description: 'List of keyframes that make up this track',
  }),
})

/**
 * Input
 */
export const zFfmpegApiComposeInput = z.object({
  tracks: z.array(zTrack).register(z.globalRegistry, {
    description: 'List of tracks to be combined into the final media',
  }),
})

/**
 * ComposeOutput
 */
export const zFfmpegApiComposeOutput = z.object({
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the processed video file',
  }),
  thumbnail_url: z.string().register(z.globalRegistry, {
    description: "URL of the video's thumbnail image",
  }),
})

/**
 * LipSyncInput
 */
export const zSyncLipsyncInput = z.object({
  model: z.optional(
    z
      .enum(['lipsync-1.8.0', 'lipsync-1.7.1', 'lipsync-1.9.0-beta'])
      .register(z.globalRegistry, {
        description: 'The model to use for lipsyncing',
      }),
  ),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the input video',
  }),
  sync_mode: z.optional(
    z
      .enum(['cut_off', 'loop', 'bounce', 'silence', 'remap'])
      .register(z.globalRegistry, {
        description:
          'Lipsync mode when audio and video durations are out of sync.',
      }),
  ),
  audio_url: z.string().register(z.globalRegistry, {
    description: 'URL of the input audio',
  }),
})

/**
 * File
 */
export const zFalAiSyncLipsyncFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * LipSyncOutput
 */
export const zSyncLipsyncOutput = z.object({
  video: zFalAiSyncLipsyncFile,
})

/**
 * CaptionInput
 */
export const zAutoCaptionInput = z.object({
  txt_font: z
    .optional(
      z.string().register(z.globalRegistry, {
        description:
          "Font for generated captions. Choose one in 'Arial','Standard','Garamond', 'Times New Roman','Georgia', or pass a url to a .ttf file",
      }),
    )
    .default('Standard'),
  video_url: z.string().register(z.globalRegistry, {
    description:
      'URL to the .mp4 video with audio. Only videos of size <100MB are allowed.',
  }),
  top_align: z.optional(z.union([z.string(), z.number()])),
  txt_color: z
    .optional(
      z.string().register(z.globalRegistry, {
        description:
          'Colour of the text. Can be a RGB tuple, a color name, or an hexadecimal notation.',
      }),
    )
    .default('white'),
  stroke_width: z
    .optional(
      z.int().register(z.globalRegistry, {
        description: 'Width of the text strokes in pixels',
      }),
    )
    .default(1),
  refresh_interval: z
    .optional(
      z.number().gte(0.5).lte(3).register(z.globalRegistry, {
        description:
          'Number of seconds the captions should stay on screen. A higher number will also result in more text being displayed at once.',
      }),
    )
    .default(1.5),
  font_size: z
    .optional(
      z.int().register(z.globalRegistry, {
        description: 'Size of text in generated captions.',
      }),
    )
    .default(24),
  left_align: z.optional(z.union([z.string(), z.number()])),
})

/**
 * Output
 */
export const zAutoCaptionOutput = z.object({
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL to the caption .mp4 video.',
  }),
})

/**
 * InputModel
 */
export const zDubbingInput = z.object({
  do_lipsync: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to lip sync the audio to the video',
      }),
    )
    .default(true),
  video_url: z.string().register(z.globalRegistry, {
    description: 'Input video URL to be dubbed.',
  }),
  target_language: z.optional(
    z.enum(['hindi', 'turkish', 'english']).register(z.globalRegistry, {
      description: 'Target language to dub the video to',
    }),
  ),
})

/**
 * File
 */
export const zFalAiDubbingFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * OutputModel
 */
export const zDubbingOutput = z.object({
  video: zFalAiDubbingFile,
})

/**
 * Input
 */
export const zVideoUpscalerInput = z.object({
  video_url: z.string().register(z.globalRegistry, {
    description: 'The URL of the video to upscale',
  }),
  scale: z
    .optional(
      z.number().gte(1).lte(8).register(z.globalRegistry, {
        description: 'The scale factor',
      }),
    )
    .default(2),
})

/**
 * File
 */
export const zFalAiVideoUpscalerFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * Output
 */
export const zVideoUpscalerOutput = z.object({
  video: zFalAiVideoUpscalerFile,
})

/**
 * LoraWeight
 */
export const zFalAiCogvideox5bVideoToVideoLoraWeight = z.object({
  path: z.string().register(z.globalRegistry, {
    description: 'URL or the path to the LoRA weights.',
  }),
  scale: z
    .optional(
      z.number().gte(0).lte(4).register(z.globalRegistry, {
        description:
          '\n            The scale of the LoRA weight. This is used to scale the LoRA weight\n            before merging it with the base model.\n        ',
      }),
    )
    .default(1),
})

/**
 * ImageSize
 */
export const zFalAiCogvideox5bVideoToVideoImageSize = z.object({
  height: z
    .optional(
      z.int().lte(14142).register(z.globalRegistry, {
        description: 'The height of the generated image.',
      }),
    )
    .default(512),
  width: z
    .optional(
      z.int().lte(14142).register(z.globalRegistry, {
        description: 'The width of the generated image.',
      }),
    )
    .default(512),
})

/**
 * VideoToVideoInput
 */
export const zCogvideox5bVideoToVideoInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt to generate the video from.',
  }),
  video_url: z.string().register(z.globalRegistry, {
    description: 'The video to generate the video from.',
  }),
  use_rife: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Use RIFE for video interpolation',
      }),
    )
    .default(true),
  loras: z
    .optional(
      z
        .array(zFalAiCogvideox5bVideoToVideoLoraWeight)
        .register(z.globalRegistry, {
          description:
            '\n            The LoRAs to use for the image generation. We currently support one lora.\n        ',
        }),
    )
    .default([]),
  video_size: z.optional(
    z.union([
      zFalAiCogvideox5bVideoToVideoImageSize,
      z.enum([
        'square_hd',
        'square',
        'portrait_4_3',
        'portrait_16_9',
        'landscape_4_3',
        'landscape_16_9',
      ]),
    ]),
  ),
  strength: z
    .optional(
      z.number().gte(0.05).lte(1).register(z.globalRegistry, {
        description:
          'The strength to use for Video to Video.  1.0 completely remakes the video while 0.0 preserves the original.',
      }),
    )
    .default(0.8),
  guidance_scale: z
    .optional(
      z.number().gte(0).lte(20).register(z.globalRegistry, {
        description:
          '\n            The CFG (Classifier Free Guidance) scale is a measure of how close you want\n            the model to stick to your prompt when looking for a related video to show you.\n        ',
      }),
    )
    .default(7),
  num_inference_steps: z
    .optional(
      z.int().gte(1).lte(50).register(z.globalRegistry, {
        description: 'The number of inference steps to perform.',
      }),
    )
    .default(50),
  export_fps: z
    .optional(
      z.int().gte(4).lte(32).register(z.globalRegistry, {
        description: 'The target FPS of the video',
      }),
    )
    .default(16),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'The negative prompt to generate video from',
      }),
    )
    .default(''),
  seed: z.optional(
    z.int().register(z.globalRegistry, {
      description:
        '\n            The same seed and the same prompt given to the same version of the model\n            will output the same video every time.\n        ',
    }),
  ),
})

/**
 * File
 */
export const zFalAiCogvideox5bVideoToVideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * Output
 */
export const zCogvideox5bVideoToVideoOutput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt used for generating the video.',
  }),
  timings: z.record(z.string(), z.number()),
  seed: z.int().register(z.globalRegistry, {
    description:
      '\n            Seed of the generated video. It will be the same value of the one passed in the\n            input or the randomly generated that was used in case none was passed.\n        ',
  }),
  video: zFalAiCogvideox5bVideoToVideoFile,
})

/**
 * ControlNeXtInput
 */
export const zControlnextInput = z.object({
  controlnext_cond_scale: z
    .optional(
      z.number().gte(0.1).lte(10).register(z.globalRegistry, {
        description: 'Condition scale for ControlNeXt.',
      }),
    )
    .default(1),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the input video.',
  }),
  fps: z
    .optional(
      z.int().gte(1).lte(60).register(z.globalRegistry, {
        description: 'Frames per second for the output video.',
      }),
    )
    .default(7),
  max_frame_num: z
    .optional(
      z.int().gte(1).lte(1000).register(z.globalRegistry, {
        description: 'Maximum number of frames to process.',
      }),
    )
    .default(240),
  width: z
    .optional(
      z.int().gte(64).lte(1024).register(z.globalRegistry, {
        description: 'Width of the output video.',
      }),
    )
    .default(576),
  overlap: z
    .optional(
      z.int().gte(0).lte(20).register(z.globalRegistry, {
        description: 'Number of overlapping frames between batches.',
      }),
    )
    .default(6),
  guidance_scale: z
    .optional(
      z.number().gte(0.1).lte(10).register(z.globalRegistry, {
        description: 'Guidance scale for the diffusion process.',
      }),
    )
    .default(3),
  batch_frames: z
    .optional(
      z.int().gte(1).lte(50).register(z.globalRegistry, {
        description: 'Number of frames to process in each batch.',
      }),
    )
    .default(24),
  height: z
    .optional(
      z.int().gte(64).lte(1024).register(z.globalRegistry, {
        description: 'Height of the output video.',
      }),
    )
    .default(1024),
  sample_stride: z
    .optional(
      z.int().gte(1).lte(10).register(z.globalRegistry, {
        description: 'Stride for sampling frames from the input video.',
      }),
    )
    .default(2),
  image_url: z.string().register(z.globalRegistry, {
    description: 'URL of the reference image.',
  }),
  decode_chunk_size: z
    .optional(
      z.int().gte(1).lte(10).register(z.globalRegistry, {
        description: 'Chunk size for decoding frames.',
      }),
    )
    .default(2),
  motion_bucket_id: z
    .optional(
      z.number().gte(0).lte(255).register(z.globalRegistry, {
        description: 'Motion bucket ID for the pipeline.',
      }),
    )
    .default(127),
  num_inference_steps: z
    .optional(
      z.int().gte(1).lte(100).register(z.globalRegistry, {
        description: 'Number of inference steps.',
      }),
    )
    .default(25),
})

/**
 * File
 */
export const zFalAiControlnextFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * ControlNeXtOutput
 */
export const zControlnextOutput = z.object({
  video: zFalAiControlnextFile,
})

/**
 * PointPrompt
 */
export const zFalAiSam2VideoPointPrompt = z.object({
  y: z
    .optional(
      z.int().register(z.globalRegistry, {
        description: 'Y Coordinate of the prompt',
      }),
    )
    .default(350),
  label: z.optional(
    z.union([z.literal(0), z.literal(1)]).register(z.globalRegistry, {
      description: 'Label of the prompt. 1 for foreground, 0 for background',
    }),
  ),
  frame_index: z
    .optional(
      z.int().register(z.globalRegistry, {
        description: 'The frame index to interact with.',
      }),
    )
    .default(0),
  x: z
    .optional(
      z.int().register(z.globalRegistry, {
        description: 'X Coordinate of the prompt',
      }),
    )
    .default(305),
})

/**
 * BoxPrompt
 */
export const zFalAiSam2VideoBoxPrompt = z.object({
  y_min: z
    .optional(
      z.int().register(z.globalRegistry, {
        description: 'Y Min Coordinate of the box',
      }),
    )
    .default(0),
  frame_index: z
    .optional(
      z.int().register(z.globalRegistry, {
        description: 'The frame index to interact with.',
      }),
    )
    .default(0),
  x_max: z
    .optional(
      z.int().register(z.globalRegistry, {
        description: 'X Max Coordinate of the prompt',
      }),
    )
    .default(0),
  x_min: z
    .optional(
      z.int().register(z.globalRegistry, {
        description: 'X Min Coordinate of the box',
      }),
    )
    .default(0),
  y_max: z
    .optional(
      z.int().register(z.globalRegistry, {
        description: 'Y Max Coordinate of the prompt',
      }),
    )
    .default(0),
})

/**
 * SAM2VideoRLEInput
 */
export const zSam2VideoInput = z.object({
  video_url: z.string().register(z.globalRegistry, {
    description: 'The URL of the video to be segmented.',
  }),
  prompts: z
    .optional(
      z.array(zFalAiSam2VideoPointPrompt).register(z.globalRegistry, {
        description: 'List of prompts to segment the video',
      }),
    )
    .default([]),
  boundingbox_zip: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Return per-frame bounding box overlays as a zip archive.',
      }),
    )
    .default(false),
  mask_url: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The URL of the mask to be applied initially.',
    }),
  ),
  apply_mask: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Apply the mask on the video.',
      }),
    )
    .default(false),
  box_prompts: z
    .optional(
      z.array(zFalAiSam2VideoBoxPrompt).register(z.globalRegistry, {
        description: 'Coordinates for boxes',
      }),
    )
    .default([]),
})

/**
 * File
 */
export const zFalAiSam2VideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * SAM2VideoOutput
 */
export const zSam2VideoOutput = z.object({
  boundingbox_frames_zip: z.optional(zFalAiSam2VideoFile),
  video: zFalAiSam2VideoFile,
})

/**
 * AMTInterpolationInput
 */
export const zAmtInterpolationInput = z.object({
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the video to be processed',
  }),
  recursive_interpolation_passes: z
    .optional(
      z.int().register(z.globalRegistry, {
        description: 'Number of recursive interpolation passes',
      }),
    )
    .default(2),
  output_fps: z
    .optional(
      z.int().register(z.globalRegistry, {
        description: 'Output frames per second',
      }),
    )
    .default(24),
})

/**
 * File
 */
export const zFalAiAmtInterpolationFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * AMTInterpolationOutput
 */
export const zAmtInterpolationOutput = z.object({
  video: zFalAiAmtInterpolationFile,
})

/**
 * AnimateDiffV2VTurboInput
 */
export const zFastAnimatediffTurboVideoToVideoInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description:
      'The prompt to use for generating the image. Be as descriptive as possible for best results.',
  }),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the video.',
  }),
  first_n_seconds: z
    .optional(
      z.int().gte(2).lte(12).register(z.globalRegistry, {
        description: 'The first N number of seconds of video to animate.',
      }),
    )
    .default(3),
  fps: z
    .optional(
      z.int().gte(1).lte(16).register(z.globalRegistry, {
        description: 'Number of frames per second to extract from the video.',
      }),
    )
    .default(8),
  strength: z
    .optional(
      z.number().gte(0.1).lte(1).register(z.globalRegistry, {
        description: 'The strength of the input video in the final output.',
      }),
    )
    .default(0.7),
  guidance_scale: z
    .optional(
      z.number().gte(0).lte(20).register(z.globalRegistry, {
        description:
          'The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.',
      }),
    )
    .default(1),
  num_inference_steps: z
    .optional(
      z.int().gte(1).lte(32).register(z.globalRegistry, {
        description:
          'The number of inference steps to perform. 4-12 is recommended for turbo mode.',
      }),
    )
    .default(8),
  seed: z.optional(
    z.int().register(z.globalRegistry, {
      description:
        '\n            The same seed and the same prompt given to the same version of Stable Diffusion\n            will output the same image every time.\n        ',
    }),
  ),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description:
          "\n            The negative prompt to use. Use it to address details that you don't want\n            in the image. This could be colors, objects, scenery and even the small details\n            (e.g. moustache, blurry, low resolution).\n        ",
      }),
    )
    .default('(bad quality, worst quality:1.2), ugly faces, bad anime'),
  motions: z.optional(
    z
      .array(
        z.enum([
          'zoom-out',
          'zoom-in',
          'pan-left',
          'pan-right',
          'tilt-up',
          'tilt-down',
        ]),
      )
      .register(z.globalRegistry, {
        description: 'The motions to apply to the video.',
      }),
  ),
})

/**
 * File
 */
export const zFalAiFastAnimatediffTurboVideoToVideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * AnimateDiffV2VOutput
 */
export const zFastAnimatediffTurboVideoToVideoOutput = z.object({
  seed: z.int().register(z.globalRegistry, {
    description: 'Seed used for generating the video.',
  }),
  video: zFalAiFastAnimatediffTurboVideoToVideoFile,
})

/**
 * AnimateDiffV2VInput
 */
export const zFastAnimatediffVideoToVideoInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description:
      'The prompt to use for generating the image. Be as descriptive as possible for best results.',
  }),
  video_url: z.string().register(z.globalRegistry, {
    description: 'URL of the video.',
  }),
  first_n_seconds: z
    .optional(
      z.int().gte(2).lte(4).register(z.globalRegistry, {
        description: 'The first N number of seconds of video to animate.',
      }),
    )
    .default(3),
  fps: z
    .optional(
      z.int().gte(1).lte(16).register(z.globalRegistry, {
        description: 'Number of frames per second to extract from the video.',
      }),
    )
    .default(8),
  strength: z
    .optional(
      z.number().gte(0.1).lte(1).register(z.globalRegistry, {
        description: 'The strength of the input video in the final output.',
      }),
    )
    .default(0.7),
  guidance_scale: z
    .optional(
      z.number().gte(0).lte(20).register(z.globalRegistry, {
        description:
          '\n            The CFG (Classifier Free Guidance) scale is a measure of how close you want\n            the model to stick to your prompt when looking for a related image to show you.\n        ',
      }),
    )
    .default(7.5),
  num_inference_steps: z
    .optional(
      z.int().gte(1).lte(50).register(z.globalRegistry, {
        description: 'The number of inference steps to perform.',
      }),
    )
    .default(25),
  seed: z.optional(
    z.int().register(z.globalRegistry, {
      description:
        '\n            The same seed and the same prompt given to the same version of Stable Diffusion\n            will output the same image every time.\n        ',
    }),
  ),
  negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description:
          "\n            The negative prompt to use. Use it to address details that you don't want\n            in the image. This could be colors, objects, scenery and even the small details\n            (e.g. moustache, blurry, low resolution).\n        ",
      }),
    )
    .default('(bad quality, worst quality:1.2), ugly faces, bad anime'),
  motions: z.optional(
    z
      .array(
        z.enum([
          'zoom-out',
          'zoom-in',
          'pan-left',
          'pan-right',
          'tilt-up',
          'tilt-down',
        ]),
      )
      .register(z.globalRegistry, {
        description: 'The motions to apply to the video.',
      }),
  ),
})

/**
 * File
 */
export const zFalAiFastAnimatediffVideoToVideoFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * AnimateDiffV2VOutput
 */
export const zFastAnimatediffVideoToVideoOutput = z.object({
  seed: z.int().register(z.globalRegistry, {
    description: 'Seed used for generating the video.',
  }),
  video: zFalAiFastAnimatediffVideoToVideoFile,
})
