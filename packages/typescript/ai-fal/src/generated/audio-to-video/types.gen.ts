// This file is auto-generated by @hey-api/openapi-ts

export type ClientOptions = {
  baseUrl: 'https://queue.fal.run' | (string & {})
}

/**
 * EchoMimicResponse
 */
export type SchemaEchomimicV3Output = {
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * File
 */
export type SchemaFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * EchoMimicRequest
 */
export type SchemaEchomimicV3Input = {
  /**
   * Prompt
   *
   * The prompt to use for the video generation.
   */
  prompt: string
  /**
   * Audio URL
   *
   * The URL of the audio to use as a reference for the video generation.
   */
  audio_url: string
  /**
   * Image URL
   *
   * The URL of the image to use as a reference for the video generation.
   */
  image_url: string
  /**
   * Guidance Scale
   *
   * The guidance scale to use for the video generation.
   */
  guidance_scale?: number
  /**
   * Audio Guidance Scale
   *
   * The audio guidance scale to use for the video generation.
   */
  audio_guidance_scale?: number
  /**
   * Number of frames per generation
   *
   * The number of frames to generate at once.
   */
  num_frames_per_generation?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to use for the video generation.
   */
  negative_prompt?: string
  /**
   * Seed
   *
   * The seed to use for the video generation.
   */
  seed?: number
}

/**
 * StableAvatarResponse
 */
export type SchemaStableAvatarOutput = {
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * StableAvatarRequest
 */
export type SchemaStableAvatarInput = {
  /**
   * Prompt
   *
   * The prompt to use for the video generation.
   */
  prompt: string
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the video to generate. If 'auto', the aspect ratio will be determined by the reference image.
   */
  aspect_ratio?: '16:9' | '1:1' | '9:16' | 'auto'
  /**
   * Perturbation
   *
   * The amount of perturbation to use for the video generation. 0.0 means no perturbation, 1.0 means full perturbation.
   */
  perturbation?: number
  /**
   * Image URL
   *
   * The URL of the image to use as a reference for the video generation.
   */
  image_url: string
  /**
   * Guidance Scale
   *
   * The guidance scale to use for the video generation.
   */
  guidance_scale?: number
  /**
   * Seed
   *
   * The seed to use for the video generation.
   */
  seed?: number
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use for the video generation.
   */
  num_inference_steps?: number
  /**
   * Audio URL
   *
   * The URL of the audio to use as a reference for the video generation.
   */
  audio_url: string
  /**
   * Audio Guidance Scale
   *
   * The audio guidance scale to use for the video generation.
   */
  audio_guidance_scale?: number
}

/**
 * WanS2VResponse
 */
export type SchemaWanV2214bSpeechToVideoOutput = {
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * WanS2VRequest
 */
export type SchemaWanV2214bSpeechToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt used for video generation.
   */
  prompt: string
  /**
   * Shift
   *
   * Shift value for the video. Must be between 1.0 and 10.0.
   */
  shift?: number
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.
   */
  frames_per_second?: number
  /**
   * Guidance Scale
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guidance_scale?: number
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 40 to 120, (must be multiple of 4).
   */
  num_frames?: number
  /**
   * Enable Safety Checker
   *
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Resolution
   *
   * Resolution of the generated video (480p, 580p, or 720p).
   */
  resolution?: '480p' | '580p' | '720p'
  /**
   * Enable Output Safety Checker
   *
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean
  /**
   * Image URL
   *
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string
  /**
   * Video Quality
   *
   * The quality of the output video. Higher quality means better visual quality but larger file size.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Audio URL
   *
   * The URL of the audio file.
   */
  audio_url: string
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number
}

/**
 * AvatarsAppOutput
 */
export type SchemaAvatarsAudioToVideoOutput = {
  video: SchemaFile
}

/**
 * Audio2VideoInput
 */
export type SchemaAvatarsAudioToVideoInput = {
  /**
   * Audio Url
   */
  audio_url: string
  /**
   * Avatar Id
   *
   * The avatar to use for the video
   */
  avatar_id:
    | 'emily_vertical_primary'
    | 'emily_vertical_secondary'
    | 'marcus_vertical_primary'
    | 'marcus_vertical_secondary'
    | 'mira_vertical_primary'
    | 'mira_vertical_secondary'
    | 'jasmine_vertical_primary'
    | 'jasmine_vertical_secondary'
    | 'jasmine_vertical_walking'
    | 'aisha_vertical_walking'
    | 'elena_vertical_primary'
    | 'elena_vertical_secondary'
    | 'any_male_vertical_primary'
    | 'any_female_vertical_primary'
    | 'any_male_vertical_secondary'
    | 'any_female_vertical_secondary'
    | 'any_female_vertical_walking'
    | 'emily_primary'
    | 'emily_side'
    | 'marcus_primary'
    | 'marcus_side'
    | 'aisha_walking'
    | 'elena_primary'
    | 'elena_side'
    | 'any_male_primary'
    | 'any_female_primary'
    | 'any_male_side'
    | 'any_female_side'
}

/**
 * AudioToVideoResponse
 *
 * Response model for audio-to-video generation (no reference image).
 */
export type SchemaLongcatSingleAvatarAudioToVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * AudioToVideoRequest
 *
 * Request model for audio-to-video generation.
 */
export type SchemaLongcatSingleAvatarAudioToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to guide the video generation.
   */
  prompt?: string
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p). Billing is per video-second (16 frames): 480p is 1 unit per second and 720p is 4 units per second.
   */
  resolution?: '480p' | '720p'
  /**
   * Enable Safety Checker
   *
   * Whether to enable safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Audio Guidance Scale
   *
   * The audio guidance scale. Higher values may lead to exaggerated mouth movements.
   */
  audio_guidance_scale?: number
  /**
   * Number of Segments
   *
   * Number of video segments to generate. Each segment adds ~5 seconds of video. First segment is ~5.8s, additional segments are 5s each.
   */
  num_segments?: number
  /**
   * Audio URL
   *
   * The URL of the audio file to drive the avatar.
   */
  audio_url: string
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to avoid in the video generation.
   */
  negative_prompt?: string
  /**
   * Text Guidance Scale
   *
   * The text guidance scale for classifier-free guidance.
   */
  text_guidance_scale?: number
}

/**
 * ImageAudioToVideoResponse
 *
 * Response model for image+audio to video generation.
 */
export type SchemaLongcatSingleAvatarImageAudioToVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * ImageAudioToVideoRequest
 *
 * Request model for image+audio to video generation.
 */
export type SchemaLongcatSingleAvatarImageAudioToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to guide the video generation.
   */
  prompt: string
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p). Billing is per video-second (16 frames): 480p is 1 unit per second and 720p is 4 units per second.
   */
  resolution?: '480p' | '720p'
  /**
   * Enable Safety Checker
   *
   * Whether to enable safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Audio Guidance Scale
   *
   * The audio guidance scale. Higher values may lead to exaggerated mouth movements.
   */
  audio_guidance_scale?: number
  /**
   * Number of Segments
   *
   * Number of video segments to generate. Each segment adds ~5 seconds of video. First segment is ~5.8s, additional segments are 5s each.
   */
  num_segments?: number
  /**
   * Image URL
   *
   * The URL of the image to animate.
   */
  image_url: string
  /**
   * Audio URL
   *
   * The URL of the audio file to drive the avatar.
   */
  audio_url: string
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to avoid in the video generation.
   */
  negative_prompt?: string
  /**
   * Text Guidance Scale
   *
   * The text guidance scale for classifier-free guidance.
   */
  text_guidance_scale?: number
}

/**
 * BoundingBox
 */
export type SchemaBoundingBox = {
  /**
   * Y
   *
   * Y-coordinate of the top-left corner
   */
  y: number
  /**
   * X
   *
   * X-coordinate of the top-left corner
   */
  x: number
  /**
   * H
   *
   * Height of the bounding box
   */
  h: number
  /**
   * W
   *
   * Width of the bounding box
   */
  w: number
  /**
   * Label
   *
   * Label of the bounding box
   */
  label: string
}

/**
 * MultiSpeakerImageAudioToVideoResponse
 *
 * Response model for multi-speaker image+audio to video generation.
 */
export type SchemaLongcatMultiAvatarImageAudioToVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number
  /**
   * Video
   *
   * The generated video file.
   */
  video: SchemaFile
}

/**
 * MultiSpeakerImageAudioToVideoRequest
 *
 * Request model for multi-speaker image+audio to video generation.
 */
export type SchemaLongcatMultiAvatarImageAudioToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to guide the video generation.
   */
  prompt?: string
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number
  /**
   * Audio URL Person 2
   *
   * The URL of the audio file for person 2 (right side).
   */
  audio_url_person2?: string
  /**
   * Enable Safety Checker
   *
   * Whether to enable safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Bbox Person1
   *
   * Bounding box for person 1. If not provided, defaults to left half of image.
   */
  bbox_person1?: SchemaBoundingBox
  /**
   * Negative Prompt
   *
   * The negative prompt to avoid in the video generation.
   */
  negative_prompt?: string
  /**
   * Text Guidance Scale
   *
   * The text guidance scale for classifier-free guidance.
   */
  text_guidance_scale?: number
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p). Billing is per video-second (16 frames): 480p is 1 unit per second and 720p is 4 units per second.
   */
  resolution?: '480p' | '720p'
  /**
   * Audio Type
   *
   * How to combine the two audio tracks. 'para' (parallel) plays both simultaneously, 'add' (sequential) plays person 1 first then person 2.
   */
  audio_type?: 'para' | 'add'
  /**
   * Image URL
   *
   * The URL of the image containing two speakers.
   */
  image_url: string
  /**
   * Audio URL Person 1
   *
   * The URL of the audio file for person 1 (left side).
   */
  audio_url_person1?: string
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number
  /**
   * Audio Guidance Scale
   *
   * The audio guidance scale. Higher values may lead to exaggerated mouth movements.
   */
  audio_guidance_scale?: number
  /**
   * Bbox Person2
   *
   * Bounding box for person 2. If not provided, defaults to right half of image.
   */
  bbox_person2?: SchemaBoundingBox
  /**
   * Number of Segments
   *
   * Number of video segments to generate. Each segment adds ~5 seconds of video. First segment is ~5.8s, additional segments are 5s each.
   */
  num_segments?: number
}

/**
 * DubbingVideoOutput
 */
export type SchemaElevenlabsDubbingOutput = {
  /**
   * Target Lang
   *
   * The target language of the dubbed content
   */
  target_lang: string
  video: SchemaFile
}

/**
 * DubbingRequest
 */
export type SchemaElevenlabsDubbingInput = {
  /**
   * Video Url
   *
   * URL of the video file to dub. Either audio_url or video_url must be provided. If both are provided, video_url takes priority.
   */
  video_url?: string | unknown
  /**
   * Audio Url
   *
   * URL of the audio file to dub. Either audio_url or video_url must be provided.
   */
  audio_url?: string | unknown
  /**
   * Highest Resolution
   *
   * Whether to use the highest resolution for dubbing.
   */
  highest_resolution?: boolean
  /**
   * Target Lang
   *
   * Target language code for dubbing (ISO 639-1)
   */
  target_lang: string
  /**
   * Source Lang
   *
   * Source language code. If not provided, will be auto-detected.
   */
  source_lang?: string | unknown
  /**
   * Num Speakers
   *
   * Number of speakers in the audio. If not provided, will be auto-detected.
   */
  num_speakers?: number | unknown
}

/**
 * LTX2AudioToVideoOutput
 */
export type SchemaLtx219bAudioToVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * VideoFile
 */
export type SchemaVideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number | unknown
  /**
   * Height
   *
   * The height of the video
   */
  height?: number | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * Width
   *
   * The width of the video
   */
  width?: number | unknown
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number | unknown
}

/**
 * LTX2AudioToVideoInput
 */
export type SchemaLtx219bAudioToVideoInput = {
  /**
   * Match Audio Length
   *
   * When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames.
   */
  match_audio_length?: boolean
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: 'none' | 'regular' | 'high' | 'full'
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | 'dolly_in'
    | 'dolly_out'
    | 'dolly_left'
    | 'dolly_right'
    | 'jib_up'
    | 'jib_down'
    | 'static'
    | 'none'
  /**
   * Video Size
   *
   * The size of the generated video. Use 'auto' to match the input image dimensions if provided.
   */
  video_size?:
    | SchemaImageSize
    | 'auto'
    | 'square_hd'
    | 'square'
    | 'portrait_4_3'
    | 'portrait_16_9'
    | 'landscape_4_3'
    | 'landscape_16_9'
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number
  /**
   * Image Strength
   *
   * The strength of the image to use for the video generation.
   */
  image_strength?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string
  /**
   * Guidance Scale
   *
   * The guidance scale to use.
   */
  guidance_scale?: number
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | 'X264 (.mp4)'
    | 'VP9 (.webm)'
    | 'PRORES4444 (.mov)'
    | 'GIF (.gif)'
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number
  /**
   * Preprocess Audio
   *
   * Whether to preprocess the audio before using it as conditioning.
   */
  preprocess_audio?: boolean
  /**
   * Image URL
   *
   * Optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | unknown
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Audio URL
   *
   * The URL of the audio to generate the video from.
   */
  audio_url: string
  /**
   * Audio Strength
   *
   * Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification.
   */
  audio_strength?: number
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
}

/**
 * ImageSize
 */
export type SchemaImageSize = {
  /**
   * Height
   *
   * The height of the generated image.
   */
  height?: number
  /**
   * Width
   *
   * The width of the generated image.
   */
  width?: number
}

/**
 * LTX2AudioToVideoOutput
 */
export type SchemaLtx219bDistilledAudioToVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * LTX2DistilledAudioToVideoInput
 */
export type SchemaLtx219bDistilledAudioToVideoInput = {
  /**
   * Match Audio Length
   *
   * When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames.
   */
  match_audio_length?: boolean
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: 'none' | 'regular' | 'high' | 'full'
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | 'dolly_in'
    | 'dolly_out'
    | 'dolly_left'
    | 'dolly_right'
    | 'jib_up'
    | 'jib_down'
    | 'static'
    | 'none'
  /**
   * Video Size
   *
   * The size of the generated video. Use 'auto' to match the input image dimensions if provided.
   */
  video_size?:
    | SchemaImageSize
    | 'auto'
    | 'square_hd'
    | 'square'
    | 'portrait_4_3'
    | 'portrait_16_9'
    | 'landscape_4_3'
    | 'landscape_16_9'
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number
  /**
   * Image Strength
   *
   * The strength of the image to use for the video generation.
   */
  image_strength?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | 'X264 (.mp4)'
    | 'VP9 (.webm)'
    | 'PRORES4444 (.mov)'
    | 'GIF (.gif)'
  /**
   * Preprocess Audio
   *
   * Whether to preprocess the audio before using it as conditioning.
   */
  preprocess_audio?: boolean
  /**
   * Image URL
   *
   * Optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | unknown
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Audio URL
   *
   * The URL of the audio to generate the video from.
   */
  audio_url: string
  /**
   * Audio Strength
   *
   * Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification.
   */
  audio_strength?: number
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
}

/**
 * LTX2AudioToVideoOutput
 */
export type SchemaLtx219bAudioToVideoLoraOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * LTX2LoRAAudioToVideoInput
 */
export type SchemaLtx219bAudioToVideoLoraInput = {
  /**
   * Match Audio Length
   *
   * When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames.
   */
  match_audio_length?: boolean
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: 'none' | 'regular' | 'high' | 'full'
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number
  /**
   * LoRAs
   *
   * The LoRAs to use for the generation.
   */
  loras: Array<SchemaLoRaInput>
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | 'dolly_in'
    | 'dolly_out'
    | 'dolly_left'
    | 'dolly_right'
    | 'jib_up'
    | 'jib_down'
    | 'static'
    | 'none'
  /**
   * Video Size
   *
   * The size of the generated video. Use 'auto' to match the input image dimensions if provided.
   */
  video_size?:
    | SchemaImageSize
    | 'auto'
    | 'square_hd'
    | 'square'
    | 'portrait_4_3'
    | 'portrait_16_9'
    | 'landscape_4_3'
    | 'landscape_16_9'
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number
  /**
   * Image Strength
   *
   * The strength of the image to use for the video generation.
   */
  image_strength?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string
  /**
   * Guidance Scale
   *
   * The guidance scale to use.
   */
  guidance_scale?: number
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | 'X264 (.mp4)'
    | 'VP9 (.webm)'
    | 'PRORES4444 (.mov)'
    | 'GIF (.gif)'
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number
  /**
   * Preprocess Audio
   *
   * Whether to preprocess the audio before using it as conditioning.
   */
  preprocess_audio?: boolean
  /**
   * Image URL
   *
   * Optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | unknown
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Audio URL
   *
   * The URL of the audio to generate the video from.
   */
  audio_url: string
  /**
   * Audio Strength
   *
   * Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification.
   */
  audio_strength?: number
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
}

/**
 * LoRAInput
 *
 * LoRA weight configuration.
 */
export type SchemaLoRaInput = {
  /**
   * Path
   *
   * URL, HuggingFace repo ID (owner/repo) to lora weights.
   */
  path: string
  /**
   * Scale
   *
   * Scale factor for LoRA application (0.0 to 4.0).
   */
  scale?: number
  /**
   * Weight Name
   *
   * Name of the LoRA weight. Only used if `path` is a HuggingFace repository, and is only required when the repository contains multiple LoRA weights.
   */
  weight_name?: string | unknown
}

/**
 * LTX2AudioToVideoOutput
 */
export type SchemaLtx219bDistilledAudioToVideoLoraOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number
  video: SchemaVideoFile
}

/**
 * LTX2LoRADistilledAudioToVideoInput
 */
export type SchemaLtx219bDistilledAudioToVideoLoraInput = {
  /**
   * Match Audio Length
   *
   * When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames.
   */
  match_audio_length?: boolean
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: 'none' | 'regular' | 'high' | 'full'
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number
  /**
   * LoRAs
   *
   * The LoRAs to use for the generation.
   */
  loras: Array<SchemaLoRaInput>
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | 'dolly_in'
    | 'dolly_out'
    | 'dolly_left'
    | 'dolly_right'
    | 'jib_up'
    | 'jib_down'
    | 'static'
    | 'none'
  /**
   * Video Size
   *
   * The size of the generated video. Use 'auto' to match the input image dimensions if provided.
   */
  video_size?:
    | SchemaImageSize
    | 'auto'
    | 'square_hd'
    | 'square'
    | 'portrait_4_3'
    | 'portrait_16_9'
    | 'landscape_4_3'
    | 'landscape_16_9'
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number
  /**
   * Image Strength
   *
   * The strength of the image to use for the video generation.
   */
  image_strength?: number
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: 'fast' | 'balanced' | 'small'
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | 'X264 (.mp4)'
    | 'VP9 (.webm)'
    | 'PRORES4444 (.mov)'
    | 'GIF (.gif)'
  /**
   * Preprocess Audio
   *
   * Whether to preprocess the audio before using it as conditioning.
   */
  preprocess_audio?: boolean
  /**
   * Image URL
   *
   * Optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | unknown
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: 'low' | 'medium' | 'high' | 'maximum'
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Audio URL
   *
   * The URL of the audio to generate the video from.
   */
  audio_url: string
  /**
   * Audio Strength
   *
   * Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification.
   */
  audio_strength?: number
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean
}

export type SchemaQueueStatus = {
  status: 'IN_QUEUE' | 'IN_PROGRESS' | 'COMPLETED'
  /**
   * The request id.
   */
  request_id: string
  /**
   * The response url.
   */
  response_url?: string
  /**
   * The status url.
   */
  status_url?: string
  /**
   * The cancel url.
   */
  cancel_url?: string
  /**
   * The logs.
   */
  logs?: {
    [key: string]: unknown
  }
  /**
   * The metrics.
   */
  metrics?: {
    [key: string]: unknown
  }
  /**
   * The queue position.
   */
  queue_position?: number
}

export type GetFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/ltx-2-19b/distilled/audio-to-video/lora/requests/{request_id}/status'
  }

export type GetFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdStatusResponses]

export type PutFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/ltx-2-19b/distilled/audio-to-video/lora/requests/{request_id}/cancel'
  }

export type PutFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdCancelResponses]

export type PostFalAiLtx219bDistilledAudioToVideoLoraData = {
  body: SchemaLtx219bDistilledAudioToVideoLoraInput
  path?: never
  query?: never
  url: '/fal-ai/ltx-2-19b/distilled/audio-to-video/lora'
}

export type PostFalAiLtx219bDistilledAudioToVideoLoraResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLtx219bDistilledAudioToVideoLoraResponse =
  PostFalAiLtx219bDistilledAudioToVideoLoraResponses[keyof PostFalAiLtx219bDistilledAudioToVideoLoraResponses]

export type GetFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-2-19b/distilled/audio-to-video/lora/requests/{request_id}'
}

export type GetFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaLtx219bDistilledAudioToVideoLoraOutput
  }

export type GetFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdResponse =
  GetFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdResponses[keyof GetFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdResponses]

export type GetFalAiLtx219bAudioToVideoLoraRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/ltx-2-19b/audio-to-video/lora/requests/{request_id}/status'
}

export type GetFalAiLtx219bAudioToVideoLoraRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiLtx219bAudioToVideoLoraRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bAudioToVideoLoraRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bAudioToVideoLoraRequestsByRequestIdStatusResponses]

export type PutFalAiLtx219bAudioToVideoLoraRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-2-19b/audio-to-video/lora/requests/{request_id}/cancel'
}

export type PutFalAiLtx219bAudioToVideoLoraRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiLtx219bAudioToVideoLoraRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bAudioToVideoLoraRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bAudioToVideoLoraRequestsByRequestIdCancelResponses]

export type PostFalAiLtx219bAudioToVideoLoraData = {
  body: SchemaLtx219bAudioToVideoLoraInput
  path?: never
  query?: never
  url: '/fal-ai/ltx-2-19b/audio-to-video/lora'
}

export type PostFalAiLtx219bAudioToVideoLoraResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLtx219bAudioToVideoLoraResponse =
  PostFalAiLtx219bAudioToVideoLoraResponses[keyof PostFalAiLtx219bAudioToVideoLoraResponses]

export type GetFalAiLtx219bAudioToVideoLoraRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-2-19b/audio-to-video/lora/requests/{request_id}'
}

export type GetFalAiLtx219bAudioToVideoLoraRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLtx219bAudioToVideoLoraOutput
}

export type GetFalAiLtx219bAudioToVideoLoraRequestsByRequestIdResponse =
  GetFalAiLtx219bAudioToVideoLoraRequestsByRequestIdResponses[keyof GetFalAiLtx219bAudioToVideoLoraRequestsByRequestIdResponses]

export type GetFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/ltx-2-19b/distilled/audio-to-video/requests/{request_id}/status'
  }

export type GetFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdStatusResponses]

export type PutFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/ltx-2-19b/distilled/audio-to-video/requests/{request_id}/cancel'
  }

export type PutFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdCancelResponses]

export type PostFalAiLtx219bDistilledAudioToVideoData = {
  body: SchemaLtx219bDistilledAudioToVideoInput
  path?: never
  query?: never
  url: '/fal-ai/ltx-2-19b/distilled/audio-to-video'
}

export type PostFalAiLtx219bDistilledAudioToVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLtx219bDistilledAudioToVideoResponse =
  PostFalAiLtx219bDistilledAudioToVideoResponses[keyof PostFalAiLtx219bDistilledAudioToVideoResponses]

export type GetFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-2-19b/distilled/audio-to-video/requests/{request_id}'
}

export type GetFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLtx219bDistilledAudioToVideoOutput
}

export type GetFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdResponse =
  GetFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdResponses[keyof GetFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdResponses]

export type GetFalAiLtx219bAudioToVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/ltx-2-19b/audio-to-video/requests/{request_id}/status'
}

export type GetFalAiLtx219bAudioToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiLtx219bAudioToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bAudioToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bAudioToVideoRequestsByRequestIdStatusResponses]

export type PutFalAiLtx219bAudioToVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-2-19b/audio-to-video/requests/{request_id}/cancel'
}

export type PutFalAiLtx219bAudioToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiLtx219bAudioToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bAudioToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bAudioToVideoRequestsByRequestIdCancelResponses]

export type PostFalAiLtx219bAudioToVideoData = {
  body: SchemaLtx219bAudioToVideoInput
  path?: never
  query?: never
  url: '/fal-ai/ltx-2-19b/audio-to-video'
}

export type PostFalAiLtx219bAudioToVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLtx219bAudioToVideoResponse =
  PostFalAiLtx219bAudioToVideoResponses[keyof PostFalAiLtx219bAudioToVideoResponses]

export type GetFalAiLtx219bAudioToVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ltx-2-19b/audio-to-video/requests/{request_id}'
}

export type GetFalAiLtx219bAudioToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaLtx219bAudioToVideoOutput
}

export type GetFalAiLtx219bAudioToVideoRequestsByRequestIdResponse =
  GetFalAiLtx219bAudioToVideoRequestsByRequestIdResponses[keyof GetFalAiLtx219bAudioToVideoRequestsByRequestIdResponses]

export type GetFalAiElevenlabsDubbingRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/elevenlabs/dubbing/requests/{request_id}/status'
}

export type GetFalAiElevenlabsDubbingRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiElevenlabsDubbingRequestsByRequestIdStatusResponse =
  GetFalAiElevenlabsDubbingRequestsByRequestIdStatusResponses[keyof GetFalAiElevenlabsDubbingRequestsByRequestIdStatusResponses]

export type PutFalAiElevenlabsDubbingRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/elevenlabs/dubbing/requests/{request_id}/cancel'
}

export type PutFalAiElevenlabsDubbingRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiElevenlabsDubbingRequestsByRequestIdCancelResponse =
  PutFalAiElevenlabsDubbingRequestsByRequestIdCancelResponses[keyof PutFalAiElevenlabsDubbingRequestsByRequestIdCancelResponses]

export type PostFalAiElevenlabsDubbingData = {
  body: SchemaElevenlabsDubbingInput
  path?: never
  query?: never
  url: '/fal-ai/elevenlabs/dubbing'
}

export type PostFalAiElevenlabsDubbingResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiElevenlabsDubbingResponse =
  PostFalAiElevenlabsDubbingResponses[keyof PostFalAiElevenlabsDubbingResponses]

export type GetFalAiElevenlabsDubbingRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/elevenlabs/dubbing/requests/{request_id}'
}

export type GetFalAiElevenlabsDubbingRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaElevenlabsDubbingOutput
}

export type GetFalAiElevenlabsDubbingRequestsByRequestIdResponse =
  GetFalAiElevenlabsDubbingRequestsByRequestIdResponses[keyof GetFalAiElevenlabsDubbingRequestsByRequestIdResponses]

export type GetFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/longcat-multi-avatar/image-audio-to-video/requests/{request_id}/status'
  }

export type GetFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdStatusResponses]

export type PutFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/longcat-multi-avatar/image-audio-to-video/requests/{request_id}/cancel'
  }

export type PutFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdCancelResponses]

export type PostFalAiLongcatMultiAvatarImageAudioToVideoData = {
  body: SchemaLongcatMultiAvatarImageAudioToVideoInput
  path?: never
  query?: never
  url: '/fal-ai/longcat-multi-avatar/image-audio-to-video'
}

export type PostFalAiLongcatMultiAvatarImageAudioToVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLongcatMultiAvatarImageAudioToVideoResponse =
  PostFalAiLongcatMultiAvatarImageAudioToVideoResponses[keyof PostFalAiLongcatMultiAvatarImageAudioToVideoResponses]

export type GetFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/longcat-multi-avatar/image-audio-to-video/requests/{request_id}'
  }

export type GetFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaLongcatMultiAvatarImageAudioToVideoOutput
  }

export type GetFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdResponse =
  GetFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdResponses[keyof GetFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdResponses]

export type GetFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/longcat-single-avatar/image-audio-to-video/requests/{request_id}/status'
  }

export type GetFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdStatusResponses]

export type PutFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/longcat-single-avatar/image-audio-to-video/requests/{request_id}/cancel'
  }

export type PutFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdCancelResponses]

export type PostFalAiLongcatSingleAvatarImageAudioToVideoData = {
  body: SchemaLongcatSingleAvatarImageAudioToVideoInput
  path?: never
  query?: never
  url: '/fal-ai/longcat-single-avatar/image-audio-to-video'
}

export type PostFalAiLongcatSingleAvatarImageAudioToVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLongcatSingleAvatarImageAudioToVideoResponse =
  PostFalAiLongcatSingleAvatarImageAudioToVideoResponses[keyof PostFalAiLongcatSingleAvatarImageAudioToVideoResponses]

export type GetFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/longcat-single-avatar/image-audio-to-video/requests/{request_id}'
  }

export type GetFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaLongcatSingleAvatarImageAudioToVideoOutput
  }

export type GetFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdResponse =
  GetFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdResponses[keyof GetFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdResponses]

export type GetFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/longcat-single-avatar/audio-to-video/requests/{request_id}/status'
  }

export type GetFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdStatusResponses]

export type PutFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/longcat-single-avatar/audio-to-video/requests/{request_id}/cancel'
  }

export type PutFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdCancelResponses]

export type PostFalAiLongcatSingleAvatarAudioToVideoData = {
  body: SchemaLongcatSingleAvatarAudioToVideoInput
  path?: never
  query?: never
  url: '/fal-ai/longcat-single-avatar/audio-to-video'
}

export type PostFalAiLongcatSingleAvatarAudioToVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiLongcatSingleAvatarAudioToVideoResponse =
  PostFalAiLongcatSingleAvatarAudioToVideoResponses[keyof PostFalAiLongcatSingleAvatarAudioToVideoResponses]

export type GetFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/longcat-single-avatar/audio-to-video/requests/{request_id}'
}

export type GetFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaLongcatSingleAvatarAudioToVideoOutput
  }

export type GetFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdResponse =
  GetFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdResponses[keyof GetFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdResponses]

export type GetArgilAvatarsAudioToVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/argil/avatars/audio-to-video/requests/{request_id}/status'
}

export type GetArgilAvatarsAudioToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetArgilAvatarsAudioToVideoRequestsByRequestIdStatusResponse =
  GetArgilAvatarsAudioToVideoRequestsByRequestIdStatusResponses[keyof GetArgilAvatarsAudioToVideoRequestsByRequestIdStatusResponses]

export type PutArgilAvatarsAudioToVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/argil/avatars/audio-to-video/requests/{request_id}/cancel'
}

export type PutArgilAvatarsAudioToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutArgilAvatarsAudioToVideoRequestsByRequestIdCancelResponse =
  PutArgilAvatarsAudioToVideoRequestsByRequestIdCancelResponses[keyof PutArgilAvatarsAudioToVideoRequestsByRequestIdCancelResponses]

export type PostArgilAvatarsAudioToVideoData = {
  body: SchemaAvatarsAudioToVideoInput
  path?: never
  query?: never
  url: '/argil/avatars/audio-to-video'
}

export type PostArgilAvatarsAudioToVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostArgilAvatarsAudioToVideoResponse =
  PostArgilAvatarsAudioToVideoResponses[keyof PostArgilAvatarsAudioToVideoResponses]

export type GetArgilAvatarsAudioToVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/argil/avatars/audio-to-video/requests/{request_id}'
}

export type GetArgilAvatarsAudioToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaAvatarsAudioToVideoOutput
}

export type GetArgilAvatarsAudioToVideoRequestsByRequestIdResponse =
  GetArgilAvatarsAudioToVideoRequestsByRequestIdResponses[keyof GetArgilAvatarsAudioToVideoRequestsByRequestIdResponses]

export type GetFalAiWanV2214bSpeechToVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/wan/v2.2-14b/speech-to-video/requests/{request_id}/status'
}

export type GetFalAiWanV2214bSpeechToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiWanV2214bSpeechToVideoRequestsByRequestIdStatusResponse =
  GetFalAiWanV2214bSpeechToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiWanV2214bSpeechToVideoRequestsByRequestIdStatusResponses]

export type PutFalAiWanV2214bSpeechToVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan/v2.2-14b/speech-to-video/requests/{request_id}/cancel'
}

export type PutFalAiWanV2214bSpeechToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiWanV2214bSpeechToVideoRequestsByRequestIdCancelResponse =
  PutFalAiWanV2214bSpeechToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiWanV2214bSpeechToVideoRequestsByRequestIdCancelResponses]

export type PostFalAiWanV2214bSpeechToVideoData = {
  body: SchemaWanV2214bSpeechToVideoInput
  path?: never
  query?: never
  url: '/fal-ai/wan/v2.2-14b/speech-to-video'
}

export type PostFalAiWanV2214bSpeechToVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWanV2214bSpeechToVideoResponse =
  PostFalAiWanV2214bSpeechToVideoResponses[keyof PostFalAiWanV2214bSpeechToVideoResponses]

export type GetFalAiWanV2214bSpeechToVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wan/v2.2-14b/speech-to-video/requests/{request_id}'
}

export type GetFalAiWanV2214bSpeechToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWanV2214bSpeechToVideoOutput
}

export type GetFalAiWanV2214bSpeechToVideoRequestsByRequestIdResponse =
  GetFalAiWanV2214bSpeechToVideoRequestsByRequestIdResponses[keyof GetFalAiWanV2214bSpeechToVideoRequestsByRequestIdResponses]

export type GetFalAiStableAvatarRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/stable-avatar/requests/{request_id}/status'
}

export type GetFalAiStableAvatarRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiStableAvatarRequestsByRequestIdStatusResponse =
  GetFalAiStableAvatarRequestsByRequestIdStatusResponses[keyof GetFalAiStableAvatarRequestsByRequestIdStatusResponses]

export type PutFalAiStableAvatarRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/stable-avatar/requests/{request_id}/cancel'
}

export type PutFalAiStableAvatarRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiStableAvatarRequestsByRequestIdCancelResponse =
  PutFalAiStableAvatarRequestsByRequestIdCancelResponses[keyof PutFalAiStableAvatarRequestsByRequestIdCancelResponses]

export type PostFalAiStableAvatarData = {
  body: SchemaStableAvatarInput
  path?: never
  query?: never
  url: '/fal-ai/stable-avatar'
}

export type PostFalAiStableAvatarResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiStableAvatarResponse =
  PostFalAiStableAvatarResponses[keyof PostFalAiStableAvatarResponses]

export type GetFalAiStableAvatarRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/stable-avatar/requests/{request_id}'
}

export type GetFalAiStableAvatarRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaStableAvatarOutput
}

export type GetFalAiStableAvatarRequestsByRequestIdResponse =
  GetFalAiStableAvatarRequestsByRequestIdResponses[keyof GetFalAiStableAvatarRequestsByRequestIdResponses]

export type GetFalAiEchomimicV3RequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/echomimic-v3/requests/{request_id}/status'
}

export type GetFalAiEchomimicV3RequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiEchomimicV3RequestsByRequestIdStatusResponse =
  GetFalAiEchomimicV3RequestsByRequestIdStatusResponses[keyof GetFalAiEchomimicV3RequestsByRequestIdStatusResponses]

export type PutFalAiEchomimicV3RequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/echomimic-v3/requests/{request_id}/cancel'
}

export type PutFalAiEchomimicV3RequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiEchomimicV3RequestsByRequestIdCancelResponse =
  PutFalAiEchomimicV3RequestsByRequestIdCancelResponses[keyof PutFalAiEchomimicV3RequestsByRequestIdCancelResponses]

export type PostFalAiEchomimicV3Data = {
  body: SchemaEchomimicV3Input
  path?: never
  query?: never
  url: '/fal-ai/echomimic-v3'
}

export type PostFalAiEchomimicV3Responses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiEchomimicV3Response =
  PostFalAiEchomimicV3Responses[keyof PostFalAiEchomimicV3Responses]

export type GetFalAiEchomimicV3RequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/echomimic-v3/requests/{request_id}'
}

export type GetFalAiEchomimicV3RequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaEchomimicV3Output
}

export type GetFalAiEchomimicV3RequestsByRequestIdResponse =
  GetFalAiEchomimicV3RequestsByRequestIdResponses[keyof GetFalAiEchomimicV3RequestsByRequestIdResponses]

export type GetVeedAvatarsAudioToVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/veed/avatars/audio-to-video/requests/{request_id}/status'
}

export type GetVeedAvatarsAudioToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetVeedAvatarsAudioToVideoRequestsByRequestIdStatusResponse =
  GetVeedAvatarsAudioToVideoRequestsByRequestIdStatusResponses[keyof GetVeedAvatarsAudioToVideoRequestsByRequestIdStatusResponses]

export type PutVeedAvatarsAudioToVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/veed/avatars/audio-to-video/requests/{request_id}/cancel'
}

export type PutVeedAvatarsAudioToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutVeedAvatarsAudioToVideoRequestsByRequestIdCancelResponse =
  PutVeedAvatarsAudioToVideoRequestsByRequestIdCancelResponses[keyof PutVeedAvatarsAudioToVideoRequestsByRequestIdCancelResponses]

export type PostVeedAvatarsAudioToVideoData = {
  body: SchemaAvatarsAudioToVideoInput
  path?: never
  query?: never
  url: '/veed/avatars/audio-to-video'
}

export type PostVeedAvatarsAudioToVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostVeedAvatarsAudioToVideoResponse =
  PostVeedAvatarsAudioToVideoResponses[keyof PostVeedAvatarsAudioToVideoResponses]

export type GetVeedAvatarsAudioToVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/veed/avatars/audio-to-video/requests/{request_id}'
}

export type GetVeedAvatarsAudioToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaAvatarsAudioToVideoOutput
}

export type GetVeedAvatarsAudioToVideoRequestsByRequestIdResponse =
  GetVeedAvatarsAudioToVideoRequestsByRequestIdResponses[keyof GetVeedAvatarsAudioToVideoRequestsByRequestIdResponses]
