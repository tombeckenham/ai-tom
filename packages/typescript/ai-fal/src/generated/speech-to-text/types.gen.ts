// This file is auto-generated by @hey-api/openapi-ts

export type ClientOptions = {
  baseUrl: 'https://queue.fal.run' | (string & {})
}

/**
 * DiarizationSegment
 */
export type SchemaDiarizationSegment = {
  /**
   * Timestamp
   *
   * Start and end timestamp of the segment
   */
  timestamp: [unknown, unknown]
  /**
   * Speaker
   *
   * Speaker ID of the segment
   */
  speaker: string
}

/**
 * WhisperOutput
 */
export type SchemaWhisperOutput = {
  /**
   * Text
   *
   * Transcription of the audio file
   */
  text: string
  /**
   * Inferred Languages
   *
   * List of languages that the audio file is inferred to be. Defaults to null.
   */
  inferred_languages: Array<
    | 'af'
    | 'am'
    | 'ar'
    | 'as'
    | 'az'
    | 'ba'
    | 'be'
    | 'bg'
    | 'bn'
    | 'bo'
    | 'br'
    | 'bs'
    | 'ca'
    | 'cs'
    | 'cy'
    | 'da'
    | 'de'
    | 'el'
    | 'en'
    | 'es'
    | 'et'
    | 'eu'
    | 'fa'
    | 'fi'
    | 'fo'
    | 'fr'
    | 'gl'
    | 'gu'
    | 'ha'
    | 'haw'
    | 'he'
    | 'hi'
    | 'hr'
    | 'ht'
    | 'hu'
    | 'hy'
    | 'id'
    | 'is'
    | 'it'
    | 'ja'
    | 'jw'
    | 'ka'
    | 'kk'
    | 'km'
    | 'kn'
    | 'ko'
    | 'la'
    | 'lb'
    | 'ln'
    | 'lo'
    | 'lt'
    | 'lv'
    | 'mg'
    | 'mi'
    | 'mk'
    | 'ml'
    | 'mn'
    | 'mr'
    | 'ms'
    | 'mt'
    | 'my'
    | 'ne'
    | 'nl'
    | 'nn'
    | 'no'
    | 'oc'
    | 'pa'
    | 'pl'
    | 'ps'
    | 'pt'
    | 'ro'
    | 'ru'
    | 'sa'
    | 'sd'
    | 'si'
    | 'sk'
    | 'sl'
    | 'sn'
    | 'so'
    | 'sq'
    | 'sr'
    | 'su'
    | 'sv'
    | 'sw'
    | 'ta'
    | 'te'
    | 'tg'
    | 'th'
    | 'tk'
    | 'tl'
    | 'tr'
    | 'tt'
    | 'uk'
    | 'ur'
    | 'uz'
    | 'vi'
    | 'yi'
    | 'yo'
    | 'zh'
  >
  /**
   * Chunks
   *
   * Timestamp chunks of the audio file
   */
  chunks?: Array<SchemaWhisperChunk>
  /**
   * Diarization Segments
   *
   * Speaker diarization segments of the audio file. Only present if diarization is enabled.
   */
  diarization_segments: Array<SchemaDiarizationSegment>
}

/**
 * WhisperChunk
 */
export type SchemaWhisperChunk = {
  /**
   * Text
   *
   * Transcription of the chunk
   */
  text: string
  /**
   * Timestamp
   *
   * Start and end timestamp of the chunk
   */
  timestamp: [unknown, unknown]
  /**
   * Speaker
   *
   * Speaker ID of the chunk. Only present if diarization is enabled.
   */
  speaker?: string
}

/**
 * WhisperInput
 */
export type SchemaWhisperInput = {
  /**
   * Version
   *
   * Version of the model to use. All of the models are the Whisper large variant.
   */
  version?: '3'
  /**
   * Batch Size
   */
  batch_size?: number
  /**
   * Language
   *
   *
   * Language of the audio file. If set to null, the language will be
   * automatically detected. Defaults to null.
   *
   * If translate is selected as the task, the audio will be translated to
   * English, regardless of the language selected.
   *
   */
  language?:
    | 'af'
    | 'am'
    | 'ar'
    | 'as'
    | 'az'
    | 'ba'
    | 'be'
    | 'bg'
    | 'bn'
    | 'bo'
    | 'br'
    | 'bs'
    | 'ca'
    | 'cs'
    | 'cy'
    | 'da'
    | 'de'
    | 'el'
    | 'en'
    | 'es'
    | 'et'
    | 'eu'
    | 'fa'
    | 'fi'
    | 'fo'
    | 'fr'
    | 'gl'
    | 'gu'
    | 'ha'
    | 'haw'
    | 'he'
    | 'hi'
    | 'hr'
    | 'ht'
    | 'hu'
    | 'hy'
    | 'id'
    | 'is'
    | 'it'
    | 'ja'
    | 'jw'
    | 'ka'
    | 'kk'
    | 'km'
    | 'kn'
    | 'ko'
    | 'la'
    | 'lb'
    | 'ln'
    | 'lo'
    | 'lt'
    | 'lv'
    | 'mg'
    | 'mi'
    | 'mk'
    | 'ml'
    | 'mn'
    | 'mr'
    | 'ms'
    | 'mt'
    | 'my'
    | 'ne'
    | 'nl'
    | 'nn'
    | 'no'
    | 'oc'
    | 'pa'
    | 'pl'
    | 'ps'
    | 'pt'
    | 'ro'
    | 'ru'
    | 'sa'
    | 'sd'
    | 'si'
    | 'sk'
    | 'sl'
    | 'sn'
    | 'so'
    | 'sq'
    | 'sr'
    | 'su'
    | 'sv'
    | 'sw'
    | 'ta'
    | 'te'
    | 'tg'
    | 'th'
    | 'tk'
    | 'tl'
    | 'tr'
    | 'tt'
    | 'uk'
    | 'ur'
    | 'uz'
    | 'vi'
    | 'yi'
    | 'yo'
    | 'zh'
  /**
   * Prompt
   *
   * Prompt to use for generation. Defaults to an empty string.
   */
  prompt?: string
  /**
   * Num Speakers
   *
   *
   * Number of speakers in the audio file. Defaults to null.
   * If not provided, the number of speakers will be automatically
   * detected.
   *
   */
  num_speakers?: number | null
  /**
   * Task
   *
   * Task to perform on the audio file. Either transcribe or translate.
   */
  task?: 'transcribe' | 'translate'
  /**
   * Chunk Level
   *
   * Level of the chunks to return. Either none, segment or word. `none` would imply that all of the audio will be transcribed without the timestamp tokens, we suggest to switch to `none` if you are not satisfied with the transcription quality, since it will usually improve the quality of the results. Switching to `none` will also provide minor speed ups in the transcription due to less amount of generated tokens. Notice that setting to none will produce **a single chunk with the whole transcription**.
   */
  chunk_level?: 'none' | 'segment' | 'word'
  /**
   * Audio Url
   *
   * URL of the audio file to transcribe. Supported formats: mp3, mp4, mpeg, mpga, m4a, wav or webm.
   */
  audio_url: string
  /**
   * Diarize
   *
   * Whether to diarize the audio file. Defaults to false. Setting to true will add costs proportional to diarization inference time.
   */
  diarize?: boolean
}

/**
 * WhisperOutput
 */
export type SchemaWizperOutput = {
  /**
   * Text
   *
   * Transcription of the audio file
   */
  text: string
  /**
   * Languages
   *
   * List of languages that the audio file is inferred to be. Defaults to null.
   */
  languages: Array<
    | 'af'
    | 'am'
    | 'ar'
    | 'as'
    | 'az'
    | 'ba'
    | 'be'
    | 'bg'
    | 'bn'
    | 'bo'
    | 'br'
    | 'bs'
    | 'ca'
    | 'cs'
    | 'cy'
    | 'da'
    | 'de'
    | 'el'
    | 'en'
    | 'es'
    | 'et'
    | 'eu'
    | 'fa'
    | 'fi'
    | 'fo'
    | 'fr'
    | 'gl'
    | 'gu'
    | 'ha'
    | 'haw'
    | 'he'
    | 'hi'
    | 'hr'
    | 'ht'
    | 'hu'
    | 'hy'
    | 'id'
    | 'is'
    | 'it'
    | 'ja'
    | 'jw'
    | 'ka'
    | 'kk'
    | 'km'
    | 'kn'
    | 'ko'
    | 'la'
    | 'lb'
    | 'ln'
    | 'lo'
    | 'lt'
    | 'lv'
    | 'mg'
    | 'mi'
    | 'mk'
    | 'ml'
    | 'mn'
    | 'mr'
    | 'ms'
    | 'mt'
    | 'my'
    | 'ne'
    | 'nl'
    | 'nn'
    | 'no'
    | 'oc'
    | 'pa'
    | 'pl'
    | 'ps'
    | 'pt'
    | 'ro'
    | 'ru'
    | 'sa'
    | 'sd'
    | 'si'
    | 'sk'
    | 'sl'
    | 'sn'
    | 'so'
    | 'sq'
    | 'sr'
    | 'su'
    | 'sv'
    | 'sw'
    | 'ta'
    | 'te'
    | 'tg'
    | 'th'
    | 'tk'
    | 'tl'
    | 'tr'
    | 'tt'
    | 'uk'
    | 'ur'
    | 'uz'
    | 'vi'
    | 'yi'
    | 'yo'
    | 'zh'
  >
  /**
   * Chunks
   *
   * Timestamp chunks of the audio file
   */
  chunks: Array<SchemaWhisperChunk>
}

/**
 * WhisperInput
 */
export type SchemaWizperInput = {
  /**
   * Language
   *
   *
   * Language of the audio file.
   * If translate is selected as the task, the audio will be translated to
   * English, regardless of the language selected. If `None` is passed,
   * the language will be automatically detected. This will also increase
   * the inference time.
   *
   */
  language?:
    | 'af'
    | 'am'
    | 'ar'
    | 'as'
    | 'az'
    | 'ba'
    | 'be'
    | 'bg'
    | 'bn'
    | 'bo'
    | 'br'
    | 'bs'
    | 'ca'
    | 'cs'
    | 'cy'
    | 'da'
    | 'de'
    | 'el'
    | 'en'
    | 'es'
    | 'et'
    | 'eu'
    | 'fa'
    | 'fi'
    | 'fo'
    | 'fr'
    | 'gl'
    | 'gu'
    | 'ha'
    | 'haw'
    | 'he'
    | 'hi'
    | 'hr'
    | 'ht'
    | 'hu'
    | 'hy'
    | 'id'
    | 'is'
    | 'it'
    | 'ja'
    | 'jw'
    | 'ka'
    | 'kk'
    | 'km'
    | 'kn'
    | 'ko'
    | 'la'
    | 'lb'
    | 'ln'
    | 'lo'
    | 'lt'
    | 'lv'
    | 'mg'
    | 'mi'
    | 'mk'
    | 'ml'
    | 'mn'
    | 'mr'
    | 'ms'
    | 'mt'
    | 'my'
    | 'ne'
    | 'nl'
    | 'nn'
    | 'no'
    | 'oc'
    | 'pa'
    | 'pl'
    | 'ps'
    | 'pt'
    | 'ro'
    | 'ru'
    | 'sa'
    | 'sd'
    | 'si'
    | 'sk'
    | 'sl'
    | 'sn'
    | 'so'
    | 'sq'
    | 'sr'
    | 'su'
    | 'sv'
    | 'sw'
    | 'ta'
    | 'te'
    | 'tg'
    | 'th'
    | 'tk'
    | 'tl'
    | 'tr'
    | 'tt'
    | 'uk'
    | 'ur'
    | 'uz'
    | 'vi'
    | 'yi'
    | 'yo'
    | 'zh'
    | unknown
  /**
   * Version
   *
   * Version of the model to use. All of the models are the Whisper large variant.
   */
  version?: string
  /**
   * Max Segment Len
   *
   * Maximum speech segment duration in seconds before splitting.
   */
  max_segment_len?: number
  /**
   * Task
   *
   * Task to perform on the audio file. Either transcribe or translate.
   */
  task?: 'transcribe' | 'translate'
  /**
   * Chunk Level
   *
   * Level of the chunks to return.
   */
  chunk_level?: string
  /**
   * Audio Url
   *
   * URL of the audio file to transcribe. Supported formats: mp3, mp4, mpeg, mpga, m4a, wav or webm.
   */
  audio_url: string
  /**
   * Merge Chunks
   *
   * Whether to merge consecutive chunks. When enabled, chunks are merged if their combined duration does not exceed max_segment_len.
   */
  merge_chunks?: boolean
}

/**
 * TranscriptionOutput
 */
export type SchemaElevenlabsSpeechToTextOutput = {
  /**
   * Text
   *
   * The full transcribed text
   */
  text: string
  /**
   * Language Probability
   *
   * Confidence in language detection
   */
  language_probability: number
  /**
   * Language Code
   *
   * Detected or specified language code
   */
  language_code: string
  /**
   * Words
   *
   * Word-level transcription details
   */
  words: Array<SchemaTranscriptionWord>
}

/**
 * TranscriptionWord
 */
export type SchemaTranscriptionWord = {
  /**
   * Text
   *
   * The transcribed word or audio event
   */
  text: string
  /**
   * Start
   *
   * Start time in seconds
   */
  start: number | unknown
  /**
   * Type
   *
   * Type of element (word, spacing, or audio_event)
   */
  type: string
  /**
   * End
   *
   * End time in seconds
   */
  end: number | unknown
  /**
   * Speaker Id
   *
   * Speaker identifier if diarization was enabled
   */
  speaker_id?: string | unknown
}

/**
 * SpeechToTextRequest
 */
export type SchemaElevenlabsSpeechToTextInput = {
  /**
   * Language Code
   *
   * Language code of the audio
   */
  language_code?: string | unknown
  /**
   * Audio Url
   *
   * URL of the audio file to transcribe
   */
  audio_url: string
  /**
   * Diarize
   *
   * Whether to annotate who is speaking
   */
  diarize?: boolean
  /**
   * Tag Audio Events
   *
   * Tag audio events like laughter, applause, etc.
   */
  tag_audio_events?: boolean
}

/**
 * SpeechOutput
 */
export type SchemaSpeechToTextOutput = {
  /**
   * Partial
   *
   * Indicates if this is a partial (in-progress) transcript
   */
  partial?: boolean
  /**
   * Transcribed Text
   *
   * The partial or final transcription output from Canary
   */
  output: string
}

/**
 * SpeechInput
 */
export type SchemaSpeechToTextInput = {
  /**
   * Audio Path
   *
   * Local filesystem path (or remote URL) to a long audio file
   */
  audio_url: string
  /**
   * Use Punctuation/Capitalization (PnC)
   *
   * Whether to use Canary's built-in punctuation & capitalization
   */
  use_pnc?: boolean
}

export type SchemaSpeechToTextStreamOutput = unknown

/**
 * SpeechInput
 */
export type SchemaSpeechToTextStreamInput = {
  /**
   * Audio Path
   *
   * Local filesystem path (or remote URL) to a long audio file
   */
  audio_url: string
  /**
   * Use Punctuation/Capitalization (PnC)
   *
   * Whether to use Canary's built-in punctuation & capitalization
   */
  use_pnc?: boolean
}

export type SchemaSpeechToTextTurboStreamOutput = unknown

/**
 * SpeechInput
 */
export type SchemaSpeechToTextTurboStreamInput = {
  /**
   * Audio Path
   *
   * Local filesystem path (or remote URL) to a long audio file
   */
  audio_url: string
  /**
   * Use Punctuation/Capitalization (PnC)
   *
   * Whether to use Canary's built-in punctuation & capitalization
   */
  use_pnc?: boolean
}

/**
 * SpeechOutput
 */
export type SchemaSpeechToTextTurboOutput = {
  /**
   * Partial
   *
   * Indicates if this is a partial (in-progress) transcript
   */
  partial?: boolean
  /**
   * Transcribed Text
   *
   * The partial or final transcription output from Canary
   */
  output: string
}

/**
 * SpeechInput
 */
export type SchemaSpeechToTextTurboInput = {
  /**
   * Audio Path
   *
   * Local filesystem path (or remote URL) to a long audio file
   */
  audio_url: string
  /**
   * Use Punctuation/Capitalization (PnC)
   *
   * Whether to use Canary's built-in punctuation & capitalization
   */
  use_pnc?: boolean
}

/**
 * Output
 */
export type SchemaSmartTurnOutput = {
  /**
   * Prediction
   *
   * The predicted turn type. 1 for Complete, 0 for Incomplete.
   */
  prediction: number
  /**
   * Probability
   *
   * The probability of the predicted turn type.
   */
  probability: number
  /**
   * Metrics
   *
   * The metrics of the inference.
   */
  metrics: {
    [key: string]: unknown
  }
}

/**
 * SmartTurnInput
 */
export type SchemaSmartTurnInput = {
  /**
   * Audio Url
   *
   * The URL of the audio file to be processed.
   */
  audio_url: string
}

/**
 * TranscriptionOutputV2
 */
export type SchemaElevenlabsSpeechToTextScribeV2Output = {
  /**
   * Text
   *
   * The full transcribed text
   */
  text: string
  /**
   * Language Probability
   *
   * Confidence in language detection
   */
  language_probability: number
  /**
   * Language Code
   *
   * Detected or specified language code
   */
  language_code: string
  /**
   * Words
   *
   * Word-level transcription details
   */
  words: Array<SchemaTranscriptionWord>
}

/**
 * SpeechToTextRequestScribeV2
 */
export type SchemaElevenlabsSpeechToTextScribeV2Input = {
  /**
   * Keyterms
   *
   * Words or sentences to bias the model towards transcribing. Up to 100 keyterms, max 50 characters each. Adds 30% premium over base transcription price.
   */
  keyterms?: Array<string>
  /**
   * Audio Url
   *
   * URL of the audio file to transcribe
   */
  audio_url: string
  /**
   * Diarize
   *
   * Whether to annotate who is speaking
   */
  diarize?: boolean
  /**
   * Language Code
   *
   * Language code of the audio
   */
  language_code?: string | unknown
  /**
   * Tag Audio Events
   *
   * Tag audio events like laughter, applause, etc.
   */
  tag_audio_events?: boolean
}

export type SchemaQueueStatus = {
  status: 'IN_QUEUE' | 'IN_PROGRESS' | 'COMPLETED'
  /**
   * The request id.
   */
  request_id: string
  /**
   * The response url.
   */
  response_url?: string
  /**
   * The status url.
   */
  status_url?: string
  /**
   * The cancel url.
   */
  cancel_url?: string
  /**
   * The logs.
   */
  logs?: {
    [key: string]: unknown
  }
  /**
   * The metrics.
   */
  metrics?: {
    [key: string]: unknown
  }
  /**
   * The queue position.
   */
  queue_position?: number
}

export type GetFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/elevenlabs/speech-to-text/scribe-v2/requests/{request_id}/status'
  }

export type GetFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdStatusResponse =
  GetFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdStatusResponses[keyof GetFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdStatusResponses]

export type PutFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/elevenlabs/speech-to-text/scribe-v2/requests/{request_id}/cancel'
  }

export type PutFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdCancelResponse =
  PutFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdCancelResponses[keyof PutFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdCancelResponses]

export type PostFalAiElevenlabsSpeechToTextScribeV2Data = {
  body: SchemaElevenlabsSpeechToTextScribeV2Input
  path?: never
  query?: never
  url: '/fal-ai/elevenlabs/speech-to-text/scribe-v2'
}

export type PostFalAiElevenlabsSpeechToTextScribeV2Responses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiElevenlabsSpeechToTextScribeV2Response =
  PostFalAiElevenlabsSpeechToTextScribeV2Responses[keyof PostFalAiElevenlabsSpeechToTextScribeV2Responses]

export type GetFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/elevenlabs/speech-to-text/scribe-v2/requests/{request_id}'
}

export type GetFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaElevenlabsSpeechToTextScribeV2Output
  }

export type GetFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdResponse =
  GetFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdResponses[keyof GetFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdResponses]

export type GetFalAiSmartTurnRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/smart-turn/requests/{request_id}/status'
}

export type GetFalAiSmartTurnRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiSmartTurnRequestsByRequestIdStatusResponse =
  GetFalAiSmartTurnRequestsByRequestIdStatusResponses[keyof GetFalAiSmartTurnRequestsByRequestIdStatusResponses]

export type PutFalAiSmartTurnRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/smart-turn/requests/{request_id}/cancel'
}

export type PutFalAiSmartTurnRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiSmartTurnRequestsByRequestIdCancelResponse =
  PutFalAiSmartTurnRequestsByRequestIdCancelResponses[keyof PutFalAiSmartTurnRequestsByRequestIdCancelResponses]

export type PostFalAiSmartTurnData = {
  body: SchemaSmartTurnInput
  path?: never
  query?: never
  url: '/fal-ai/smart-turn'
}

export type PostFalAiSmartTurnResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiSmartTurnResponse =
  PostFalAiSmartTurnResponses[keyof PostFalAiSmartTurnResponses]

export type GetFalAiSmartTurnRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/smart-turn/requests/{request_id}'
}

export type GetFalAiSmartTurnRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSmartTurnOutput
}

export type GetFalAiSmartTurnRequestsByRequestIdResponse =
  GetFalAiSmartTurnRequestsByRequestIdResponses[keyof GetFalAiSmartTurnRequestsByRequestIdResponses]

export type GetFalAiSpeechToTextTurboRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/speech-to-text/turbo/requests/{request_id}/status'
}

export type GetFalAiSpeechToTextTurboRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiSpeechToTextTurboRequestsByRequestIdStatusResponse =
  GetFalAiSpeechToTextTurboRequestsByRequestIdStatusResponses[keyof GetFalAiSpeechToTextTurboRequestsByRequestIdStatusResponses]

export type PutFalAiSpeechToTextTurboRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/speech-to-text/turbo/requests/{request_id}/cancel'
}

export type PutFalAiSpeechToTextTurboRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiSpeechToTextTurboRequestsByRequestIdCancelResponse =
  PutFalAiSpeechToTextTurboRequestsByRequestIdCancelResponses[keyof PutFalAiSpeechToTextTurboRequestsByRequestIdCancelResponses]

export type PostFalAiSpeechToTextTurboData = {
  body: SchemaSpeechToTextTurboInput
  path?: never
  query?: never
  url: '/fal-ai/speech-to-text/turbo'
}

export type PostFalAiSpeechToTextTurboResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiSpeechToTextTurboResponse =
  PostFalAiSpeechToTextTurboResponses[keyof PostFalAiSpeechToTextTurboResponses]

export type GetFalAiSpeechToTextTurboRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/speech-to-text/turbo/requests/{request_id}'
}

export type GetFalAiSpeechToTextTurboRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSpeechToTextTurboOutput
}

export type GetFalAiSpeechToTextTurboRequestsByRequestIdResponse =
  GetFalAiSpeechToTextTurboRequestsByRequestIdResponses[keyof GetFalAiSpeechToTextTurboRequestsByRequestIdResponses]

export type GetFalAiSpeechToTextTurboStreamRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/speech-to-text/turbo/stream/requests/{request_id}/status'
}

export type GetFalAiSpeechToTextTurboStreamRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiSpeechToTextTurboStreamRequestsByRequestIdStatusResponse =
  GetFalAiSpeechToTextTurboStreamRequestsByRequestIdStatusResponses[keyof GetFalAiSpeechToTextTurboStreamRequestsByRequestIdStatusResponses]

export type PutFalAiSpeechToTextTurboStreamRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/speech-to-text/turbo/stream/requests/{request_id}/cancel'
}

export type PutFalAiSpeechToTextTurboStreamRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiSpeechToTextTurboStreamRequestsByRequestIdCancelResponse =
  PutFalAiSpeechToTextTurboStreamRequestsByRequestIdCancelResponses[keyof PutFalAiSpeechToTextTurboStreamRequestsByRequestIdCancelResponses]

export type PostFalAiSpeechToTextTurboStreamData = {
  body: SchemaSpeechToTextTurboStreamInput
  path?: never
  query?: never
  url: '/fal-ai/speech-to-text/turbo/stream'
}

export type PostFalAiSpeechToTextTurboStreamResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiSpeechToTextTurboStreamResponse =
  PostFalAiSpeechToTextTurboStreamResponses[keyof PostFalAiSpeechToTextTurboStreamResponses]

export type GetFalAiSpeechToTextTurboStreamRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/speech-to-text/turbo/stream/requests/{request_id}'
}

export type GetFalAiSpeechToTextTurboStreamRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSpeechToTextTurboStreamOutput
}

export type GetFalAiSpeechToTextTurboStreamRequestsByRequestIdResponse =
  GetFalAiSpeechToTextTurboStreamRequestsByRequestIdResponses[keyof GetFalAiSpeechToTextTurboStreamRequestsByRequestIdResponses]

export type GetFalAiSpeechToTextStreamRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/speech-to-text/stream/requests/{request_id}/status'
}

export type GetFalAiSpeechToTextStreamRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiSpeechToTextStreamRequestsByRequestIdStatusResponse =
  GetFalAiSpeechToTextStreamRequestsByRequestIdStatusResponses[keyof GetFalAiSpeechToTextStreamRequestsByRequestIdStatusResponses]

export type PutFalAiSpeechToTextStreamRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/speech-to-text/stream/requests/{request_id}/cancel'
}

export type PutFalAiSpeechToTextStreamRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiSpeechToTextStreamRequestsByRequestIdCancelResponse =
  PutFalAiSpeechToTextStreamRequestsByRequestIdCancelResponses[keyof PutFalAiSpeechToTextStreamRequestsByRequestIdCancelResponses]

export type PostFalAiSpeechToTextStreamData = {
  body: SchemaSpeechToTextStreamInput
  path?: never
  query?: never
  url: '/fal-ai/speech-to-text/stream'
}

export type PostFalAiSpeechToTextStreamResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiSpeechToTextStreamResponse =
  PostFalAiSpeechToTextStreamResponses[keyof PostFalAiSpeechToTextStreamResponses]

export type GetFalAiSpeechToTextStreamRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/speech-to-text/stream/requests/{request_id}'
}

export type GetFalAiSpeechToTextStreamRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSpeechToTextStreamOutput
}

export type GetFalAiSpeechToTextStreamRequestsByRequestIdResponse =
  GetFalAiSpeechToTextStreamRequestsByRequestIdResponses[keyof GetFalAiSpeechToTextStreamRequestsByRequestIdResponses]

export type GetFalAiSpeechToTextRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/speech-to-text/requests/{request_id}/status'
}

export type GetFalAiSpeechToTextRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiSpeechToTextRequestsByRequestIdStatusResponse =
  GetFalAiSpeechToTextRequestsByRequestIdStatusResponses[keyof GetFalAiSpeechToTextRequestsByRequestIdStatusResponses]

export type PutFalAiSpeechToTextRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/speech-to-text/requests/{request_id}/cancel'
}

export type PutFalAiSpeechToTextRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiSpeechToTextRequestsByRequestIdCancelResponse =
  PutFalAiSpeechToTextRequestsByRequestIdCancelResponses[keyof PutFalAiSpeechToTextRequestsByRequestIdCancelResponses]

export type PostFalAiSpeechToTextData = {
  body: SchemaSpeechToTextInput
  path?: never
  query?: never
  url: '/fal-ai/speech-to-text'
}

export type PostFalAiSpeechToTextResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiSpeechToTextResponse =
  PostFalAiSpeechToTextResponses[keyof PostFalAiSpeechToTextResponses]

export type GetFalAiSpeechToTextRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/speech-to-text/requests/{request_id}'
}

export type GetFalAiSpeechToTextRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSpeechToTextOutput
}

export type GetFalAiSpeechToTextRequestsByRequestIdResponse =
  GetFalAiSpeechToTextRequestsByRequestIdResponses[keyof GetFalAiSpeechToTextRequestsByRequestIdResponses]

export type GetFalAiElevenlabsSpeechToTextRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/elevenlabs/speech-to-text/requests/{request_id}/status'
}

export type GetFalAiElevenlabsSpeechToTextRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiElevenlabsSpeechToTextRequestsByRequestIdStatusResponse =
  GetFalAiElevenlabsSpeechToTextRequestsByRequestIdStatusResponses[keyof GetFalAiElevenlabsSpeechToTextRequestsByRequestIdStatusResponses]

export type PutFalAiElevenlabsSpeechToTextRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/elevenlabs/speech-to-text/requests/{request_id}/cancel'
}

export type PutFalAiElevenlabsSpeechToTextRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiElevenlabsSpeechToTextRequestsByRequestIdCancelResponse =
  PutFalAiElevenlabsSpeechToTextRequestsByRequestIdCancelResponses[keyof PutFalAiElevenlabsSpeechToTextRequestsByRequestIdCancelResponses]

export type PostFalAiElevenlabsSpeechToTextData = {
  body: SchemaElevenlabsSpeechToTextInput
  path?: never
  query?: never
  url: '/fal-ai/elevenlabs/speech-to-text'
}

export type PostFalAiElevenlabsSpeechToTextResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiElevenlabsSpeechToTextResponse =
  PostFalAiElevenlabsSpeechToTextResponses[keyof PostFalAiElevenlabsSpeechToTextResponses]

export type GetFalAiElevenlabsSpeechToTextRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/elevenlabs/speech-to-text/requests/{request_id}'
}

export type GetFalAiElevenlabsSpeechToTextRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaElevenlabsSpeechToTextOutput
}

export type GetFalAiElevenlabsSpeechToTextRequestsByRequestIdResponse =
  GetFalAiElevenlabsSpeechToTextRequestsByRequestIdResponses[keyof GetFalAiElevenlabsSpeechToTextRequestsByRequestIdResponses]

export type GetFalAiWizperRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/wizper/requests/{request_id}/status'
}

export type GetFalAiWizperRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiWizperRequestsByRequestIdStatusResponse =
  GetFalAiWizperRequestsByRequestIdStatusResponses[keyof GetFalAiWizperRequestsByRequestIdStatusResponses]

export type PutFalAiWizperRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wizper/requests/{request_id}/cancel'
}

export type PutFalAiWizperRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiWizperRequestsByRequestIdCancelResponse =
  PutFalAiWizperRequestsByRequestIdCancelResponses[keyof PutFalAiWizperRequestsByRequestIdCancelResponses]

export type PostFalAiWizperData = {
  body: SchemaWizperInput
  path?: never
  query?: never
  url: '/fal-ai/wizper'
}

export type PostFalAiWizperResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWizperResponse =
  PostFalAiWizperResponses[keyof PostFalAiWizperResponses]

export type GetFalAiWizperRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wizper/requests/{request_id}'
}

export type GetFalAiWizperRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWizperOutput
}

export type GetFalAiWizperRequestsByRequestIdResponse =
  GetFalAiWizperRequestsByRequestIdResponses[keyof GetFalAiWizperRequestsByRequestIdResponses]

export type GetFalAiWhisperRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/whisper/requests/{request_id}/status'
}

export type GetFalAiWhisperRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiWhisperRequestsByRequestIdStatusResponse =
  GetFalAiWhisperRequestsByRequestIdStatusResponses[keyof GetFalAiWhisperRequestsByRequestIdStatusResponses]

export type PutFalAiWhisperRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/whisper/requests/{request_id}/cancel'
}

export type PutFalAiWhisperRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiWhisperRequestsByRequestIdCancelResponse =
  PutFalAiWhisperRequestsByRequestIdCancelResponses[keyof PutFalAiWhisperRequestsByRequestIdCancelResponses]

export type PostFalAiWhisperData = {
  body: SchemaWhisperInput
  path?: never
  query?: never
  url: '/fal-ai/whisper'
}

export type PostFalAiWhisperResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWhisperResponse =
  PostFalAiWhisperResponses[keyof PostFalAiWhisperResponses]

export type GetFalAiWhisperRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/whisper/requests/{request_id}'
}

export type GetFalAiWhisperRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWhisperOutput
}

export type GetFalAiWhisperRequestsByRequestIdResponse =
  GetFalAiWhisperRequestsByRequestIdResponses[keyof GetFalAiWhisperRequestsByRequestIdResponses]
