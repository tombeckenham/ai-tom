// This file is auto-generated by @hey-api/openapi-ts

export type ClientOptions = {
    baseUrl: `${string}://${string}` | (string & {});
};

export type File = {
    url: string;
    content_type?: string;
    file_name?: string;
    file_size?: number;
};

export type QueueStatus = {
    status: 'IN_PROGRESS' | 'COMPLETED' | 'FAILED';
    response_url?: string;
};

/**
 * SpeechToTextRequestScribeV2
 */
export type ElevenlabsSpeechToTextScribeV2Input = {
    /**
     * Keyterms
     *
     * Words or sentences to bias the model towards transcribing. Up to 100 keyterms, max 50 characters each. Adds 30% premium over base transcription price.
     */
    keyterms?: Array<string>;
    /**
     * Audio Url
     *
     * URL of the audio file to transcribe
     */
    audio_url: string;
    /**
     * Diarize
     *
     * Whether to annotate who is speaking
     */
    diarize?: boolean;
    /**
     * Language Code
     *
     * Language code of the audio
     */
    language_code?: string | unknown;
    /**
     * Tag Audio Events
     *
     * Tag audio events like laughter, applause, etc.
     */
    tag_audio_events?: boolean;
};

/**
 * TranscriptionOutputV2
 */
export type ElevenlabsSpeechToTextScribeV2Output = {
    /**
     * Text
     *
     * The full transcribed text
     */
    text: string;
    /**
     * Language Probability
     *
     * Confidence in language detection
     */
    language_probability: number;
    /**
     * Language Code
     *
     * Detected or specified language code
     */
    language_code: string;
    /**
     * Words
     *
     * Word-level transcription details
     */
    words: Array<TranscriptionWord>;
};

/**
 * TranscriptionWord
 */
export type TranscriptionWord = {
    /**
     * Text
     *
     * The transcribed word or audio event
     */
    text: string;
    /**
     * Start
     *
     * Start time in seconds
     */
    start: number | unknown;
    /**
     * Type
     *
     * Type of element (word, spacing, or audio_event)
     */
    type: string;
    /**
     * End
     *
     * End time in seconds
     */
    end: number | unknown;
    /**
     * Speaker Id
     *
     * Speaker identifier if diarization was enabled
     */
    speaker_id?: string | unknown;
};

/**
 * SmartTurnInput
 */
export type SmartTurnInput = {
    /**
     * Audio Url
     *
     * The URL of the audio file to be processed.
     */
    audio_url: string;
};

/**
 * Output
 */
export type SmartTurnOutput = {
    /**
     * Prediction
     *
     * The predicted turn type. 1 for Complete, 0 for Incomplete.
     */
    prediction: number;
    /**
     * Probability
     *
     * The probability of the predicted turn type.
     */
    probability: number;
    /**
     * Metrics
     *
     * The metrics of the inference.
     */
    metrics: {
        [key: string]: unknown;
    };
};

/**
 * SpeechInput
 */
export type SpeechToTextTurboInput = {
    /**
     * Audio Path
     *
     * Local filesystem path (or remote URL) to a long audio file
     */
    audio_url: string;
    /**
     * Use Punctuation/Capitalization (PnC)
     *
     * Whether to use Canary's built-in punctuation & capitalization
     */
    use_pnc?: boolean;
};

/**
 * SpeechOutput
 */
export type SpeechToTextTurboOutput = {
    /**
     * Partial
     *
     * Indicates if this is a partial (in-progress) transcript
     */
    partial?: boolean;
    /**
     * Transcribed Text
     *
     * The partial or final transcription output from Canary
     */
    output: string;
};

/**
 * SpeechInput
 */
export type SpeechToTextTurboStreamInput = {
    /**
     * Audio Path
     *
     * Local filesystem path (or remote URL) to a long audio file
     */
    audio_url: string;
    /**
     * Use Punctuation/Capitalization (PnC)
     *
     * Whether to use Canary's built-in punctuation & capitalization
     */
    use_pnc?: boolean;
};

export type SpeechToTextTurboStreamOutput = unknown;

/**
 * SpeechInput
 */
export type SpeechToTextStreamInput = {
    /**
     * Audio Path
     *
     * Local filesystem path (or remote URL) to a long audio file
     */
    audio_url: string;
    /**
     * Use Punctuation/Capitalization (PnC)
     *
     * Whether to use Canary's built-in punctuation & capitalization
     */
    use_pnc?: boolean;
};

export type SpeechToTextStreamOutput = unknown;

/**
 * SpeechInput
 */
export type SpeechToTextInput = {
    /**
     * Audio Path
     *
     * Local filesystem path (or remote URL) to a long audio file
     */
    audio_url: string;
    /**
     * Use Punctuation/Capitalization (PnC)
     *
     * Whether to use Canary's built-in punctuation & capitalization
     */
    use_pnc?: boolean;
};

/**
 * SpeechOutput
 */
export type SpeechToTextOutput = {
    /**
     * Partial
     *
     * Indicates if this is a partial (in-progress) transcript
     */
    partial?: boolean;
    /**
     * Transcribed Text
     *
     * The partial or final transcription output from Canary
     */
    output: string;
};

/**
 * SpeechToTextRequest
 */
export type ElevenlabsSpeechToTextInput = {
    /**
     * Language Code
     *
     * Language code of the audio
     */
    language_code?: string | unknown;
    /**
     * Audio Url
     *
     * URL of the audio file to transcribe
     */
    audio_url: string;
    /**
     * Diarize
     *
     * Whether to annotate who is speaking
     */
    diarize?: boolean;
    /**
     * Tag Audio Events
     *
     * Tag audio events like laughter, applause, etc.
     */
    tag_audio_events?: boolean;
};

/**
 * TranscriptionOutput
 */
export type ElevenlabsSpeechToTextOutput = {
    /**
     * Text
     *
     * The full transcribed text
     */
    text: string;
    /**
     * Language Probability
     *
     * Confidence in language detection
     */
    language_probability: number;
    /**
     * Language Code
     *
     * Detected or specified language code
     */
    language_code: string;
    /**
     * Words
     *
     * Word-level transcription details
     */
    words: Array<FalAiElevenlabsSpeechToTextTranscriptionWord>;
};

/**
 * TranscriptionWord
 */
export type FalAiElevenlabsSpeechToTextTranscriptionWord = {
    /**
     * Text
     *
     * The transcribed word or audio event
     */
    text: string;
    /**
     * Start
     *
     * Start time in seconds
     */
    start: number | unknown;
    /**
     * Type
     *
     * Type of element (word, spacing, or audio_event)
     */
    type: string;
    /**
     * End
     *
     * End time in seconds
     */
    end: number | unknown;
    /**
     * Speaker Id
     *
     * Speaker identifier if diarization was enabled
     */
    speaker_id?: string | unknown;
};

/**
 * WhisperInput
 */
export type WizperInput = {
    /**
     * Language
     *
     *
     * Language of the audio file.
     * If translate is selected as the task, the audio will be translated to
     * English, regardless of the language selected. If `None` is passed,
     * the language will be automatically detected. This will also increase
     * the inference time.
     *
     */
    language?: 'af' | 'am' | 'ar' | 'as' | 'az' | 'ba' | 'be' | 'bg' | 'bn' | 'bo' | 'br' | 'bs' | 'ca' | 'cs' | 'cy' | 'da' | 'de' | 'el' | 'en' | 'es' | 'et' | 'eu' | 'fa' | 'fi' | 'fo' | 'fr' | 'gl' | 'gu' | 'ha' | 'haw' | 'he' | 'hi' | 'hr' | 'ht' | 'hu' | 'hy' | 'id' | 'is' | 'it' | 'ja' | 'jw' | 'ka' | 'kk' | 'km' | 'kn' | 'ko' | 'la' | 'lb' | 'ln' | 'lo' | 'lt' | 'lv' | 'mg' | 'mi' | 'mk' | 'ml' | 'mn' | 'mr' | 'ms' | 'mt' | 'my' | 'ne' | 'nl' | 'nn' | 'no' | 'oc' | 'pa' | 'pl' | 'ps' | 'pt' | 'ro' | 'ru' | 'sa' | 'sd' | 'si' | 'sk' | 'sl' | 'sn' | 'so' | 'sq' | 'sr' | 'su' | 'sv' | 'sw' | 'ta' | 'te' | 'tg' | 'th' | 'tk' | 'tl' | 'tr' | 'tt' | 'uk' | 'ur' | 'uz' | 'vi' | 'yi' | 'yo' | 'zh' | unknown;
    /**
     * Version
     *
     * Version of the model to use. All of the models are the Whisper large variant.
     */
    version?: string;
    /**
     * Max Segment Len
     *
     * Maximum speech segment duration in seconds before splitting.
     */
    max_segment_len?: number;
    /**
     * Task
     *
     * Task to perform on the audio file. Either transcribe or translate.
     */
    task?: 'transcribe' | 'translate';
    /**
     * Chunk Level
     *
     * Level of the chunks to return.
     */
    chunk_level?: string;
    /**
     * Audio Url
     *
     * URL of the audio file to transcribe. Supported formats: mp3, mp4, mpeg, mpga, m4a, wav or webm.
     */
    audio_url: string;
    /**
     * Merge Chunks
     *
     * Whether to merge consecutive chunks. When enabled, chunks are merged if their combined duration does not exceed max_segment_len.
     */
    merge_chunks?: boolean;
};

/**
 * WhisperOutput
 */
export type WizperOutput = {
    /**
     * Text
     *
     * Transcription of the audio file
     */
    text: string;
    /**
     * Languages
     *
     * List of languages that the audio file is inferred to be. Defaults to null.
     */
    languages: Array<'af' | 'am' | 'ar' | 'as' | 'az' | 'ba' | 'be' | 'bg' | 'bn' | 'bo' | 'br' | 'bs' | 'ca' | 'cs' | 'cy' | 'da' | 'de' | 'el' | 'en' | 'es' | 'et' | 'eu' | 'fa' | 'fi' | 'fo' | 'fr' | 'gl' | 'gu' | 'ha' | 'haw' | 'he' | 'hi' | 'hr' | 'ht' | 'hu' | 'hy' | 'id' | 'is' | 'it' | 'ja' | 'jw' | 'ka' | 'kk' | 'km' | 'kn' | 'ko' | 'la' | 'lb' | 'ln' | 'lo' | 'lt' | 'lv' | 'mg' | 'mi' | 'mk' | 'ml' | 'mn' | 'mr' | 'ms' | 'mt' | 'my' | 'ne' | 'nl' | 'nn' | 'no' | 'oc' | 'pa' | 'pl' | 'ps' | 'pt' | 'ro' | 'ru' | 'sa' | 'sd' | 'si' | 'sk' | 'sl' | 'sn' | 'so' | 'sq' | 'sr' | 'su' | 'sv' | 'sw' | 'ta' | 'te' | 'tg' | 'th' | 'tk' | 'tl' | 'tr' | 'tt' | 'uk' | 'ur' | 'uz' | 'vi' | 'yi' | 'yo' | 'zh'>;
    /**
     * Chunks
     *
     * Timestamp chunks of the audio file
     */
    chunks: Array<WhisperChunk>;
};

/**
 * WhisperChunk
 */
export type WhisperChunk = {
    /**
     * Text
     *
     * Transcription of the chunk
     */
    text: string;
    /**
     * Timestamp
     *
     * Start and end timestamp of the chunk
     */
    timestamp: [
    ];
};

/**
 * WhisperInput
 */
export type WhisperInput = {
    /**
     * Version
     *
     * Version of the model to use. All of the models are the Whisper large variant.
     */
    version?: '3';
    /**
     * Batch Size
     */
    batch_size?: number;
    /**
     * Language
     *
     *
     * Language of the audio file. If set to null, the language will be
     * automatically detected. Defaults to null.
     *
     * If translate is selected as the task, the audio will be translated to
     * English, regardless of the language selected.
     *
     */
    language?: 'af' | 'am' | 'ar' | 'as' | 'az' | 'ba' | 'be' | 'bg' | 'bn' | 'bo' | 'br' | 'bs' | 'ca' | 'cs' | 'cy' | 'da' | 'de' | 'el' | 'en' | 'es' | 'et' | 'eu' | 'fa' | 'fi' | 'fo' | 'fr' | 'gl' | 'gu' | 'ha' | 'haw' | 'he' | 'hi' | 'hr' | 'ht' | 'hu' | 'hy' | 'id' | 'is' | 'it' | 'ja' | 'jw' | 'ka' | 'kk' | 'km' | 'kn' | 'ko' | 'la' | 'lb' | 'ln' | 'lo' | 'lt' | 'lv' | 'mg' | 'mi' | 'mk' | 'ml' | 'mn' | 'mr' | 'ms' | 'mt' | 'my' | 'ne' | 'nl' | 'nn' | 'no' | 'oc' | 'pa' | 'pl' | 'ps' | 'pt' | 'ro' | 'ru' | 'sa' | 'sd' | 'si' | 'sk' | 'sl' | 'sn' | 'so' | 'sq' | 'sr' | 'su' | 'sv' | 'sw' | 'ta' | 'te' | 'tg' | 'th' | 'tk' | 'tl' | 'tr' | 'tt' | 'uk' | 'ur' | 'uz' | 'vi' | 'yi' | 'yo' | 'zh';
    /**
     * Prompt
     *
     * Prompt to use for generation. Defaults to an empty string.
     */
    prompt?: string;
    /**
     * Num Speakers
     *
     *
     * Number of speakers in the audio file. Defaults to null.
     * If not provided, the number of speakers will be automatically
     * detected.
     *
     */
    num_speakers?: number | null;
    /**
     * Task
     *
     * Task to perform on the audio file. Either transcribe or translate.
     */
    task?: 'transcribe' | 'translate';
    /**
     * Chunk Level
     *
     * Level of the chunks to return. Either none, segment or word. `none` would imply that all of the audio will be transcribed without the timestamp tokens, we suggest to switch to `none` if you are not satisfied with the transcription quality, since it will usually improve the quality of the results. Switching to `none` will also provide minor speed ups in the transcription due to less amount of generated tokens. Notice that setting to none will produce **a single chunk with the whole transcription**.
     */
    chunk_level?: 'none' | 'segment' | 'word';
    /**
     * Audio Url
     *
     * URL of the audio file to transcribe. Supported formats: mp3, mp4, mpeg, mpga, m4a, wav or webm.
     */
    audio_url: string;
    /**
     * Diarize
     *
     * Whether to diarize the audio file. Defaults to false. Setting to true will add costs proportional to diarization inference time.
     */
    diarize?: boolean;
};

/**
 * WhisperOutput
 */
export type WhisperOutput = {
    /**
     * Text
     *
     * Transcription of the audio file
     */
    text: string;
    /**
     * Inferred Languages
     *
     * List of languages that the audio file is inferred to be. Defaults to null.
     */
    inferred_languages: Array<'af' | 'am' | 'ar' | 'as' | 'az' | 'ba' | 'be' | 'bg' | 'bn' | 'bo' | 'br' | 'bs' | 'ca' | 'cs' | 'cy' | 'da' | 'de' | 'el' | 'en' | 'es' | 'et' | 'eu' | 'fa' | 'fi' | 'fo' | 'fr' | 'gl' | 'gu' | 'ha' | 'haw' | 'he' | 'hi' | 'hr' | 'ht' | 'hu' | 'hy' | 'id' | 'is' | 'it' | 'ja' | 'jw' | 'ka' | 'kk' | 'km' | 'kn' | 'ko' | 'la' | 'lb' | 'ln' | 'lo' | 'lt' | 'lv' | 'mg' | 'mi' | 'mk' | 'ml' | 'mn' | 'mr' | 'ms' | 'mt' | 'my' | 'ne' | 'nl' | 'nn' | 'no' | 'oc' | 'pa' | 'pl' | 'ps' | 'pt' | 'ro' | 'ru' | 'sa' | 'sd' | 'si' | 'sk' | 'sl' | 'sn' | 'so' | 'sq' | 'sr' | 'su' | 'sv' | 'sw' | 'ta' | 'te' | 'tg' | 'th' | 'tk' | 'tl' | 'tr' | 'tt' | 'uk' | 'ur' | 'uz' | 'vi' | 'yi' | 'yo' | 'zh'>;
    /**
     * Chunks
     *
     * Timestamp chunks of the audio file
     */
    chunks?: Array<FalAiWhisperWhisperChunk>;
    /**
     * Diarization Segments
     *
     * Speaker diarization segments of the audio file. Only present if diarization is enabled.
     */
    diarization_segments: Array<DiarizationSegment>;
};

/**
 * WhisperChunk
 */
export type FalAiWhisperWhisperChunk = {
    /**
     * Text
     *
     * Transcription of the chunk
     */
    text: string;
    /**
     * Timestamp
     *
     * Start and end timestamp of the chunk
     */
    timestamp: [
        unknown,
        unknown
    ];
    /**
     * Speaker
     *
     * Speaker ID of the chunk. Only present if diarization is enabled.
     */
    speaker?: string;
};

/**
 * DiarizationSegment
 */
export type DiarizationSegment = {
    /**
     * Timestamp
     *
     * Start and end timestamp of the segment
     */
    timestamp: [
        unknown,
        unknown
    ];
    /**
     * Speaker
     *
     * Speaker ID of the segment
     */
    speaker: string;
};
