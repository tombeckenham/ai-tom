// This file is auto-generated by @hey-api/openapi-ts

import { z } from 'zod'

export const zFile = z.object({
  url: z.url(),
  content_type: z.optional(z.string()),
  file_name: z.optional(z.string()),
  file_size: z.optional(z.int()),
})

export const zQueueStatus = z.object({
  status: z.enum(['IN_PROGRESS', 'COMPLETED', 'FAILED']),
  response_url: z.optional(z.url()),
})

/**
 * SpeechToTextRequestScribeV2
 */
export const zElevenlabsSpeechToTextScribeV2Input = z.object({
  keyterms: z
    .optional(
      z.array(z.string()).max(100).register(z.globalRegistry, {
        description:
          'Words or sentences to bias the model towards transcribing. Up to 100 keyterms, max 50 characters each. Adds 30% premium over base transcription price.',
      }),
    )
    .default([]),
  audio_url: z.string().register(z.globalRegistry, {
    description: 'URL of the audio file to transcribe',
  }),
  diarize: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to annotate who is speaking',
      }),
    )
    .default(true),
  language_code: z.optional(z.union([z.string(), z.unknown()])),
  tag_audio_events: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Tag audio events like laughter, applause, etc.',
      }),
    )
    .default(true),
})

/**
 * TranscriptionWord
 */
export const zTranscriptionWord = z.object({
  text: z.string().register(z.globalRegistry, {
    description: 'The transcribed word or audio event',
  }),
  start: z.union([z.number(), z.unknown()]),
  type: z.string().register(z.globalRegistry, {
    description: 'Type of element (word, spacing, or audio_event)',
  }),
  end: z.union([z.number(), z.unknown()]),
  speaker_id: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * TranscriptionOutputV2
 */
export const zElevenlabsSpeechToTextScribeV2Output = z.object({
  text: z.string().register(z.globalRegistry, {
    description: 'The full transcribed text',
  }),
  language_probability: z.number().register(z.globalRegistry, {
    description: 'Confidence in language detection',
  }),
  language_code: z.string().register(z.globalRegistry, {
    description: 'Detected or specified language code',
  }),
  words: z.array(zTranscriptionWord).register(z.globalRegistry, {
    description: 'Word-level transcription details',
  }),
})

/**
 * SmartTurnInput
 */
export const zSmartTurnInput = z.object({
  audio_url: z.string().register(z.globalRegistry, {
    description: 'The URL of the audio file to be processed.',
  }),
})

/**
 * Output
 */
export const zSmartTurnOutput = z.object({
  prediction: z.int().register(z.globalRegistry, {
    description: 'The predicted turn type. 1 for Complete, 0 for Incomplete.',
  }),
  probability: z.number().register(z.globalRegistry, {
    description: 'The probability of the predicted turn type.',
  }),
  metrics: z.record(z.string(), z.unknown()).register(z.globalRegistry, {
    description: 'The metrics of the inference.',
  }),
})

/**
 * SpeechInput
 */
export const zSpeechToTextTurboInput = z.object({
  audio_url: z.string().register(z.globalRegistry, {
    description: 'Local filesystem path (or remote URL) to a long audio file',
  }),
  use_pnc: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "Whether to use Canary's built-in punctuation & capitalization",
      }),
    )
    .default(true),
})

/**
 * SpeechOutput
 */
export const zSpeechToTextTurboOutput = z.object({
  partial: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Indicates if this is a partial (in-progress) transcript',
      }),
    )
    .default(false),
  output: z.string().register(z.globalRegistry, {
    description: 'The partial or final transcription output from Canary',
  }),
})

/**
 * SpeechInput
 */
export const zSpeechToTextTurboStreamInput = z.object({
  audio_url: z.string().register(z.globalRegistry, {
    description: 'Local filesystem path (or remote URL) to a long audio file',
  }),
  use_pnc: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "Whether to use Canary's built-in punctuation & capitalization",
      }),
    )
    .default(true),
})

export const zSpeechToTextTurboStreamOutput = z.unknown()

/**
 * SpeechInput
 */
export const zSpeechToTextStreamInput = z.object({
  audio_url: z.string().register(z.globalRegistry, {
    description: 'Local filesystem path (or remote URL) to a long audio file',
  }),
  use_pnc: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "Whether to use Canary's built-in punctuation & capitalization",
      }),
    )
    .default(true),
})

export const zSpeechToTextStreamOutput = z.unknown()

/**
 * SpeechInput
 */
export const zSpeechToTextInput = z.object({
  audio_url: z.string().register(z.globalRegistry, {
    description: 'Local filesystem path (or remote URL) to a long audio file',
  }),
  use_pnc: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "Whether to use Canary's built-in punctuation & capitalization",
      }),
    )
    .default(true),
})

/**
 * SpeechOutput
 */
export const zSpeechToTextOutput = z.object({
  partial: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Indicates if this is a partial (in-progress) transcript',
      }),
    )
    .default(false),
  output: z.string().register(z.globalRegistry, {
    description: 'The partial or final transcription output from Canary',
  }),
})

/**
 * SpeechToTextRequest
 */
export const zElevenlabsSpeechToTextInput = z.object({
  language_code: z.optional(z.union([z.string(), z.unknown()])),
  audio_url: z.string().register(z.globalRegistry, {
    description: 'URL of the audio file to transcribe',
  }),
  diarize: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to annotate who is speaking',
      }),
    )
    .default(true),
  tag_audio_events: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Tag audio events like laughter, applause, etc.',
      }),
    )
    .default(true),
})

/**
 * TranscriptionWord
 */
export const zFalAiElevenlabsSpeechToTextTranscriptionWord = z.object({
  text: z.string().register(z.globalRegistry, {
    description: 'The transcribed word or audio event',
  }),
  start: z.union([z.number(), z.unknown()]),
  type: z.string().register(z.globalRegistry, {
    description: 'Type of element (word, spacing, or audio_event)',
  }),
  end: z.union([z.number(), z.unknown()]),
  speaker_id: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * TranscriptionOutput
 */
export const zElevenlabsSpeechToTextOutput = z.object({
  text: z.string().register(z.globalRegistry, {
    description: 'The full transcribed text',
  }),
  language_probability: z.number().register(z.globalRegistry, {
    description: 'Confidence in language detection',
  }),
  language_code: z.string().register(z.globalRegistry, {
    description: 'Detected or specified language code',
  }),
  words: z
    .array(zFalAiElevenlabsSpeechToTextTranscriptionWord)
    .register(z.globalRegistry, {
      description: 'Word-level transcription details',
    }),
})

/**
 * WhisperInput
 */
export const zWizperInput = z.object({
  language: z.optional(
    z.union([
      z.enum([
        'af',
        'am',
        'ar',
        'as',
        'az',
        'ba',
        'be',
        'bg',
        'bn',
        'bo',
        'br',
        'bs',
        'ca',
        'cs',
        'cy',
        'da',
        'de',
        'el',
        'en',
        'es',
        'et',
        'eu',
        'fa',
        'fi',
        'fo',
        'fr',
        'gl',
        'gu',
        'ha',
        'haw',
        'he',
        'hi',
        'hr',
        'ht',
        'hu',
        'hy',
        'id',
        'is',
        'it',
        'ja',
        'jw',
        'ka',
        'kk',
        'km',
        'kn',
        'ko',
        'la',
        'lb',
        'ln',
        'lo',
        'lt',
        'lv',
        'mg',
        'mi',
        'mk',
        'ml',
        'mn',
        'mr',
        'ms',
        'mt',
        'my',
        'ne',
        'nl',
        'nn',
        'no',
        'oc',
        'pa',
        'pl',
        'ps',
        'pt',
        'ro',
        'ru',
        'sa',
        'sd',
        'si',
        'sk',
        'sl',
        'sn',
        'so',
        'sq',
        'sr',
        'su',
        'sv',
        'sw',
        'ta',
        'te',
        'tg',
        'th',
        'tk',
        'tl',
        'tr',
        'tt',
        'uk',
        'ur',
        'uz',
        'vi',
        'yi',
        'yo',
        'zh',
      ]),
      z.unknown(),
    ]),
  ),
  version: z
    .optional(
      z.string().register(z.globalRegistry, {
        description:
          'Version of the model to use. All of the models are the Whisper large variant.',
      }),
    )
    .default('3'),
  max_segment_len: z
    .optional(
      z.int().gte(10).lte(29).register(z.globalRegistry, {
        description:
          'Maximum speech segment duration in seconds before splitting.',
      }),
    )
    .default(29),
  task: z.optional(
    z.enum(['transcribe', 'translate']).register(z.globalRegistry, {
      description:
        'Task to perform on the audio file. Either transcribe or translate.',
    }),
  ),
  chunk_level: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'Level of the chunks to return.',
      }),
    )
    .default('segment'),
  audio_url: z.string().register(z.globalRegistry, {
    description:
      'URL of the audio file to transcribe. Supported formats: mp3, mp4, mpeg, mpga, m4a, wav or webm.',
  }),
  merge_chunks: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to merge consecutive chunks. When enabled, chunks are merged if their combined duration does not exceed max_segment_len.',
      }),
    )
    .default(true),
})

/**
 * WhisperChunk
 */
export const zWhisperChunk = z.object({
  text: z.string().register(z.globalRegistry, {
    description: 'Transcription of the chunk',
  }),
  timestamp: z.tuple([]).register(z.globalRegistry, {
    description: 'Start and end timestamp of the chunk',
  }),
})

/**
 * WhisperOutput
 */
export const zWizperOutput = z.object({
  text: z.string().register(z.globalRegistry, {
    description: 'Transcription of the audio file',
  }),
  languages: z
    .array(
      z.enum([
        'af',
        'am',
        'ar',
        'as',
        'az',
        'ba',
        'be',
        'bg',
        'bn',
        'bo',
        'br',
        'bs',
        'ca',
        'cs',
        'cy',
        'da',
        'de',
        'el',
        'en',
        'es',
        'et',
        'eu',
        'fa',
        'fi',
        'fo',
        'fr',
        'gl',
        'gu',
        'ha',
        'haw',
        'he',
        'hi',
        'hr',
        'ht',
        'hu',
        'hy',
        'id',
        'is',
        'it',
        'ja',
        'jw',
        'ka',
        'kk',
        'km',
        'kn',
        'ko',
        'la',
        'lb',
        'ln',
        'lo',
        'lt',
        'lv',
        'mg',
        'mi',
        'mk',
        'ml',
        'mn',
        'mr',
        'ms',
        'mt',
        'my',
        'ne',
        'nl',
        'nn',
        'no',
        'oc',
        'pa',
        'pl',
        'ps',
        'pt',
        'ro',
        'ru',
        'sa',
        'sd',
        'si',
        'sk',
        'sl',
        'sn',
        'so',
        'sq',
        'sr',
        'su',
        'sv',
        'sw',
        'ta',
        'te',
        'tg',
        'th',
        'tk',
        'tl',
        'tr',
        'tt',
        'uk',
        'ur',
        'uz',
        'vi',
        'yi',
        'yo',
        'zh',
      ]),
    )
    .register(z.globalRegistry, {
      description:
        'List of languages that the audio file is inferred to be. Defaults to null.',
    }),
  chunks: z.array(zWhisperChunk).register(z.globalRegistry, {
    description: 'Timestamp chunks of the audio file',
  }),
})

/**
 * WhisperInput
 */
export const zWhisperInput = z.object({
  version: z.optional(
    z.enum(['3']).register(z.globalRegistry, {
      description:
        'Version of the model to use. All of the models are the Whisper large variant.',
    }),
  ),
  batch_size: z.optional(z.int().gte(1).lte(64)).default(64),
  language: z.optional(
    z
      .enum([
        'af',
        'am',
        'ar',
        'as',
        'az',
        'ba',
        'be',
        'bg',
        'bn',
        'bo',
        'br',
        'bs',
        'ca',
        'cs',
        'cy',
        'da',
        'de',
        'el',
        'en',
        'es',
        'et',
        'eu',
        'fa',
        'fi',
        'fo',
        'fr',
        'gl',
        'gu',
        'ha',
        'haw',
        'he',
        'hi',
        'hr',
        'ht',
        'hu',
        'hy',
        'id',
        'is',
        'it',
        'ja',
        'jw',
        'ka',
        'kk',
        'km',
        'kn',
        'ko',
        'la',
        'lb',
        'ln',
        'lo',
        'lt',
        'lv',
        'mg',
        'mi',
        'mk',
        'ml',
        'mn',
        'mr',
        'ms',
        'mt',
        'my',
        'ne',
        'nl',
        'nn',
        'no',
        'oc',
        'pa',
        'pl',
        'ps',
        'pt',
        'ro',
        'ru',
        'sa',
        'sd',
        'si',
        'sk',
        'sl',
        'sn',
        'so',
        'sq',
        'sr',
        'su',
        'sv',
        'sw',
        'ta',
        'te',
        'tg',
        'th',
        'tk',
        'tl',
        'tr',
        'tt',
        'uk',
        'ur',
        'uz',
        'vi',
        'yi',
        'yo',
        'zh',
      ])
      .register(z.globalRegistry, {
        description:
          '\n        Language of the audio file. If set to null, the language will be\n        automatically detected. Defaults to null.\n\n        If translate is selected as the task, the audio will be translated to\n        English, regardless of the language selected.\n        ',
      }),
  ),
  prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description:
          'Prompt to use for generation. Defaults to an empty string.',
      }),
    )
    .default(''),
  num_speakers: z.optional(z.union([z.int().gte(1), z.null()])),
  task: z.optional(
    z.enum(['transcribe', 'translate']).register(z.globalRegistry, {
      description:
        'Task to perform on the audio file. Either transcribe or translate.',
    }),
  ),
  chunk_level: z.optional(
    z.enum(['none', 'segment', 'word']).register(z.globalRegistry, {
      description:
        'Level of the chunks to return. Either none, segment or word. `none` would imply that all of the audio will be transcribed without the timestamp tokens, we suggest to switch to `none` if you are not satisfied with the transcription quality, since it will usually improve the quality of the results. Switching to `none` will also provide minor speed ups in the transcription due to less amount of generated tokens. Notice that setting to none will produce **a single chunk with the whole transcription**.',
    }),
  ),
  audio_url: z.string().register(z.globalRegistry, {
    description:
      'URL of the audio file to transcribe. Supported formats: mp3, mp4, mpeg, mpga, m4a, wav or webm.',
  }),
  diarize: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to diarize the audio file. Defaults to false. Setting to true will add costs proportional to diarization inference time.',
      }),
    )
    .default(false),
})

/**
 * WhisperChunk
 */
export const zFalAiWhisperWhisperChunk = z.object({
  text: z.string().register(z.globalRegistry, {
    description: 'Transcription of the chunk',
  }),
  timestamp: z.tuple([z.unknown(), z.unknown()]).register(z.globalRegistry, {
    description: 'Start and end timestamp of the chunk',
  }),
  speaker: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'Speaker ID of the chunk. Only present if diarization is enabled.',
    }),
  ),
})

/**
 * DiarizationSegment
 */
export const zDiarizationSegment = z.object({
  timestamp: z.tuple([z.unknown(), z.unknown()]).register(z.globalRegistry, {
    description: 'Start and end timestamp of the segment',
  }),
  speaker: z.string().register(z.globalRegistry, {
    description: 'Speaker ID of the segment',
  }),
})

/**
 * WhisperOutput
 */
export const zWhisperOutput = z.object({
  text: z.string().register(z.globalRegistry, {
    description: 'Transcription of the audio file',
  }),
  inferred_languages: z
    .array(
      z.enum([
        'af',
        'am',
        'ar',
        'as',
        'az',
        'ba',
        'be',
        'bg',
        'bn',
        'bo',
        'br',
        'bs',
        'ca',
        'cs',
        'cy',
        'da',
        'de',
        'el',
        'en',
        'es',
        'et',
        'eu',
        'fa',
        'fi',
        'fo',
        'fr',
        'gl',
        'gu',
        'ha',
        'haw',
        'he',
        'hi',
        'hr',
        'ht',
        'hu',
        'hy',
        'id',
        'is',
        'it',
        'ja',
        'jw',
        'ka',
        'kk',
        'km',
        'kn',
        'ko',
        'la',
        'lb',
        'ln',
        'lo',
        'lt',
        'lv',
        'mg',
        'mi',
        'mk',
        'ml',
        'mn',
        'mr',
        'ms',
        'mt',
        'my',
        'ne',
        'nl',
        'nn',
        'no',
        'oc',
        'pa',
        'pl',
        'ps',
        'pt',
        'ro',
        'ru',
        'sa',
        'sd',
        'si',
        'sk',
        'sl',
        'sn',
        'so',
        'sq',
        'sr',
        'su',
        'sv',
        'sw',
        'ta',
        'te',
        'tg',
        'th',
        'tk',
        'tl',
        'tr',
        'tt',
        'uk',
        'ur',
        'uz',
        'vi',
        'yi',
        'yo',
        'zh',
      ]),
    )
    .register(z.globalRegistry, {
      description:
        'List of languages that the audio file is inferred to be. Defaults to null.',
    }),
  chunks: z.optional(
    z.array(zFalAiWhisperWhisperChunk).register(z.globalRegistry, {
      description: 'Timestamp chunks of the audio file',
    }),
  ),
  diarization_segments: z
    .array(zDiarizationSegment)
    .register(z.globalRegistry, {
      description:
        'Speaker diarization segments of the audio file. Only present if diarization is enabled.',
    }),
})
