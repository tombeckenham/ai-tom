// This file is auto-generated by @hey-api/openapi-ts

export type ClientOptions = {
  baseUrl: `${string}://${string}` | (string & {})
}

export type File = {
  url: string
  content_type?: string
  file_name?: string
  file_size?: number
}

export type QueueStatus = {
  status: 'IN_PROGRESS' | 'COMPLETED' | 'FAILED'
  response_url?: string
}

/**
 * VideoEnterpriseInput
 */
export type RouterVideoEnterpriseInput = {
  /**
   * Prompt
   *
   * Prompt to be used for the video processing
   */
  prompt: string
  /**
   * Video Urls
   *
   * List of URLs or data URIs of video files to process. Supported formats: mp4, mpeg, mov, webm. For Google Gemini on AI Studio, YouTube links are also supported. Mutually exclusive with video_url.
   */
  video_urls?: Array<string>
  /**
   * System Prompt
   *
   * System prompt to provide context or instructions to the model
   */
  system_prompt?: string
  /**
   * Reasoning
   *
   * Should reasoning be the part of the final answer.
   */
  reasoning?: boolean
  /**
   * Model
   *
   * Name of the model to use. Charged based on actual token usage.
   */
  model: string
  /**
   * Max Tokens
   *
   * This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.
   */
  max_tokens?: number
  /**
   * Temperature
   *
   * This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input.
   */
  temperature?: number
}

/**
 * VideoOutput
 */
export type RouterVideoEnterpriseOutput = {
  /**
   * Usage
   *
   * Token usage information
   */
  usage?: UsageInfo
  /**
   * Output
   *
   * Generated output from video processing
   */
  output: string
}

/**
 * UsageInfo
 */
export type UsageInfo = {
  /**
   * Prompt Tokens
   */
  prompt_tokens?: number
  /**
   * Total Tokens
   */
  total_tokens?: number
  /**
   * Completion Tokens
   */
  completion_tokens?: number
  /**
   * Cost
   */
  cost: number
}

/**
 * VideoInput
 */
export type RouterVideoInput = {
  /**
   * Prompt
   *
   * Prompt to be used for the video processing
   */
  prompt: string
  /**
   * Video Urls
   *
   * List of URLs or data URIs of video files to process. Supported formats: mp4, mpeg, mov, webm. For Google Gemini on AI Studio, YouTube links are also supported. Mutually exclusive with video_url.
   */
  video_urls?: Array<string>
  /**
   * System Prompt
   *
   * System prompt to provide context or instructions to the model
   */
  system_prompt?: string
  /**
   * Reasoning
   *
   * Should reasoning be the part of the final answer.
   */
  reasoning?: boolean
  /**
   * Model
   *
   * Name of the model to use. Charged based on actual token usage.
   */
  model: string
  /**
   * Max Tokens
   *
   * This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.
   */
  max_tokens?: number
  /**
   * Temperature
   *
   * This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input.
   */
  temperature?: number
}

/**
 * VideoOutput
 */
export type RouterVideoOutput = {
  /**
   * Usage
   *
   * Token usage information
   */
  usage?: OpenrouterRouterVideoUsageInfo
  /**
   * Output
   *
   * Generated output from video processing
   */
  output: string
}

/**
 * UsageInfo
 */
export type OpenrouterRouterVideoUsageInfo = {
  /**
   * Prompt Tokens
   */
  prompt_tokens?: number
  /**
   * Total Tokens
   */
  total_tokens?: number
  /**
   * Completion Tokens
   */
  completion_tokens?: number
  /**
   * Cost
   */
  cost: number
}
