// This file is auto-generated by @hey-api/openapi-ts

export type ClientOptions = {
  baseUrl: 'https://queue.fal.run' | (string & {})
}

/**
 * VideoOutput
 */
export type SchemaRouterVideoOutput = {
  /**
   * Usage
   *
   * Token usage information
   */
  usage?: SchemaUsageInfo
  /**
   * Output
   *
   * Generated output from video processing
   */
  output: string
}

/**
 * UsageInfo
 */
export type SchemaUsageInfo = {
  /**
   * Prompt Tokens
   */
  prompt_tokens?: number
  /**
   * Total Tokens
   */
  total_tokens?: number
  /**
   * Completion Tokens
   */
  completion_tokens?: number
  /**
   * Cost
   */
  cost: number
}

/**
 * VideoInput
 */
export type SchemaRouterVideoInput = {
  /**
   * Prompt
   *
   * Prompt to be used for the video processing
   */
  prompt: string
  /**
   * Video Urls
   *
   * List of URLs or data URIs of video files to process. Supported formats: mp4, mpeg, mov, webm. For Google Gemini on AI Studio, YouTube links are also supported. Mutually exclusive with video_url.
   */
  video_urls?: Array<string>
  /**
   * System Prompt
   *
   * System prompt to provide context or instructions to the model
   */
  system_prompt?: string
  /**
   * Reasoning
   *
   * Should reasoning be the part of the final answer.
   */
  reasoning?: boolean
  /**
   * Model
   *
   * Name of the model to use. Charged based on actual token usage.
   */
  model: string
  /**
   * Max Tokens
   *
   * This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.
   */
  max_tokens?: number
  /**
   * Temperature
   *
   * This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input.
   */
  temperature?: number
}

/**
 * VideoOutput
 */
export type SchemaRouterVideoEnterpriseOutput = {
  /**
   * Usage
   *
   * Token usage information
   */
  usage?: SchemaUsageInfo
  /**
   * Output
   *
   * Generated output from video processing
   */
  output: string
}

/**
 * VideoEnterpriseInput
 */
export type SchemaRouterVideoEnterpriseInput = {
  /**
   * Prompt
   *
   * Prompt to be used for the video processing
   */
  prompt: string
  /**
   * Video Urls
   *
   * List of URLs or data URIs of video files to process. Supported formats: mp4, mpeg, mov, webm. For Google Gemini on AI Studio, YouTube links are also supported. Mutually exclusive with video_url.
   */
  video_urls?: Array<string>
  /**
   * System Prompt
   *
   * System prompt to provide context or instructions to the model
   */
  system_prompt?: string
  /**
   * Reasoning
   *
   * Should reasoning be the part of the final answer.
   */
  reasoning?: boolean
  /**
   * Model
   *
   * Name of the model to use. Charged based on actual token usage.
   */
  model: string
  /**
   * Max Tokens
   *
   * This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.
   */
  max_tokens?: number
  /**
   * Temperature
   *
   * This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input.
   */
  temperature?: number
}

export type SchemaQueueStatus = {
  status: 'IN_QUEUE' | 'IN_PROGRESS' | 'COMPLETED'
  /**
   * The request id.
   */
  request_id: string
  /**
   * The response url.
   */
  response_url?: string
  /**
   * The status url.
   */
  status_url?: string
  /**
   * The cancel url.
   */
  cancel_url?: string
  /**
   * The logs.
   */
  logs?: {
    [key: string]: unknown
  }
  /**
   * The metrics.
   */
  metrics?: {
    [key: string]: unknown
  }
  /**
   * The queue position.
   */
  queue_position?: number
}

export type GetOpenrouterRouterVideoEnterpriseRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/openrouter/router/video/enterprise/requests/{request_id}/status'
}

export type GetOpenrouterRouterVideoEnterpriseRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetOpenrouterRouterVideoEnterpriseRequestsByRequestIdStatusResponse =
  GetOpenrouterRouterVideoEnterpriseRequestsByRequestIdStatusResponses[keyof GetOpenrouterRouterVideoEnterpriseRequestsByRequestIdStatusResponses]

export type PutOpenrouterRouterVideoEnterpriseRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/openrouter/router/video/enterprise/requests/{request_id}/cancel'
}

export type PutOpenrouterRouterVideoEnterpriseRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutOpenrouterRouterVideoEnterpriseRequestsByRequestIdCancelResponse =
  PutOpenrouterRouterVideoEnterpriseRequestsByRequestIdCancelResponses[keyof PutOpenrouterRouterVideoEnterpriseRequestsByRequestIdCancelResponses]

export type PostOpenrouterRouterVideoEnterpriseData = {
  body: SchemaRouterVideoEnterpriseInput
  path?: never
  query?: never
  url: '/openrouter/router/video/enterprise'
}

export type PostOpenrouterRouterVideoEnterpriseResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostOpenrouterRouterVideoEnterpriseResponse =
  PostOpenrouterRouterVideoEnterpriseResponses[keyof PostOpenrouterRouterVideoEnterpriseResponses]

export type GetOpenrouterRouterVideoEnterpriseRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/openrouter/router/video/enterprise/requests/{request_id}'
}

export type GetOpenrouterRouterVideoEnterpriseRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaRouterVideoEnterpriseOutput
}

export type GetOpenrouterRouterVideoEnterpriseRequestsByRequestIdResponse =
  GetOpenrouterRouterVideoEnterpriseRequestsByRequestIdResponses[keyof GetOpenrouterRouterVideoEnterpriseRequestsByRequestIdResponses]

export type GetOpenrouterRouterVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/openrouter/router/video/requests/{request_id}/status'
}

export type GetOpenrouterRouterVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetOpenrouterRouterVideoRequestsByRequestIdStatusResponse =
  GetOpenrouterRouterVideoRequestsByRequestIdStatusResponses[keyof GetOpenrouterRouterVideoRequestsByRequestIdStatusResponses]

export type PutOpenrouterRouterVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/openrouter/router/video/requests/{request_id}/cancel'
}

export type PutOpenrouterRouterVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutOpenrouterRouterVideoRequestsByRequestIdCancelResponse =
  PutOpenrouterRouterVideoRequestsByRequestIdCancelResponses[keyof PutOpenrouterRouterVideoRequestsByRequestIdCancelResponses]

export type PostOpenrouterRouterVideoData = {
  body: SchemaRouterVideoInput
  path?: never
  query?: never
  url: '/openrouter/router/video'
}

export type PostOpenrouterRouterVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostOpenrouterRouterVideoResponse =
  PostOpenrouterRouterVideoResponses[keyof PostOpenrouterRouterVideoResponses]

export type GetOpenrouterRouterVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/openrouter/router/video/requests/{request_id}'
}

export type GetOpenrouterRouterVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaRouterVideoOutput
}

export type GetOpenrouterRouterVideoRequestsByRequestIdResponse =
  GetOpenrouterRouterVideoRequestsByRequestIdResponses[keyof GetOpenrouterRouterVideoRequestsByRequestIdResponses]
