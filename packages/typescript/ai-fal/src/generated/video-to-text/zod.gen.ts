// This file is auto-generated by @hey-api/openapi-ts

import { z } from 'zod'

export const zFile = z.object({
  url: z.url(),
  content_type: z.optional(z.string()),
  file_name: z.optional(z.string()),
  file_size: z.optional(z.int()),
})

export const zQueueStatus = z.object({
  status: z.enum(['IN_PROGRESS', 'COMPLETED', 'FAILED']),
  response_url: z.optional(z.url()),
})

/**
 * VideoEnterpriseInput
 */
export const zRouterVideoEnterpriseInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'Prompt to be used for the video processing',
  }),
  video_urls: z.optional(
    z.array(z.string()).register(z.globalRegistry, {
      description:
        'List of URLs or data URIs of video files to process. Supported formats: mp4, mpeg, mov, webm. For Google Gemini on AI Studio, YouTube links are also supported. Mutually exclusive with video_url.',
    }),
  ),
  system_prompt: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'System prompt to provide context or instructions to the model',
    }),
  ),
  reasoning: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Should reasoning be the part of the final answer.',
      }),
    )
    .default(false),
  model: z.string().register(z.globalRegistry, {
    description:
      'Name of the model to use. Charged based on actual token usage.',
  }),
  max_tokens: z.optional(
    z.int().gte(1).register(z.globalRegistry, {
      description:
        "This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.",
    }),
  ),
  temperature: z
    .optional(
      z.number().gte(0).lte(2).register(z.globalRegistry, {
        description:
          "This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input.",
      }),
    )
    .default(1),
})

/**
 * UsageInfo
 */
export const zUsageInfo = z.object({
  prompt_tokens: z.optional(z.int()),
  total_tokens: z.optional(z.int()).default(0),
  completion_tokens: z.optional(z.int()),
  cost: z.number(),
})

/**
 * VideoOutput
 */
export const zRouterVideoEnterpriseOutput = z.object({
  usage: z.optional(zUsageInfo),
  output: z.string().register(z.globalRegistry, {
    description: 'Generated output from video processing',
  }),
})

/**
 * VideoInput
 */
export const zRouterVideoInput = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'Prompt to be used for the video processing',
  }),
  video_urls: z.optional(
    z.array(z.string()).register(z.globalRegistry, {
      description:
        'List of URLs or data URIs of video files to process. Supported formats: mp4, mpeg, mov, webm. For Google Gemini on AI Studio, YouTube links are also supported. Mutually exclusive with video_url.',
    }),
  ),
  system_prompt: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'System prompt to provide context or instructions to the model',
    }),
  ),
  reasoning: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Should reasoning be the part of the final answer.',
      }),
    )
    .default(false),
  model: z.string().register(z.globalRegistry, {
    description:
      'Name of the model to use. Charged based on actual token usage.',
  }),
  max_tokens: z.optional(
    z.int().gte(1).register(z.globalRegistry, {
      description:
        "This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.",
    }),
  ),
  temperature: z
    .optional(
      z.number().gte(0).lte(2).register(z.globalRegistry, {
        description:
          "This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input.",
      }),
    )
    .default(1),
})

/**
 * UsageInfo
 */
export const zOpenrouterRouterVideoUsageInfo = z.object({
  prompt_tokens: z.optional(z.int()),
  total_tokens: z.optional(z.int()).default(0),
  completion_tokens: z.optional(z.int()),
  cost: z.number(),
})

/**
 * VideoOutput
 */
export const zRouterVideoOutput = z.object({
  usage: z.optional(zOpenrouterRouterVideoUsageInfo),
  output: z.string().register(z.globalRegistry, {
    description: 'Generated output from video processing',
  }),
})
