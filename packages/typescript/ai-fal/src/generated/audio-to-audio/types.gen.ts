// This file is auto-generated by @hey-api/openapi-ts

export type ClientOptions = {
  baseUrl: 'https://queue.fal.run' | (string & {})
}

/**
 * TTSOutput
 */
export type SchemaElevenlabsAudioIsolationOutput = {
  audio: SchemaFile
  /**
   * Timestamps
   *
   * Timestamps for each word in the generated speech. Only returned if `timestamps` is set to True in the request.
   */
  timestamps?: Array<unknown> | unknown
}

/**
 * File
 */
export type SchemaFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
}

/**
 * AudioIsolationRequest
 */
export type SchemaElevenlabsAudioIsolationInput = {
  /**
   * Video Url
   *
   * Video file to use for audio isolation. Either `audio_url` or `video_url` must be provided.
   */
  video_url?: string | unknown
  /**
   * Audio Url
   *
   * URL of the audio file to isolate voice from
   */
  audio_url?: string | unknown
}

/**
 * DiaCloneOutput
 */
export type SchemaDiaTtsVoiceCloneOutput = {
  /**
   * The generated speech audio
   */
  audio: SchemaFile
}

/**
 * CloneRequest
 */
export type SchemaDiaTtsVoiceCloneInput = {
  /**
   * Text
   *
   * The text to be converted to speech.
   */
  text: string
  /**
   * Reference Text for the Reference Audio
   *
   * The reference text to be used for TTS.
   */
  ref_text: string
  /**
   * Reference Audio URL
   *
   * The URL of the reference audio file.
   */
  ref_audio_url: string
}

/**
 * ACEStepAudioToAudioResponse
 */
export type SchemaAceStepAudioToAudioOutput = {
  /**
   * Tags
   *
   * The genre tags used in the generation process.
   */
  tags: string
  /**
   * Lyrics
   *
   * The lyrics used in the generation process.
   */
  lyrics: string
  /**
   * Seed
   *
   * The random seed used for the generation process.
   */
  seed: number
  /**
   * Audio
   *
   * The generated audio file.
   */
  audio: SchemaFile
}

/**
 * ACEStepAudioToAudioRequest
 */
export type SchemaAceStepAudioToAudioInput = {
  /**
   * Number Of Steps
   *
   * Number of steps to generate the audio.
   */
  number_of_steps?: number
  /**
   * Tags
   *
   * Comma-separated list of genre tags to control the style of the generated audio.
   */
  tags: string
  /**
   * Minimum Guidance Scale
   *
   * Minimum guidance scale for the generation after the decay.
   */
  minimum_guidance_scale?: number
  /**
   * Lyrics
   *
   * Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song.
   */
  lyrics?: string
  /**
   * Tag Guidance Scale
   *
   * Tag guidance scale for the generation.
   */
  tag_guidance_scale?: number
  /**
   * Original Lyrics
   *
   * Original lyrics of the audio file.
   */
  original_lyrics?: string
  /**
   * Scheduler
   *
   * Scheduler to use for the generation process.
   */
  scheduler?: 'euler' | 'heun'
  /**
   * Guidance Scale
   *
   * Guidance scale for the generation.
   */
  guidance_scale?: number
  /**
   * Guidance Type
   *
   * Type of CFG to use for the generation process.
   */
  guidance_type?: 'cfg' | 'apg' | 'cfg_star'
  /**
   * Lyric Guidance Scale
   *
   * Lyric guidance scale for the generation.
   */
  lyric_guidance_scale?: number
  /**
   * Guidance Interval
   *
   * Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps)
   */
  guidance_interval?: number
  /**
   * Edit Mode
   *
   * Whether to edit the lyrics only or remix the audio.
   */
  edit_mode?: 'lyrics' | 'remix'
  /**
   * Guidance Interval Decay
   *
   * Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay.
   */
  guidance_interval_decay?: number
  /**
   * Audio Url
   *
   * URL of the audio file to be outpainted.
   */
  audio_url: string
  /**
   * Seed
   *
   * Random seed for reproducibility. If not provided, a random seed will be used.
   */
  seed?: number
  /**
   * Granularity Scale
   *
   * Granularity scale for the generation process. Higher values can reduce artifacts.
   */
  granularity_scale?: number
  /**
   * Original Tags
   *
   * Original tags of the audio file.
   */
  original_tags: string
  /**
   * Original Seed
   *
   * Original seed of the audio file.
   */
  original_seed?: number
}

/**
 * ACEStepAudioInpaintResponse
 */
export type SchemaAceStepAudioInpaintOutput = {
  /**
   * Tags
   *
   * The genre tags used in the generation process.
   */
  tags: string
  /**
   * Lyrics
   *
   * The lyrics used in the generation process.
   */
  lyrics: string
  /**
   * Seed
   *
   * The random seed used for the generation process.
   */
  seed: number
  /**
   * Audio
   *
   * The generated audio file.
   */
  audio: SchemaFile
}

/**
 * ACEStepAudioInpaintRequest
 */
export type SchemaAceStepAudioInpaintInput = {
  /**
   * Number Of Steps
   *
   * Number of steps to generate the audio.
   */
  number_of_steps?: number
  /**
   * Start Time
   *
   * start time in seconds for the inpainting process.
   */
  start_time?: number
  /**
   * Tags
   *
   * Comma-separated list of genre tags to control the style of the generated audio.
   */
  tags: string
  /**
   * Minimum Guidance Scale
   *
   * Minimum guidance scale for the generation after the decay.
   */
  minimum_guidance_scale?: number
  /**
   * Lyrics
   *
   * Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song.
   */
  lyrics?: string
  /**
   * End Time Relative To
   *
   * Whether the end time is relative to the start or end of the audio.
   */
  end_time_relative_to?: 'start' | 'end'
  /**
   * Tag Guidance Scale
   *
   * Tag guidance scale for the generation.
   */
  tag_guidance_scale?: number
  /**
   * Scheduler
   *
   * Scheduler to use for the generation process.
   */
  scheduler?: 'euler' | 'heun'
  /**
   * End Time
   *
   * end time in seconds for the inpainting process.
   */
  end_time?: number
  /**
   * Guidance Type
   *
   * Type of CFG to use for the generation process.
   */
  guidance_type?: 'cfg' | 'apg' | 'cfg_star'
  /**
   * Guidance Scale
   *
   * Guidance scale for the generation.
   */
  guidance_scale?: number
  /**
   * Lyric Guidance Scale
   *
   * Lyric guidance scale for the generation.
   */
  lyric_guidance_scale?: number
  /**
   * Guidance Interval
   *
   * Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps)
   */
  guidance_interval?: number
  /**
   * Variance
   *
   * Variance for the inpainting process. Higher values can lead to more diverse results.
   */
  variance?: number
  /**
   * Guidance Interval Decay
   *
   * Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay.
   */
  guidance_interval_decay?: number
  /**
   * Start Time Relative To
   *
   * Whether the start time is relative to the start or end of the audio.
   */
  start_time_relative_to?: 'start' | 'end'
  /**
   * Audio Url
   *
   * URL of the audio file to be inpainted.
   */
  audio_url: string
  /**
   * Seed
   *
   * Random seed for reproducibility. If not provided, a random seed will be used.
   */
  seed?: number
  /**
   * Granularity Scale
   *
   * Granularity scale for the generation process. Higher values can reduce artifacts.
   */
  granularity_scale?: number
}

/**
 * ACEStepResponse
 */
export type SchemaAceStepAudioOutpaintOutput = {
  /**
   * Tags
   *
   * The genre tags used in the generation process.
   */
  tags: string
  /**
   * Lyrics
   *
   * The lyrics used in the generation process.
   */
  lyrics: string
  /**
   * Seed
   *
   * The random seed used for the generation process.
   */
  seed: number
  /**
   * Audio
   *
   * The generated audio file.
   */
  audio: SchemaFile
}

/**
 * ACEStepAudioOutpaintRequest
 */
export type SchemaAceStepAudioOutpaintInput = {
  /**
   * Number Of Steps
   *
   * Number of steps to generate the audio.
   */
  number_of_steps?: number
  /**
   * Tags
   *
   * Comma-separated list of genre tags to control the style of the generated audio.
   */
  tags: string
  /**
   * Minimum Guidance Scale
   *
   * Minimum guidance scale for the generation after the decay.
   */
  minimum_guidance_scale?: number
  /**
   * Extend After Duration
   *
   * Duration in seconds to extend the audio from the end.
   */
  extend_after_duration?: number
  /**
   * Lyrics
   *
   * Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song.
   */
  lyrics?: string
  /**
   * Tag Guidance Scale
   *
   * Tag guidance scale for the generation.
   */
  tag_guidance_scale?: number
  /**
   * Scheduler
   *
   * Scheduler to use for the generation process.
   */
  scheduler?: 'euler' | 'heun'
  /**
   * Extend Before Duration
   *
   * Duration in seconds to extend the audio from the start.
   */
  extend_before_duration?: number
  /**
   * Guidance Type
   *
   * Type of CFG to use for the generation process.
   */
  guidance_type?: 'cfg' | 'apg' | 'cfg_star'
  /**
   * Guidance Scale
   *
   * Guidance scale for the generation.
   */
  guidance_scale?: number
  /**
   * Lyric Guidance Scale
   *
   * Lyric guidance scale for the generation.
   */
  lyric_guidance_scale?: number
  /**
   * Guidance Interval
   *
   * Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps)
   */
  guidance_interval?: number
  /**
   * Guidance Interval Decay
   *
   * Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay.
   */
  guidance_interval_decay?: number
  /**
   * Audio Url
   *
   * URL of the audio file to be outpainted.
   */
  audio_url: string
  /**
   * Seed
   *
   * Random seed for reproducibility. If not provided, a random seed will be used.
   */
  seed?: number
  /**
   * Granularity Scale
   *
   * Granularity scale for the generation process. Higher values can reduce artifacts.
   */
  granularity_scale?: number
}

/**
 * ExtendOutput
 */
export type SchemaV2ExtendOutput = {
  /**
   * Tags
   *
   * The style tags used for generation.
   */
  tags?: Array<string> | unknown
  /**
   * Seed
   *
   * The seed used for generation. This can be used to generate an identical song by passing the same parameters with this seed in a future request.
   */
  seed: number
  /**
   * Extend Duration
   *
   * The duration in seconds that the song was extended by.
   */
  extend_duration: number
  /**
   * Audio
   *
   * The generated audio files.
   */
  audio: Array<SchemaFile>
  /**
   * Lyrics
   *
   * The lyrics used for generation.
   */
  lyrics?: string | unknown
}

/**
 * ExtendInput
 */
export type SchemaV2ExtendInput = {
  /**
   * Prompt
   *
   * A description of the track you want to generate. This prompt will be used to automatically generate the tags and lyrics unless you manually set them. For example, if you set prompt and tags, then the prompt will be used to generate only the lyrics.
   */
  prompt?: string | unknown
  /**
   * Lyrics Prompt
   *
   * The lyrics sung in the generated song. An empty string will generate an instrumental track.
   */
  lyrics_prompt?: string | unknown
  /**
   * Tags
   *
   * Tags/styles of the music to generate. You can view a list of all available tags at https://sonauto.ai/tag-explorer.
   */
  tags?: Array<string> | unknown
  /**
   * Prompt Strength
   *
   * Controls how strongly your prompt influences the output. Greater values adhere more to the prompt but sound less natural. (This is CFG.)
   */
  prompt_strength?: number
  /**
   * Output Bit Rate
   *
   * The bit rate to use for mp3 and m4a formats. Not available for other formats.
   */
  output_bit_rate?: 128 | 192 | 256 | 320 | unknown
  /**
   * Num Songs
   *
   * Generating 2 songs costs 1.5x the price of generating 1 song. Also, note that using the same seed may not result in identical songs if the number of songs generated is changed.
   */
  num_songs?: number
  /**
   * Output Format
   */
  output_format?: 'flac' | 'mp3' | 'wav' | 'ogg' | 'm4a'
  /**
   * Side
   *
   * Add more to the beginning (left) or end (right) of the song
   */
  side: 'left' | 'right'
  /**
   * Balance Strength
   *
   * Greater means more natural vocals. Lower means sharper instrumentals. We recommend 0.7.
   */
  balance_strength?: number
  /**
   * Crop Duration
   *
   * Duration in seconds to crop from the selected side before extending from that side.
   */
  crop_duration?: number
  /**
   * Audio Url
   *
   * The URL of the audio file to alter. Must be a valid publicly accessible URL.
   */
  audio_url: string
  /**
   * Seed
   *
   * The seed to use for generation. Will pick a random seed if not provided. Repeating a request with identical parameters (must use lyrics and tags, not prompt) and the same seed will generate the same song.
   */
  seed?: number | unknown
  /**
   * Extend Duration
   *
   * Duration in seconds to extend the song. If not provided, will attempt to automatically determine.
   */
  extend_duration?: number | unknown
}

/**
 * InpaintOutput
 */
export type SchemaStableAudio25InpaintOutput = {
  /**
   * Seed
   *
   * The random seed used for generation
   */
  seed: number
  /**
   * Audio
   *
   * The generated audio clip
   */
  audio: SchemaFile
}

/**
 * InpaintInput
 */
export type SchemaStableAudio25InpaintInput = {
  /**
   * Prompt
   *
   * The prompt to guide the audio generation
   */
  prompt: string
  /**
   * Guidance Scale
   *
   * How strictly the diffusion process adheres to the prompt text (higher values make your audio closer to your prompt).
   */
  guidance_scale?: number
  /**
   * Mask End
   *
   * The end point of the audio mask
   */
  mask_end?: number
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Audio Url
   *
   * The audio clip to inpaint
   */
  audio_url: string
  /**
   * Seed
   */
  seed?: number
  /**
   * Seconds Total
   *
   * The duration of the audio clip to generate. If not provided, it will be set to the duration of the input audio.
   */
  seconds_total?: number
  /**
   * Num Inference Steps
   *
   * The number of steps to denoise the audio for
   */
  num_inference_steps?: number
  /**
   * Mask Start
   *
   * The start point of the audio mask
   */
  mask_start?: number
}

/**
 * AudioToAudioOutput
 */
export type SchemaStableAudio25AudioToAudioOutput = {
  /**
   * Seed
   *
   * The random seed used for generation
   */
  seed: number
  /**
   * Audio
   *
   * The generated audio clip
   */
  audio: SchemaFile
}

/**
 * AudioToAudioInput
 */
export type SchemaStableAudio25AudioToAudioInput = {
  /**
   * Prompt
   *
   * The prompt to guide the audio generation
   */
  prompt: string
  /**
   * Strength
   *
   * Sometimes referred to as denoising, this parameter controls how much influence the `audio_url` parameter has on the generated audio. A value of 0 would yield audio that is identical to the input. A value of 1 would be as if you passed in no audio at all.
   */
  strength?: number
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Audio Url
   *
   * The audio clip to transform
   */
  audio_url: string
  /**
   * Num Inference Steps
   *
   * The number of steps to denoise the audio for
   */
  num_inference_steps?: number
  /**
   * Guidance Scale
   *
   * How strictly the diffusion process adheres to the prompt text (higher values make your audio closer to your prompt).
   */
  guidance_scale?: number
  /**
   * Seed
   */
  seed?: number
  /**
   * Total Seconds
   *
   * The duration of the audio clip to generate. If not provided, it will be set to the duration of the input audio.
   */
  total_seconds?: number
}

/**
 * AudioUnderstandingOutput
 */
export type SchemaAudioUnderstandingOutput = {
  /**
   * Output
   *
   * The analysis of the audio content based on the prompt
   */
  output: string
}

/**
 * AudioUnderstandingInput
 */
export type SchemaAudioUnderstandingInput = {
  /**
   * Prompt
   *
   * The question or prompt about the audio content.
   */
  prompt: string
  /**
   * Detailed Analysis
   *
   * Whether to request a more detailed analysis of the audio
   */
  detailed_analysis?: boolean
  /**
   * Audio Url
   *
   * URL of the audio file to analyze
   */
  audio_url: string
}

/**
 * DemucsOutput
 */
export type SchemaDemucsOutput = {
  /**
   * Separated vocals audio file
   */
  vocals?: SchemaFile | unknown
  /**
   * Separated guitar audio file (only available for 6s models)
   */
  guitar?: SchemaFile | unknown
  /**
   * Separated bass audio file
   */
  bass?: SchemaFile | unknown
  /**
   * Separated piano audio file (only available for 6s models)
   */
  piano?: SchemaFile | unknown
  /**
   * Separated other instruments audio file
   */
  other?: SchemaFile | unknown
  /**
   * Separated drums audio file
   */
  drums?: SchemaFile | unknown
}

/**
 * DemucsInput
 */
export type SchemaDemucsInput = {
  /**
   * Segment Length
   *
   * Length in seconds of each segment for processing. Smaller values use less memory but may reduce quality. Default is model-specific.
   */
  segment_length?: number | unknown
  /**
   * Output Format
   *
   * Output audio format for the separated stems
   */
  output_format?: 'wav' | 'mp3'
  /**
   * Stems
   *
   * Specific stems to extract. If None, extracts all available stems. Available stems depend on model: vocals, drums, bass, other, guitar, piano (for 6s model)
   */
  stems?:
    | Array<'vocals' | 'drums' | 'bass' | 'other' | 'guitar' | 'piano'>
    | unknown
  /**
   * Overlap
   *
   * Overlap between segments (0.0 to 1.0). Higher values may improve quality but increase processing time.
   */
  overlap?: number
  /**
   * Model
   *
   * Demucs model to use for separation
   */
  model?:
    | 'htdemucs'
    | 'htdemucs_ft'
    | 'htdemucs_6s'
    | 'hdemucs_mmi'
    | 'mdx'
    | 'mdx_extra'
    | 'mdx_q'
    | 'mdx_extra_q'
  /**
   * Audio Url
   *
   * URL of the audio file to separate into stems
   */
  audio_url: string
  /**
   * Shifts
   *
   * Number of random shifts for equivariant stabilization. Higher values improve quality but increase processing time.
   */
  shifts?: number
}

/**
 * CreateVoiceOutput
 *
 * Response model for creating a custom voice.
 */
export type SchemaKlingVideoCreateVoiceOutput = {
  /**
   * Voice Id
   *
   * Unique identifier for the created voice
   */
  voice_id: string
}

/**
 * CreateVoiceInput
 *
 * Request model for creating a custom voice.
 */
export type SchemaKlingVideoCreateVoiceInput = {
  /**
   * Voice Url
   *
   * URL of the voice audio file. Supports .mp3/.wav audio or .mp4/.mov video. Duration must be 5-30 seconds with clean, single-voice audio.
   */
  voice_url: string
}

/**
 * MergeAudiosOutput
 */
export type SchemaFfmpegApiMergeAudiosOutput = {
  audio: SchemaFile
}

/**
 * MergeAudiosInput
 */
export type SchemaFfmpegApiMergeAudiosInput = {
  /**
   * Audio Urls
   *
   * List of audio URLs to merge in order. The 0th stream of the audio will be considered as the merge candidate.
   */
  audio_urls: Array<string>
  /**
   * Output Format
   *
   * Output format of the combined audio. If not used, will be determined automatically using FFMPEG. Formatted as codec_sample_rate_bitrate.
   */
  output_format?:
    | 'mp3_22050_32'
    | 'mp3_44100_32'
    | 'mp3_44100_64'
    | 'mp3_44100_96'
    | 'mp3_44100_128'
    | 'mp3_44100_192'
    | 'pcm_8000'
    | 'pcm_16000'
    | 'pcm_22050'
    | 'pcm_24000'
    | 'pcm_44100'
    | 'pcm_48000'
    | 'ulaw_8000'
    | 'alaw_8000'
    | 'opus_48000_32'
    | 'opus_48000_64'
    | 'opus_48000_96'
    | 'opus_48000_128'
    | 'opus_48000_192'
    | unknown
}

/**
 * AudioTimeSpan
 *
 * A time span indicating where the target sound occurs.
 */
export type SchemaAudioTimeSpan = {
  /**
   * End
   *
   * End time of the span in seconds
   */
  end: number
  /**
   * Start
   *
   * Start time of the span in seconds
   */
  start: number
  /**
   * Include
   *
   * Whether to include (True) or exclude (False) sounds in this span
   */
  include?: boolean
}

/**
 * SAMAudioSpanSeparateOutput
 *
 * Output for span-based audio separation.
 */
export type SchemaSamAudioSpanSeparateOutput = {
  /**
   * Target
   *
   * The isolated target sound.
   */
  target: SchemaFile
  /**
   * Duration
   *
   * Duration of the output audio in seconds.
   */
  duration: number
  /**
   * Sample Rate
   *
   * Sample rate of the output audio in Hz.
   */
  sample_rate?: number
  /**
   * Residual
   *
   * Everything else in the audio.
   */
  residual: SchemaFile
}

/**
 * SAMAudioSpanInput
 *
 * Input for temporal span-based audio separation.
 */
export type SchemaSamAudioSpanSeparateInput = {
  /**
   * Prompt
   *
   * Text prompt describing the sound to isolate. Optional but recommended - helps the model identify what type of sound to extract from the span.
   */
  prompt?: string
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: 'fast' | 'balanced' | 'quality'
  /**
   * Spans
   *
   * Time spans where the target sound occurs which should be isolated.
   */
  spans: Array<SchemaAudioTimeSpan>
  /**
   * Output Format
   *
   * Output audio format.
   */
  output_format?: 'wav' | 'mp3'
  /**
   * Trim To Span
   *
   * Trim output audio to only include the specified span time range. If False, returns the full audio length with the target sound isolated throughout.
   */
  trim_to_span?: boolean
  /**
   * Audio Url
   *
   * URL of the audio file to process.
   */
  audio_url: string
  /**
   * Reranking Candidates
   *
   * Number of candidates to generate and rank. Higher improves quality but increases latency and cost. Requires text prompt; ignored for span-only separation.
   */
  reranking_candidates?: number
}

/**
 * SAMAudioSeparateOutput
 *
 * Output for text-based audio separation.
 */
export type SchemaSamAudioSeparateOutput = {
  /**
   * Target
   *
   * The isolated target sound.
   */
  target: SchemaFile
  /**
   * Duration
   *
   * Duration of the output audio in seconds.
   */
  duration: number
  /**
   * Sample Rate
   *
   * Sample rate of the output audio in Hz.
   */
  sample_rate?: number
  /**
   * Residual
   *
   * Everything else in the audio.
   */
  residual: SchemaFile
}

/**
 * SAMAudioInput
 *
 * Input for text-based audio separation.
 */
export type SchemaSamAudioSeparateInput = {
  /**
   * Prompt
   *
   * Text prompt describing the sound to isolate.
   */
  prompt: string
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: 'fast' | 'balanced' | 'quality'
  /**
   * Audio Url
   *
   * URL of the audio file to process (WAV, MP3, FLAC supported)
   */
  audio_url: string
  /**
   * Predict Spans
   *
   * Automatically predict temporal spans where the target sound occurs.
   */
  predict_spans?: boolean
  /**
   * Output Format
   *
   * Output audio format.
   */
  output_format?: 'wav' | 'mp3'
  /**
   * Reranking Candidates
   *
   * Number of candidates to generate and rank. Higher improves quality but increases latency and cost.
   */
  reranking_candidates?: number
}

/**
 * DeepFilterNetTimings
 */
export type SchemaDeepFilterNetTimings = {
  /**
   * Postprocess
   *
   * Postprocessing time.
   */
  postprocess: number
  /**
   * Inference
   *
   * Inference time.
   */
  inference: number
  /**
   * Preprocess
   *
   * Preprocessing time.
   */
  preprocess: number
}

/**
 * DeepFilterNet3Output
 */
export type SchemaDeepfilternet3Output = {
  /**
   * Timings
   *
   * Timings for each step in the pipeline.
   */
  timings: SchemaDeepFilterNetTimings
  /**
   * Audio File
   *
   * The audio file that was enhanced.
   */
  audio_file: SchemaAudioFile
}

/**
 * AudioFile
 */
export type SchemaAudioFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number
  /**
   * Duration
   *
   * The duration of the audio
   */
  duration?: number
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File
  /**
   * Bitrate
   *
   * The bitrate of the audio
   */
  bitrate?: string
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string
  /**
   * Sample Rate
   *
   * The sample rate of the audio
   */
  sample_rate?: number
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string
  /**
   * Channels
   *
   * The number of channels in the audio
   */
  channels?: number
}

/**
 * DeepFilterNet3Input
 */
export type SchemaDeepfilternet3Input = {
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Audio Format
   *
   * The format for the output audio.
   */
  audio_format?: 'mp3' | 'aac' | 'm4a' | 'ogg' | 'opus' | 'flac' | 'wav'
  /**
   * Audio URL
   *
   * The URL of the audio to enhance.
   */
  audio_url: string
  /**
   * Bitrate
   *
   * The bitrate of the output audio.
   */
  bitrate?: string
}

/**
 * NovaSRTimings
 */
export type SchemaNovaSrTimings = {
  /**
   * Postprocess
   *
   * Time taken to postprocess the audio in seconds.
   */
  postprocess: number
  /**
   * Inference
   *
   * Time taken to run the inference in seconds.
   */
  inference: number
  /**
   * Preprocess
   *
   * Time taken to preprocess the audio in seconds.
   */
  preprocess: number
}

/**
 * NovaSROutput
 */
export type SchemaNovaSrOutput = {
  /**
   * Timings
   *
   * Timings for each step in the pipeline.
   */
  timings: SchemaNovaSrTimings
  /**
   * Audio
   *
   * The enhanced audio file.
   */
  audio: SchemaAudioFile
}

/**
 * NovaSRInput
 */
export type SchemaNovaSrInput = {
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean
  /**
   * Bitrate
   *
   * The bitrate of the output audio.
   */
  bitrate?: string
  /**
   * Audio URL
   *
   * The URL of the audio file to enhance.
   */
  audio_url: string
  /**
   * Audio Format
   *
   * The format for the output audio.
   */
  audio_format?: 'mp3' | 'aac' | 'm4a' | 'ogg' | 'opus' | 'flac' | 'wav'
}

/**
 * VoiceChangerOutput
 */
export type SchemaElevenlabsVoiceChangerOutput = {
  /**
   * Seed
   *
   * Random seed for reproducibility.
   */
  seed: number
  audio: SchemaFile
}

/**
 * VoiceChangerRequest
 */
export type SchemaElevenlabsVoiceChangerInput = {
  /**
   * Voice
   *
   * The voice to use for speech generation
   */
  voice?: string
  /**
   * Audio Url
   *
   * The input audio file
   */
  audio_url: string
  /**
   * Seed
   *
   * Random seed for reproducibility.
   */
  seed?: number
  /**
   * Output Format
   *
   * Output format of the generated audio. Formatted as codec_sample_rate_bitrate.
   */
  output_format?:
    | 'mp3_22050_32'
    | 'mp3_44100_32'
    | 'mp3_44100_64'
    | 'mp3_44100_96'
    | 'mp3_44100_128'
    | 'mp3_44100_192'
    | 'pcm_8000'
    | 'pcm_16000'
    | 'pcm_22050'
    | 'pcm_24000'
    | 'pcm_44100'
    | 'pcm_48000'
    | 'ulaw_8000'
    | 'alaw_8000'
    | 'opus_48000_32'
    | 'opus_48000_64'
    | 'opus_48000_96'
    | 'opus_48000_128'
    | 'opus_48000_192'
  /**
   * Remove Background Noise
   *
   * If set, will remove the background noise from your audio input using our audio isolation model.
   */
  remove_background_noise?: boolean
}

export type SchemaQueueStatus = {
  status: 'IN_QUEUE' | 'IN_PROGRESS' | 'COMPLETED'
  /**
   * The request id.
   */
  request_id: string
  /**
   * The response url.
   */
  response_url?: string
  /**
   * The status url.
   */
  status_url?: string
  /**
   * The cancel url.
   */
  cancel_url?: string
  /**
   * The logs.
   */
  logs?: {
    [key: string]: unknown
  }
  /**
   * The metrics.
   */
  metrics?: {
    [key: string]: unknown
  }
  /**
   * The queue position.
   */
  queue_position?: number
}

export type GetFalAiElevenlabsVoiceChangerRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/elevenlabs/voice-changer/requests/{request_id}/status'
}

export type GetFalAiElevenlabsVoiceChangerRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiElevenlabsVoiceChangerRequestsByRequestIdStatusResponse =
  GetFalAiElevenlabsVoiceChangerRequestsByRequestIdStatusResponses[keyof GetFalAiElevenlabsVoiceChangerRequestsByRequestIdStatusResponses]

export type PutFalAiElevenlabsVoiceChangerRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/elevenlabs/voice-changer/requests/{request_id}/cancel'
}

export type PutFalAiElevenlabsVoiceChangerRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiElevenlabsVoiceChangerRequestsByRequestIdCancelResponse =
  PutFalAiElevenlabsVoiceChangerRequestsByRequestIdCancelResponses[keyof PutFalAiElevenlabsVoiceChangerRequestsByRequestIdCancelResponses]

export type PostFalAiElevenlabsVoiceChangerData = {
  body: SchemaElevenlabsVoiceChangerInput
  path?: never
  query?: never
  url: '/fal-ai/elevenlabs/voice-changer'
}

export type PostFalAiElevenlabsVoiceChangerResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiElevenlabsVoiceChangerResponse =
  PostFalAiElevenlabsVoiceChangerResponses[keyof PostFalAiElevenlabsVoiceChangerResponses]

export type GetFalAiElevenlabsVoiceChangerRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/elevenlabs/voice-changer/requests/{request_id}'
}

export type GetFalAiElevenlabsVoiceChangerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaElevenlabsVoiceChangerOutput
}

export type GetFalAiElevenlabsVoiceChangerRequestsByRequestIdResponse =
  GetFalAiElevenlabsVoiceChangerRequestsByRequestIdResponses[keyof GetFalAiElevenlabsVoiceChangerRequestsByRequestIdResponses]

export type GetFalAiNovaSrRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/nova-sr/requests/{request_id}/status'
}

export type GetFalAiNovaSrRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiNovaSrRequestsByRequestIdStatusResponse =
  GetFalAiNovaSrRequestsByRequestIdStatusResponses[keyof GetFalAiNovaSrRequestsByRequestIdStatusResponses]

export type PutFalAiNovaSrRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/nova-sr/requests/{request_id}/cancel'
}

export type PutFalAiNovaSrRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiNovaSrRequestsByRequestIdCancelResponse =
  PutFalAiNovaSrRequestsByRequestIdCancelResponses[keyof PutFalAiNovaSrRequestsByRequestIdCancelResponses]

export type PostFalAiNovaSrData = {
  body: SchemaNovaSrInput
  path?: never
  query?: never
  url: '/fal-ai/nova-sr'
}

export type PostFalAiNovaSrResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiNovaSrResponse =
  PostFalAiNovaSrResponses[keyof PostFalAiNovaSrResponses]

export type GetFalAiNovaSrRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/nova-sr/requests/{request_id}'
}

export type GetFalAiNovaSrRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaNovaSrOutput
}

export type GetFalAiNovaSrRequestsByRequestIdResponse =
  GetFalAiNovaSrRequestsByRequestIdResponses[keyof GetFalAiNovaSrRequestsByRequestIdResponses]

export type GetFalAiDeepfilternet3RequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/deepfilternet3/requests/{request_id}/status'
}

export type GetFalAiDeepfilternet3RequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiDeepfilternet3RequestsByRequestIdStatusResponse =
  GetFalAiDeepfilternet3RequestsByRequestIdStatusResponses[keyof GetFalAiDeepfilternet3RequestsByRequestIdStatusResponses]

export type PutFalAiDeepfilternet3RequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/deepfilternet3/requests/{request_id}/cancel'
}

export type PutFalAiDeepfilternet3RequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiDeepfilternet3RequestsByRequestIdCancelResponse =
  PutFalAiDeepfilternet3RequestsByRequestIdCancelResponses[keyof PutFalAiDeepfilternet3RequestsByRequestIdCancelResponses]

export type PostFalAiDeepfilternet3Data = {
  body: SchemaDeepfilternet3Input
  path?: never
  query?: never
  url: '/fal-ai/deepfilternet3'
}

export type PostFalAiDeepfilternet3Responses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiDeepfilternet3Response =
  PostFalAiDeepfilternet3Responses[keyof PostFalAiDeepfilternet3Responses]

export type GetFalAiDeepfilternet3RequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/deepfilternet3/requests/{request_id}'
}

export type GetFalAiDeepfilternet3RequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaDeepfilternet3Output
}

export type GetFalAiDeepfilternet3RequestsByRequestIdResponse =
  GetFalAiDeepfilternet3RequestsByRequestIdResponses[keyof GetFalAiDeepfilternet3RequestsByRequestIdResponses]

export type GetFalAiSamAudioSeparateRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/sam-audio/separate/requests/{request_id}/status'
}

export type GetFalAiSamAudioSeparateRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiSamAudioSeparateRequestsByRequestIdStatusResponse =
  GetFalAiSamAudioSeparateRequestsByRequestIdStatusResponses[keyof GetFalAiSamAudioSeparateRequestsByRequestIdStatusResponses]

export type PutFalAiSamAudioSeparateRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/sam-audio/separate/requests/{request_id}/cancel'
}

export type PutFalAiSamAudioSeparateRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiSamAudioSeparateRequestsByRequestIdCancelResponse =
  PutFalAiSamAudioSeparateRequestsByRequestIdCancelResponses[keyof PutFalAiSamAudioSeparateRequestsByRequestIdCancelResponses]

export type PostFalAiSamAudioSeparateData = {
  body: SchemaSamAudioSeparateInput
  path?: never
  query?: never
  url: '/fal-ai/sam-audio/separate'
}

export type PostFalAiSamAudioSeparateResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiSamAudioSeparateResponse =
  PostFalAiSamAudioSeparateResponses[keyof PostFalAiSamAudioSeparateResponses]

export type GetFalAiSamAudioSeparateRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/sam-audio/separate/requests/{request_id}'
}

export type GetFalAiSamAudioSeparateRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSamAudioSeparateOutput
}

export type GetFalAiSamAudioSeparateRequestsByRequestIdResponse =
  GetFalAiSamAudioSeparateRequestsByRequestIdResponses[keyof GetFalAiSamAudioSeparateRequestsByRequestIdResponses]

export type GetFalAiSamAudioSpanSeparateRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/sam-audio/span-separate/requests/{request_id}/status'
}

export type GetFalAiSamAudioSpanSeparateRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiSamAudioSpanSeparateRequestsByRequestIdStatusResponse =
  GetFalAiSamAudioSpanSeparateRequestsByRequestIdStatusResponses[keyof GetFalAiSamAudioSpanSeparateRequestsByRequestIdStatusResponses]

export type PutFalAiSamAudioSpanSeparateRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/sam-audio/span-separate/requests/{request_id}/cancel'
}

export type PutFalAiSamAudioSpanSeparateRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiSamAudioSpanSeparateRequestsByRequestIdCancelResponse =
  PutFalAiSamAudioSpanSeparateRequestsByRequestIdCancelResponses[keyof PutFalAiSamAudioSpanSeparateRequestsByRequestIdCancelResponses]

export type PostFalAiSamAudioSpanSeparateData = {
  body: SchemaSamAudioSpanSeparateInput
  path?: never
  query?: never
  url: '/fal-ai/sam-audio/span-separate'
}

export type PostFalAiSamAudioSpanSeparateResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiSamAudioSpanSeparateResponse =
  PostFalAiSamAudioSpanSeparateResponses[keyof PostFalAiSamAudioSpanSeparateResponses]

export type GetFalAiSamAudioSpanSeparateRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/sam-audio/span-separate/requests/{request_id}'
}

export type GetFalAiSamAudioSpanSeparateRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSamAudioSpanSeparateOutput
}

export type GetFalAiSamAudioSpanSeparateRequestsByRequestIdResponse =
  GetFalAiSamAudioSpanSeparateRequestsByRequestIdResponses[keyof GetFalAiSamAudioSpanSeparateRequestsByRequestIdResponses]

export type GetFalAiFfmpegApiMergeAudiosRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/ffmpeg-api/merge-audios/requests/{request_id}/status'
}

export type GetFalAiFfmpegApiMergeAudiosRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiFfmpegApiMergeAudiosRequestsByRequestIdStatusResponse =
  GetFalAiFfmpegApiMergeAudiosRequestsByRequestIdStatusResponses[keyof GetFalAiFfmpegApiMergeAudiosRequestsByRequestIdStatusResponses]

export type PutFalAiFfmpegApiMergeAudiosRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ffmpeg-api/merge-audios/requests/{request_id}/cancel'
}

export type PutFalAiFfmpegApiMergeAudiosRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiFfmpegApiMergeAudiosRequestsByRequestIdCancelResponse =
  PutFalAiFfmpegApiMergeAudiosRequestsByRequestIdCancelResponses[keyof PutFalAiFfmpegApiMergeAudiosRequestsByRequestIdCancelResponses]

export type PostFalAiFfmpegApiMergeAudiosData = {
  body: SchemaFfmpegApiMergeAudiosInput
  path?: never
  query?: never
  url: '/fal-ai/ffmpeg-api/merge-audios'
}

export type PostFalAiFfmpegApiMergeAudiosResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiFfmpegApiMergeAudiosResponse =
  PostFalAiFfmpegApiMergeAudiosResponses[keyof PostFalAiFfmpegApiMergeAudiosResponses]

export type GetFalAiFfmpegApiMergeAudiosRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ffmpeg-api/merge-audios/requests/{request_id}'
}

export type GetFalAiFfmpegApiMergeAudiosRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaFfmpegApiMergeAudiosOutput
}

export type GetFalAiFfmpegApiMergeAudiosRequestsByRequestIdResponse =
  GetFalAiFfmpegApiMergeAudiosRequestsByRequestIdResponses[keyof GetFalAiFfmpegApiMergeAudiosRequestsByRequestIdResponses]

export type GetFalAiKlingVideoCreateVoiceRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/kling-video/create-voice/requests/{request_id}/status'
}

export type GetFalAiKlingVideoCreateVoiceRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiKlingVideoCreateVoiceRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoCreateVoiceRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoCreateVoiceRequestsByRequestIdStatusResponses]

export type PutFalAiKlingVideoCreateVoiceRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/kling-video/create-voice/requests/{request_id}/cancel'
}

export type PutFalAiKlingVideoCreateVoiceRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiKlingVideoCreateVoiceRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoCreateVoiceRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoCreateVoiceRequestsByRequestIdCancelResponses]

export type PostFalAiKlingVideoCreateVoiceData = {
  body: SchemaKlingVideoCreateVoiceInput
  path?: never
  query?: never
  url: '/fal-ai/kling-video/create-voice'
}

export type PostFalAiKlingVideoCreateVoiceResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiKlingVideoCreateVoiceResponse =
  PostFalAiKlingVideoCreateVoiceResponses[keyof PostFalAiKlingVideoCreateVoiceResponses]

export type GetFalAiKlingVideoCreateVoiceRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/kling-video/create-voice/requests/{request_id}'
}

export type GetFalAiKlingVideoCreateVoiceRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaKlingVideoCreateVoiceOutput
}

export type GetFalAiKlingVideoCreateVoiceRequestsByRequestIdResponse =
  GetFalAiKlingVideoCreateVoiceRequestsByRequestIdResponses[keyof GetFalAiKlingVideoCreateVoiceRequestsByRequestIdResponses]

export type GetFalAiDemucsRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/demucs/requests/{request_id}/status'
}

export type GetFalAiDemucsRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiDemucsRequestsByRequestIdStatusResponse =
  GetFalAiDemucsRequestsByRequestIdStatusResponses[keyof GetFalAiDemucsRequestsByRequestIdStatusResponses]

export type PutFalAiDemucsRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/demucs/requests/{request_id}/cancel'
}

export type PutFalAiDemucsRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiDemucsRequestsByRequestIdCancelResponse =
  PutFalAiDemucsRequestsByRequestIdCancelResponses[keyof PutFalAiDemucsRequestsByRequestIdCancelResponses]

export type PostFalAiDemucsData = {
  body: SchemaDemucsInput
  path?: never
  query?: never
  url: '/fal-ai/demucs'
}

export type PostFalAiDemucsResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiDemucsResponse =
  PostFalAiDemucsResponses[keyof PostFalAiDemucsResponses]

export type GetFalAiDemucsRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/demucs/requests/{request_id}'
}

export type GetFalAiDemucsRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaDemucsOutput
}

export type GetFalAiDemucsRequestsByRequestIdResponse =
  GetFalAiDemucsRequestsByRequestIdResponses[keyof GetFalAiDemucsRequestsByRequestIdResponses]

export type GetFalAiAudioUnderstandingRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/audio-understanding/requests/{request_id}/status'
}

export type GetFalAiAudioUnderstandingRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiAudioUnderstandingRequestsByRequestIdStatusResponse =
  GetFalAiAudioUnderstandingRequestsByRequestIdStatusResponses[keyof GetFalAiAudioUnderstandingRequestsByRequestIdStatusResponses]

export type PutFalAiAudioUnderstandingRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/audio-understanding/requests/{request_id}/cancel'
}

export type PutFalAiAudioUnderstandingRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiAudioUnderstandingRequestsByRequestIdCancelResponse =
  PutFalAiAudioUnderstandingRequestsByRequestIdCancelResponses[keyof PutFalAiAudioUnderstandingRequestsByRequestIdCancelResponses]

export type PostFalAiAudioUnderstandingData = {
  body: SchemaAudioUnderstandingInput
  path?: never
  query?: never
  url: '/fal-ai/audio-understanding'
}

export type PostFalAiAudioUnderstandingResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiAudioUnderstandingResponse =
  PostFalAiAudioUnderstandingResponses[keyof PostFalAiAudioUnderstandingResponses]

export type GetFalAiAudioUnderstandingRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/audio-understanding/requests/{request_id}'
}

export type GetFalAiAudioUnderstandingRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaAudioUnderstandingOutput
}

export type GetFalAiAudioUnderstandingRequestsByRequestIdResponse =
  GetFalAiAudioUnderstandingRequestsByRequestIdResponses[keyof GetFalAiAudioUnderstandingRequestsByRequestIdResponses]

export type GetFalAiStableAudio25AudioToAudioRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/stable-audio-25/audio-to-audio/requests/{request_id}/status'
}

export type GetFalAiStableAudio25AudioToAudioRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiStableAudio25AudioToAudioRequestsByRequestIdStatusResponse =
  GetFalAiStableAudio25AudioToAudioRequestsByRequestIdStatusResponses[keyof GetFalAiStableAudio25AudioToAudioRequestsByRequestIdStatusResponses]

export type PutFalAiStableAudio25AudioToAudioRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/stable-audio-25/audio-to-audio/requests/{request_id}/cancel'
}

export type PutFalAiStableAudio25AudioToAudioRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiStableAudio25AudioToAudioRequestsByRequestIdCancelResponse =
  PutFalAiStableAudio25AudioToAudioRequestsByRequestIdCancelResponses[keyof PutFalAiStableAudio25AudioToAudioRequestsByRequestIdCancelResponses]

export type PostFalAiStableAudio25AudioToAudioData = {
  body: SchemaStableAudio25AudioToAudioInput
  path?: never
  query?: never
  url: '/fal-ai/stable-audio-25/audio-to-audio'
}

export type PostFalAiStableAudio25AudioToAudioResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiStableAudio25AudioToAudioResponse =
  PostFalAiStableAudio25AudioToAudioResponses[keyof PostFalAiStableAudio25AudioToAudioResponses]

export type GetFalAiStableAudio25AudioToAudioRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/stable-audio-25/audio-to-audio/requests/{request_id}'
}

export type GetFalAiStableAudio25AudioToAudioRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaStableAudio25AudioToAudioOutput
}

export type GetFalAiStableAudio25AudioToAudioRequestsByRequestIdResponse =
  GetFalAiStableAudio25AudioToAudioRequestsByRequestIdResponses[keyof GetFalAiStableAudio25AudioToAudioRequestsByRequestIdResponses]

export type GetFalAiStableAudio25InpaintRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/stable-audio-25/inpaint/requests/{request_id}/status'
}

export type GetFalAiStableAudio25InpaintRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiStableAudio25InpaintRequestsByRequestIdStatusResponse =
  GetFalAiStableAudio25InpaintRequestsByRequestIdStatusResponses[keyof GetFalAiStableAudio25InpaintRequestsByRequestIdStatusResponses]

export type PutFalAiStableAudio25InpaintRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/stable-audio-25/inpaint/requests/{request_id}/cancel'
}

export type PutFalAiStableAudio25InpaintRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiStableAudio25InpaintRequestsByRequestIdCancelResponse =
  PutFalAiStableAudio25InpaintRequestsByRequestIdCancelResponses[keyof PutFalAiStableAudio25InpaintRequestsByRequestIdCancelResponses]

export type PostFalAiStableAudio25InpaintData = {
  body: SchemaStableAudio25InpaintInput
  path?: never
  query?: never
  url: '/fal-ai/stable-audio-25/inpaint'
}

export type PostFalAiStableAudio25InpaintResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiStableAudio25InpaintResponse =
  PostFalAiStableAudio25InpaintResponses[keyof PostFalAiStableAudio25InpaintResponses]

export type GetFalAiStableAudio25InpaintRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/stable-audio-25/inpaint/requests/{request_id}'
}

export type GetFalAiStableAudio25InpaintRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaStableAudio25InpaintOutput
}

export type GetFalAiStableAudio25InpaintRequestsByRequestIdResponse =
  GetFalAiStableAudio25InpaintRequestsByRequestIdResponses[keyof GetFalAiStableAudio25InpaintRequestsByRequestIdResponses]

export type GetSonautoV2ExtendRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/sonauto/v2/extend/requests/{request_id}/status'
}

export type GetSonautoV2ExtendRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetSonautoV2ExtendRequestsByRequestIdStatusResponse =
  GetSonautoV2ExtendRequestsByRequestIdStatusResponses[keyof GetSonautoV2ExtendRequestsByRequestIdStatusResponses]

export type PutSonautoV2ExtendRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/sonauto/v2/extend/requests/{request_id}/cancel'
}

export type PutSonautoV2ExtendRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutSonautoV2ExtendRequestsByRequestIdCancelResponse =
  PutSonautoV2ExtendRequestsByRequestIdCancelResponses[keyof PutSonautoV2ExtendRequestsByRequestIdCancelResponses]

export type PostSonautoV2ExtendData = {
  body: SchemaV2ExtendInput
  path?: never
  query?: never
  url: '/sonauto/v2/extend'
}

export type PostSonautoV2ExtendResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostSonautoV2ExtendResponse =
  PostSonautoV2ExtendResponses[keyof PostSonautoV2ExtendResponses]

export type GetSonautoV2ExtendRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/sonauto/v2/extend/requests/{request_id}'
}

export type GetSonautoV2ExtendRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaV2ExtendOutput
}

export type GetSonautoV2ExtendRequestsByRequestIdResponse =
  GetSonautoV2ExtendRequestsByRequestIdResponses[keyof GetSonautoV2ExtendRequestsByRequestIdResponses]

export type GetFalAiAceStepAudioOutpaintRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/ace-step/audio-outpaint/requests/{request_id}/status'
}

export type GetFalAiAceStepAudioOutpaintRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiAceStepAudioOutpaintRequestsByRequestIdStatusResponse =
  GetFalAiAceStepAudioOutpaintRequestsByRequestIdStatusResponses[keyof GetFalAiAceStepAudioOutpaintRequestsByRequestIdStatusResponses]

export type PutFalAiAceStepAudioOutpaintRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ace-step/audio-outpaint/requests/{request_id}/cancel'
}

export type PutFalAiAceStepAudioOutpaintRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiAceStepAudioOutpaintRequestsByRequestIdCancelResponse =
  PutFalAiAceStepAudioOutpaintRequestsByRequestIdCancelResponses[keyof PutFalAiAceStepAudioOutpaintRequestsByRequestIdCancelResponses]

export type PostFalAiAceStepAudioOutpaintData = {
  body: SchemaAceStepAudioOutpaintInput
  path?: never
  query?: never
  url: '/fal-ai/ace-step/audio-outpaint'
}

export type PostFalAiAceStepAudioOutpaintResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiAceStepAudioOutpaintResponse =
  PostFalAiAceStepAudioOutpaintResponses[keyof PostFalAiAceStepAudioOutpaintResponses]

export type GetFalAiAceStepAudioOutpaintRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ace-step/audio-outpaint/requests/{request_id}'
}

export type GetFalAiAceStepAudioOutpaintRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaAceStepAudioOutpaintOutput
}

export type GetFalAiAceStepAudioOutpaintRequestsByRequestIdResponse =
  GetFalAiAceStepAudioOutpaintRequestsByRequestIdResponses[keyof GetFalAiAceStepAudioOutpaintRequestsByRequestIdResponses]

export type GetFalAiAceStepAudioInpaintRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/ace-step/audio-inpaint/requests/{request_id}/status'
}

export type GetFalAiAceStepAudioInpaintRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiAceStepAudioInpaintRequestsByRequestIdStatusResponse =
  GetFalAiAceStepAudioInpaintRequestsByRequestIdStatusResponses[keyof GetFalAiAceStepAudioInpaintRequestsByRequestIdStatusResponses]

export type PutFalAiAceStepAudioInpaintRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ace-step/audio-inpaint/requests/{request_id}/cancel'
}

export type PutFalAiAceStepAudioInpaintRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiAceStepAudioInpaintRequestsByRequestIdCancelResponse =
  PutFalAiAceStepAudioInpaintRequestsByRequestIdCancelResponses[keyof PutFalAiAceStepAudioInpaintRequestsByRequestIdCancelResponses]

export type PostFalAiAceStepAudioInpaintData = {
  body: SchemaAceStepAudioInpaintInput
  path?: never
  query?: never
  url: '/fal-ai/ace-step/audio-inpaint'
}

export type PostFalAiAceStepAudioInpaintResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiAceStepAudioInpaintResponse =
  PostFalAiAceStepAudioInpaintResponses[keyof PostFalAiAceStepAudioInpaintResponses]

export type GetFalAiAceStepAudioInpaintRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ace-step/audio-inpaint/requests/{request_id}'
}

export type GetFalAiAceStepAudioInpaintRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaAceStepAudioInpaintOutput
}

export type GetFalAiAceStepAudioInpaintRequestsByRequestIdResponse =
  GetFalAiAceStepAudioInpaintRequestsByRequestIdResponses[keyof GetFalAiAceStepAudioInpaintRequestsByRequestIdResponses]

export type GetFalAiAceStepAudioToAudioRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/ace-step/audio-to-audio/requests/{request_id}/status'
}

export type GetFalAiAceStepAudioToAudioRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiAceStepAudioToAudioRequestsByRequestIdStatusResponse =
  GetFalAiAceStepAudioToAudioRequestsByRequestIdStatusResponses[keyof GetFalAiAceStepAudioToAudioRequestsByRequestIdStatusResponses]

export type PutFalAiAceStepAudioToAudioRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ace-step/audio-to-audio/requests/{request_id}/cancel'
}

export type PutFalAiAceStepAudioToAudioRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiAceStepAudioToAudioRequestsByRequestIdCancelResponse =
  PutFalAiAceStepAudioToAudioRequestsByRequestIdCancelResponses[keyof PutFalAiAceStepAudioToAudioRequestsByRequestIdCancelResponses]

export type PostFalAiAceStepAudioToAudioData = {
  body: SchemaAceStepAudioToAudioInput
  path?: never
  query?: never
  url: '/fal-ai/ace-step/audio-to-audio'
}

export type PostFalAiAceStepAudioToAudioResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiAceStepAudioToAudioResponse =
  PostFalAiAceStepAudioToAudioResponses[keyof PostFalAiAceStepAudioToAudioResponses]

export type GetFalAiAceStepAudioToAudioRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/ace-step/audio-to-audio/requests/{request_id}'
}

export type GetFalAiAceStepAudioToAudioRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaAceStepAudioToAudioOutput
}

export type GetFalAiAceStepAudioToAudioRequestsByRequestIdResponse =
  GetFalAiAceStepAudioToAudioRequestsByRequestIdResponses[keyof GetFalAiAceStepAudioToAudioRequestsByRequestIdResponses]

export type GetFalAiDiaTtsVoiceCloneRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/dia-tts/voice-clone/requests/{request_id}/status'
}

export type GetFalAiDiaTtsVoiceCloneRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiDiaTtsVoiceCloneRequestsByRequestIdStatusResponse =
  GetFalAiDiaTtsVoiceCloneRequestsByRequestIdStatusResponses[keyof GetFalAiDiaTtsVoiceCloneRequestsByRequestIdStatusResponses]

export type PutFalAiDiaTtsVoiceCloneRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/dia-tts/voice-clone/requests/{request_id}/cancel'
}

export type PutFalAiDiaTtsVoiceCloneRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiDiaTtsVoiceCloneRequestsByRequestIdCancelResponse =
  PutFalAiDiaTtsVoiceCloneRequestsByRequestIdCancelResponses[keyof PutFalAiDiaTtsVoiceCloneRequestsByRequestIdCancelResponses]

export type PostFalAiDiaTtsVoiceCloneData = {
  body: SchemaDiaTtsVoiceCloneInput
  path?: never
  query?: never
  url: '/fal-ai/dia-tts/voice-clone'
}

export type PostFalAiDiaTtsVoiceCloneResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiDiaTtsVoiceCloneResponse =
  PostFalAiDiaTtsVoiceCloneResponses[keyof PostFalAiDiaTtsVoiceCloneResponses]

export type GetFalAiDiaTtsVoiceCloneRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/dia-tts/voice-clone/requests/{request_id}'
}

export type GetFalAiDiaTtsVoiceCloneRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaDiaTtsVoiceCloneOutput
}

export type GetFalAiDiaTtsVoiceCloneRequestsByRequestIdResponse =
  GetFalAiDiaTtsVoiceCloneRequestsByRequestIdResponses[keyof GetFalAiDiaTtsVoiceCloneRequestsByRequestIdResponses]

export type GetFalAiElevenlabsAudioIsolationRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/elevenlabs/audio-isolation/requests/{request_id}/status'
}

export type GetFalAiElevenlabsAudioIsolationRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiElevenlabsAudioIsolationRequestsByRequestIdStatusResponse =
  GetFalAiElevenlabsAudioIsolationRequestsByRequestIdStatusResponses[keyof GetFalAiElevenlabsAudioIsolationRequestsByRequestIdStatusResponses]

export type PutFalAiElevenlabsAudioIsolationRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/elevenlabs/audio-isolation/requests/{request_id}/cancel'
}

export type PutFalAiElevenlabsAudioIsolationRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiElevenlabsAudioIsolationRequestsByRequestIdCancelResponse =
  PutFalAiElevenlabsAudioIsolationRequestsByRequestIdCancelResponses[keyof PutFalAiElevenlabsAudioIsolationRequestsByRequestIdCancelResponses]

export type PostFalAiElevenlabsAudioIsolationData = {
  body: SchemaElevenlabsAudioIsolationInput
  path?: never
  query?: never
  url: '/fal-ai/elevenlabs/audio-isolation'
}

export type PostFalAiElevenlabsAudioIsolationResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiElevenlabsAudioIsolationResponse =
  PostFalAiElevenlabsAudioIsolationResponses[keyof PostFalAiElevenlabsAudioIsolationResponses]

export type GetFalAiElevenlabsAudioIsolationRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/elevenlabs/audio-isolation/requests/{request_id}'
}

export type GetFalAiElevenlabsAudioIsolationRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaElevenlabsAudioIsolationOutput
}

export type GetFalAiElevenlabsAudioIsolationRequestsByRequestIdResponse =
  GetFalAiElevenlabsAudioIsolationRequestsByRequestIdResponses[keyof GetFalAiElevenlabsAudioIsolationRequestsByRequestIdResponses]
