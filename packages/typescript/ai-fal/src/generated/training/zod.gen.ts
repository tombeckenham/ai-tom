// This file is auto-generated by @hey-api/openapi-ts

import { z } from 'zod'

export const zFile = z.object({
  url: z.url(),
  content_type: z.optional(z.string()),
  file_name: z.optional(z.string()),
  file_size: z.optional(z.int()),
})

export const zQueueStatus = z.object({
  status: z.enum(['IN_PROGRESS', 'COMPLETED', 'FAILED']),
  response_url: z.optional(z.url()),
})

/**
 * PublicInput
 */
export const zFluxKreaTrainerInput = z.object({
  images_data_url: z.string().register(z.globalRegistry, {
    description:
      '\n        URL to zip archive with images. Try to use at least 4 images in general the more the better.\n\n        In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.\n    ',
  }),
  is_input_format_already_preprocessed: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "Specifies whether the input data is already in a processed format. When set to False (default), the system expects raw input where image files and their corresponding caption files share the same name (e.g., 'photo.jpg' and 'photo.txt'). Set to True if your data is already in a preprocessed format.",
      }),
    )
    .default(false),
  trigger_word: z.optional(z.union([z.string(), z.null()])),
  steps: z.optional(
    z.int().gte(1).lte(10000).register(z.globalRegistry, {
      description: 'Number of steps to train the LoRA on.',
    }),
  ),
  data_archive_format: z.optional(z.union([z.string(), z.null()])),
  is_style: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If True, the training will be for a style. This will deactivate segmentation, captioning and will use trigger word instead. Use the trigger word to specify the style.',
      }),
    )
    .default(false),
  create_masks: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If True segmentation masks will be used in the weight the training loss. For people a face mask is used if possible.',
      }),
    )
    .default(true),
})

/**
 * File
 */
export const zFalAiFluxKreaTrainerFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * Output
 */
export const zFluxKreaTrainerOutput = z.object({
  config_file: zFalAiFluxKreaTrainerFile,
  debug_preprocessed_output: z.optional(zFalAiFluxKreaTrainerFile),
  diffusers_lora_file: zFalAiFluxKreaTrainerFile,
})

/**
 * PublicInput
 */
export const zFluxLoraFastTrainingInput = z.object({
  images_data_url: z.string().register(z.globalRegistry, {
    description:
      '\n        URL to zip archive with images. Try to use at least 4 images in general the more the better.\n\n        In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.\n    ',
  }),
  is_input_format_already_preprocessed: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          "Specifies whether the input data is already in a processed format. When set to False (default), the system expects raw input where image files and their corresponding caption files share the same name (e.g., 'photo.jpg' and 'photo.txt'). Set to True if your data is already in a preprocessed format.",
      }),
    )
    .default(false),
  trigger_word: z.optional(z.union([z.string(), z.null()])),
  steps: z.optional(
    z.int().gte(1).lte(10000).register(z.globalRegistry, {
      description: 'Number of steps to train the LoRA on.',
    }),
  ),
  data_archive_format: z.optional(z.union([z.string(), z.null()])),
  is_style: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If True, the training will be for a style. This will deactivate segmentation, captioning and will use trigger word instead. Use the trigger word to specify the style.',
      }),
    )
    .default(false),
  create_masks: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If True segmentation masks will be used in the weight the training loss. For people a face mask is used if possible.',
      }),
    )
    .default(true),
})

/**
 * File
 */
export const zFalAiFluxLoraFastTrainingFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * Output
 */
export const zFluxLoraFastTrainingOutput = z.object({
  config_file: zFalAiFluxLoraFastTrainingFile,
  debug_preprocessed_output: z.optional(zFalAiFluxLoraFastTrainingFile),
  diffusers_lora_file: zFalAiFluxLoraFastTrainingFile,
})

/**
 * PublicInput
 */
export const zFluxLoraPortraitTrainerInput = z.object({
  images_data_url: z.string().register(z.globalRegistry, {
    description:
      '\n        URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.\n\n        In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.\n\n        The captions can include a special string `[trigger]`. If a trigger_word is specified, it will replace `[trigger]` in the captions.\n    ',
  }),
  trigger_phrase: z.optional(z.union([z.string(), z.null()])),
  resume_from_checkpoint: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'URL to a checkpoint to resume training from.',
      }),
    )
    .default(''),
  subject_crop: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If True, the subject will be cropped from the image.',
      }),
    )
    .default(true),
  learning_rate: z
    .optional(
      z.number().gte(0.000001).lte(0.001).register(z.globalRegistry, {
        description: 'Learning rate to use for training.',
      }),
    )
    .default(0.00009),
  multiresolution_training: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If True, multiresolution training will be used.',
      }),
    )
    .default(true),
  steps: z
    .optional(
      z.int().gte(1).lte(10000).register(z.globalRegistry, {
        description: 'Number of steps to train the LoRA on.',
      }),
    )
    .default(2500),
  data_archive_format: z.optional(z.union([z.string(), z.null()])),
  create_masks: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'If True, masks will be created for the subject.',
      }),
    )
    .default(false),
})

/**
 * File
 */
export const zFalAiFluxLoraPortraitTrainerFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * Output
 */
export const zFluxLoraPortraitTrainerOutput = z.object({
  config_file: zFalAiFluxLoraPortraitTrainerFile,
  diffusers_lora_file: zFalAiFluxLoraPortraitTrainerFile,
})

/**
 * InputEditV2
 */
export const zFlux2Klein9bBaseTrainerEditInput = z.object({
  steps: z
    .optional(
      z.int().gte(100).lte(10000).register(z.globalRegistry, {
        description: 'Total number of training steps.',
      }),
    )
    .default(1000),
  image_data_url: z.string().register(z.globalRegistry, {
    description:
      '\n    URL to the input data zip archive.\n\n    The zip should contain pairs of images. The images should be named:\n\n    ROOT_start.EXT and ROOT_end.EXT\n    For example:\n    photo_start.jpg and photo_end.jpg\n\n    The zip can also contain up to four reference image for each image pair. The reference images should be named:\n    ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT\n    For example:\n    photo_start.jpg, photo_start2.jpg, photo_end.jpg\n\n    The zip can also contain a text file for each image pair. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    ',
  }),
  learning_rate: z
    .optional(
      z.number().register(z.globalRegistry, {
        description: 'Learning rate applied to trainable parameters.',
      }),
    )
    .default(0.00005),
  default_caption: z.optional(z.union([z.string(), z.unknown()])),
  output_lora_format: z.optional(
    z.enum(['fal', 'comfy']).register(z.globalRegistry, {
      description: 'Dictates the naming scheme for the output weights',
    }),
  ),
})

/**
 * File
 */
export const zFalAiFlux2Klein9bBaseTrainerEditFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * Output
 */
export const zFlux2Klein9bBaseTrainerEditOutput = z.object({
  config_file: zFalAiFlux2Klein9bBaseTrainerEditFile,
  diffusers_lora_file: zFalAiFlux2Klein9bBaseTrainerEditFile,
})

/**
 * InputT2IV2
 *
 * V2 input with multi-resolution bucketing.
 */
export const zFlux2Klein9bBaseTrainerInput = z
  .object({
    steps: z
      .optional(
        z.int().gte(100).lte(10000).register(z.globalRegistry, {
          description: 'Total number of training steps.',
        }),
      )
      .default(1000),
    image_data_url: z.string().register(z.globalRegistry, {
      description:
        '\n    URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.\n\n    The zip can also contain a text file for each image. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    ',
    }),
    learning_rate: z
      .optional(
        z.number().register(z.globalRegistry, {
          description: 'Learning rate applied to trainable parameters.',
        }),
      )
      .default(0.00005),
    default_caption: z.optional(z.union([z.string(), z.unknown()])),
    output_lora_format: z.optional(
      z.enum(['fal', 'comfy']).register(z.globalRegistry, {
        description: 'Dictates the naming scheme for the output weights',
      }),
    ),
  })
  .register(z.globalRegistry, {
    description: 'V2 input with multi-resolution bucketing.',
  })

/**
 * File
 */
export const zFalAiFlux2Klein9bBaseTrainerFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * Output
 */
export const zFlux2Klein9bBaseTrainerOutput = z.object({
  config_file: zFalAiFlux2Klein9bBaseTrainerFile,
  diffusers_lora_file: zFalAiFlux2Klein9bBaseTrainerFile,
})

/**
 * InputT2IV2
 *
 * V2 input with multi-resolution bucketing.
 */
export const zFlux2Klein4bBaseTrainerInput = z
  .object({
    steps: z
      .optional(
        z.int().gte(100).lte(10000).register(z.globalRegistry, {
          description: 'Total number of training steps.',
        }),
      )
      .default(1000),
    image_data_url: z.string().register(z.globalRegistry, {
      description:
        '\n    URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.\n\n    The zip can also contain a text file for each image. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    ',
    }),
    learning_rate: z
      .optional(
        z.number().register(z.globalRegistry, {
          description: 'Learning rate applied to trainable parameters.',
        }),
      )
      .default(0.00005),
    default_caption: z.optional(z.union([z.string(), z.unknown()])),
    output_lora_format: z.optional(
      z.enum(['fal', 'comfy']).register(z.globalRegistry, {
        description: 'Dictates the naming scheme for the output weights',
      }),
    ),
  })
  .register(z.globalRegistry, {
    description: 'V2 input with multi-resolution bucketing.',
  })

/**
 * File
 */
export const zFalAiFlux2Klein4bBaseTrainerFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * Output
 */
export const zFlux2Klein4bBaseTrainerOutput = z.object({
  config_file: zFalAiFlux2Klein4bBaseTrainerFile,
  diffusers_lora_file: zFalAiFlux2Klein4bBaseTrainerFile,
})

/**
 * InputEditV2
 */
export const zFlux2Klein4bBaseTrainerEditInput = z.object({
  steps: z
    .optional(
      z.int().gte(100).lte(10000).register(z.globalRegistry, {
        description: 'Total number of training steps.',
      }),
    )
    .default(1000),
  image_data_url: z.string().register(z.globalRegistry, {
    description:
      '\n    URL to the input data zip archive.\n\n    The zip should contain pairs of images. The images should be named:\n\n    ROOT_start.EXT and ROOT_end.EXT\n    For example:\n    photo_start.jpg and photo_end.jpg\n\n    The zip can also contain up to four reference image for each image pair. The reference images should be named:\n    ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT\n    For example:\n    photo_start.jpg, photo_start2.jpg, photo_end.jpg\n\n    The zip can also contain a text file for each image pair. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    ',
  }),
  learning_rate: z
    .optional(
      z.number().register(z.globalRegistry, {
        description: 'Learning rate applied to trainable parameters.',
      }),
    )
    .default(0.00005),
  default_caption: z.optional(z.union([z.string(), z.unknown()])),
  output_lora_format: z.optional(
    z.enum(['fal', 'comfy']).register(z.globalRegistry, {
      description: 'Dictates the naming scheme for the output weights',
    }),
  ),
})

/**
 * File
 */
export const zFalAiFlux2Klein4bBaseTrainerEditFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * Output
 */
export const zFlux2Klein4bBaseTrainerEditOutput = z.object({
  config_file: zFalAiFlux2Klein4bBaseTrainerEditFile,
  diffusers_lora_file: zFalAiFlux2Klein4bBaseTrainerEditFile,
})

/**
 * Input
 */
export const zQwenImage2512TrainerV2Input = z.object({
  steps: z
    .optional(
      z.int().gte(10).lte(40000).register(z.globalRegistry, {
        description: 'Number of steps to train for',
      }),
    )
    .default(2000),
  image_data_url: z.string().register(z.globalRegistry, {
    description:
      '\n        URL to the input data zip archive.\n\n        The zip should contain pairs of images and corresponding captions.\n\n        The images should be named: ROOT.EXT. For example: 001.jpg\n\n        The corresponding captions should be named: ROOT.txt. For example: 001.txt\n\n        If no text file is provided for an image, the default_caption will be used.\n        ',
  }),
  learning_rate: z
    .optional(
      z.number().register(z.globalRegistry, {
        description: 'Learning rate.',
      }),
    )
    .default(0.0005),
  default_caption: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * File
 */
export const zFalAiQwenImage2512TrainerV2File = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * Output
 */
export const zQwenImage2512TrainerV2Output = z.object({
  config_file: zFalAiQwenImage2512TrainerV2File,
  diffusers_lora_file: zFalAiQwenImage2512TrainerV2File,
})

/**
 * InputEditV2
 */
export const zFlux2TrainerV2EditInput = z.object({
  steps: z
    .optional(
      z.int().gte(100).lte(10000).register(z.globalRegistry, {
        description: 'Total number of training steps.',
      }),
    )
    .default(1000),
  image_data_url: z.string().register(z.globalRegistry, {
    description:
      '\n    URL to the input data zip archive.\n\n    The zip should contain pairs of images. The images should be named:\n\n    ROOT_start.EXT and ROOT_end.EXT\n    For example:\n    photo_start.jpg and photo_end.jpg\n\n    The zip can also contain up to four reference image for each image pair. The reference images should be named:\n    ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT\n    For example:\n    photo_start.jpg, photo_start2.jpg, photo_end.jpg\n\n    The zip can also contain a text file for each image pair. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    ',
  }),
  learning_rate: z
    .optional(
      z.number().register(z.globalRegistry, {
        description: 'Learning rate applied to trainable parameters.',
      }),
    )
    .default(0.00005),
  default_caption: z.optional(z.union([z.string(), z.unknown()])),
  output_lora_format: z.optional(
    z.enum(['fal', 'comfy']).register(z.globalRegistry, {
      description: 'Dictates the naming scheme for the output weights',
    }),
  ),
})

/**
 * File
 */
export const zFalAiFlux2TrainerV2EditFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * Output
 */
export const zFlux2TrainerV2EditOutput = z.object({
  config_file: zFalAiFlux2TrainerV2EditFile,
  diffusers_lora_file: zFalAiFlux2TrainerV2EditFile,
})

/**
 * InputT2IV2
 *
 * V2 input with multi-resolution bucketing.
 */
export const zFlux2TrainerV2Input = z
  .object({
    steps: z
      .optional(
        z.int().gte(100).lte(10000).register(z.globalRegistry, {
          description: 'Total number of training steps.',
        }),
      )
      .default(1000),
    image_data_url: z.string().register(z.globalRegistry, {
      description:
        '\n    URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.\n\n    The zip can also contain a text file for each image. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    ',
    }),
    learning_rate: z
      .optional(
        z.number().register(z.globalRegistry, {
          description: 'Learning rate applied to trainable parameters.',
        }),
      )
      .default(0.00005),
    default_caption: z.optional(z.union([z.string(), z.unknown()])),
    output_lora_format: z.optional(
      z.enum(['fal', 'comfy']).register(z.globalRegistry, {
        description: 'Dictates the naming scheme for the output weights',
      }),
    ),
  })
  .register(z.globalRegistry, {
    description: 'V2 input with multi-resolution bucketing.',
  })

/**
 * File
 */
export const zFalAiFlux2TrainerV2File = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * Output
 */
export const zFlux2TrainerV2Output = z.object({
  config_file: zFalAiFlux2TrainerV2File,
  diffusers_lora_file: zFalAiFlux2TrainerV2File,
})

/**
 * V2VValidation
 *
 * Validation input for video-to-video training.
 */
export const zV2vValidation = z
  .object({
    prompt: z.string().register(z.globalRegistry, {
      description: 'The prompt to use for validation.',
    }),
    reference_video_url: z.string().min(1).register(z.globalRegistry, {
      description:
        'URL to reference video for IC-LoRA validation. This is the input video that will be transformed.',
    }),
  })
  .register(z.globalRegistry, {
    description: 'Validation input for video-to-video training.',
  })

/**
 * LTX2V2VInput
 *
 * Input configuration for LTX-2 video-to-video (IC-LoRA) training.
 */
export const zLtx2V2vTrainerInput = z
  .object({
    number_of_steps: z
      .optional(
        z.int().gte(100).lte(20000).register(z.globalRegistry, {
          description: 'The number of training steps.',
        }),
      )
      .default(2000),
    frame_rate: z
      .optional(
        z.int().gte(8).lte(60).register(z.globalRegistry, {
          description: 'Target frames per second for the video.',
        }),
      )
      .default(25),
    learning_rate: z
      .optional(
        z.number().gte(0.000001).lte(1).register(z.globalRegistry, {
          description:
            'Learning rate for optimization. Higher values can lead to faster training but may cause overfitting.',
        }),
      )
      .default(0.0002),
    validation: z
      .optional(
        z.array(zV2vValidation).max(2).register(z.globalRegistry, {
          description:
            'A list of validation inputs with prompts and reference videos.',
        }),
      )
      .default([]),
    number_of_frames: z
      .optional(
        z.int().gte(9).lte(121).register(z.globalRegistry, {
          description:
            'Number of frames per training sample. Must satisfy frames % 8 == 1 (e.g., 1, 9, 17, 25, 33, 41, 49, 57, 65, 73, 81, 89, 97).',
        }),
      )
      .default(89),
    training_data_url: z.string().register(z.globalRegistry, {
      description:
        'URL to zip archive with videos or images. Try to use at least 10 files, although more is better.\n\n        **Supported video formats:** .mp4, .mov, .avi, .mkv\n        **Supported image formats:** .png, .jpg, .jpeg\n\n        Note: The dataset must contain ONLY videos OR ONLY images - mixed datasets are not supported.\n\n        The archive can also contain text files with captions. Each text file should have the same name as the media file it corresponds to.',
    }),
    split_input_duration_threshold: z
      .optional(
        z.number().gte(1).lte(60).register(z.globalRegistry, {
          description:
            'The duration threshold in seconds. If a video is longer than this, it will be split into scenes.',
        }),
      )
      .default(30),
    rank: z.optional(
      z
        .union([
          z.literal(8),
          z.literal(16),
          z.literal(32),
          z.literal(64),
          z.literal(128),
        ])
        .register(z.globalRegistry, {
          description:
            'The rank of the LoRA adaptation. Higher values increase capacity but use more memory.',
        }),
    ),
    first_frame_conditioning_p: z
      .optional(
        z.number().gte(0).lte(1).register(z.globalRegistry, {
          description:
            'Probability of conditioning on the first frame during training. Lower values work better for video-to-video transformation.',
        }),
      )
      .default(0.1),
    stg_scale: z
      .optional(
        z.number().gte(0).lte(3).register(z.globalRegistry, {
          description:
            'STG (Spatio-Temporal Guidance) scale. 0.0 disables STG. Recommended value is 1.0.',
        }),
      )
      .default(1),
    resolution: z.optional(
      z.enum(['low', 'medium', 'high']).register(z.globalRegistry, {
        description:
          'Resolution to use for training. Higher resolutions require more memory.',
      }),
    ),
    split_input_into_scenes: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'If true, videos above a certain duration threshold will be split into scenes.',
        }),
      )
      .default(true),
    trigger_phrase: z
      .optional(
        z.string().register(z.globalRegistry, {
          description:
            'A phrase that will trigger the LoRA style. Will be prepended to captions during training.',
        }),
      )
      .default(''),
    validation_frame_rate: z
      .optional(
        z.int().gte(8).lte(60).register(z.globalRegistry, {
          description: 'Target frames per second for validation videos.',
        }),
      )
      .default(25),
    aspect_ratio: z.optional(
      z.enum(['16:9', '1:1', '9:16']).register(z.globalRegistry, {
        description: 'Aspect ratio to use for training.',
      }),
    ),
    validation_resolution: z.optional(
      z.enum(['low', 'medium', 'high']).register(z.globalRegistry, {
        description: 'The resolution to use for validation.',
      }),
    ),
    validation_number_of_frames: z
      .optional(
        z.int().gte(9).lte(121).register(z.globalRegistry, {
          description: 'The number of frames in validation videos.',
        }),
      )
      .default(89),
    validation_aspect_ratio: z.optional(
      z.enum(['16:9', '1:1', '9:16']).register(z.globalRegistry, {
        description: 'The aspect ratio to use for validation.',
      }),
    ),
    validation_negative_prompt: z
      .optional(
        z.string().register(z.globalRegistry, {
          description: 'A negative prompt to use for validation.',
        }),
      )
      .default(
        'worst quality, inconsistent motion, blurry, jittery, distorted',
      ),
    auto_scale_input: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'If true, videos will be automatically scaled to the target frame count and fps. This option has no effect on image datasets.',
        }),
      )
      .default(false),
  })
  .register(z.globalRegistry, {
    description:
      'Input configuration for LTX-2 video-to-video (IC-LoRA) training.',
  })

/**
 * File
 */
export const zFalAiLtx2V2vTrainerFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * LTX2V2VOutput
 *
 * Output from LTX-2 video-to-video training.
 */
export const zLtx2V2vTrainerOutput = z
  .object({
    lora_file: zFalAiLtx2V2vTrainerFile,
    config_file: zFalAiLtx2V2vTrainerFile,
    debug_dataset: z.optional(z.union([zFalAiLtx2V2vTrainerFile, z.unknown()])),
    video: z.union([zFalAiLtx2V2vTrainerFile, z.unknown()]),
  })
  .register(z.globalRegistry, {
    description: 'Output from LTX-2 video-to-video training.',
  })

/**
 * Validation
 */
export const zValidation = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt to use for validation.',
  }),
  image_url: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * LTX2Input
 *
 * Input configuration for LTX-2 text-to-video training.
 */
export const zLtx2VideoTrainerInput = z
  .object({
    audio_normalize: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'Normalize audio peak amplitude to a consistent level. Recommended for consistent audio levels across the dataset.',
        }),
      )
      .default(true),
    audio_preserve_pitch: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            "When audio duration doesn't match video duration, stretch/compress audio without changing pitch. If disabled, audio is trimmed or padded with silence.",
        }),
      )
      .default(true),
    frame_rate: z
      .optional(
        z.int().gte(8).lte(60).register(z.globalRegistry, {
          description: 'Target frames per second for the video.',
        }),
      )
      .default(25),
    number_of_steps: z
      .optional(
        z.int().gte(100).lte(20000).register(z.globalRegistry, {
          description: 'The number of training steps.',
        }),
      )
      .default(2000),
    learning_rate: z
      .optional(
        z.number().gte(0.000001).lte(1).register(z.globalRegistry, {
          description:
            'Learning rate for optimization. Higher values can lead to faster training but may cause overfitting.',
        }),
      )
      .default(0.0002),
    validation: z
      .optional(
        z.array(zValidation).max(2).register(z.globalRegistry, {
          description:
            'A list of validation prompts to use during training. When providing an image, _all_ validation inputs must have an image.',
        }),
      )
      .default([]),
    number_of_frames: z
      .optional(
        z.int().gte(9).lte(121).register(z.globalRegistry, {
          description:
            'Number of frames per training sample. Must satisfy frames % 8 == 1 (e.g., 1, 9, 17, 25, 33, 41, 49, 57, 65, 73, 81, 89, 97).',
        }),
      )
      .default(89),
    training_data_url: z.string().register(z.globalRegistry, {
      description:
        'URL to zip archive with videos or images. Try to use at least 10 files, although more is better.\n\n        **Supported video formats:** .mp4, .mov, .avi, .mkv\n        **Supported image formats:** .png, .jpg, .jpeg\n\n        Note: The dataset must contain ONLY videos OR ONLY images - mixed datasets are not supported.\n\n        The archive can also contain text files with captions. Each text file should have the same name as the media file it corresponds to.',
    }),
    split_input_duration_threshold: z
      .optional(
        z.number().gte(1).lte(60).register(z.globalRegistry, {
          description:
            'The duration threshold in seconds. If a video is longer than this, it will be split into scenes.',
        }),
      )
      .default(30),
    rank: z.optional(
      z
        .union([
          z.literal(8),
          z.literal(16),
          z.literal(32),
          z.literal(64),
          z.literal(128),
        ])
        .register(z.globalRegistry, {
          description:
            'The rank of the LoRA adaptation. Higher values increase capacity but use more memory.',
        }),
    ),
    first_frame_conditioning_p: z
      .optional(
        z.number().gte(0).lte(1).register(z.globalRegistry, {
          description:
            'Probability of conditioning on the first frame during training. Higher values improve image-to-video performance.',
        }),
      )
      .default(0.5),
    stg_scale: z
      .optional(
        z.number().gte(0).lte(3).register(z.globalRegistry, {
          description:
            'STG (Spatio-Temporal Guidance) scale. 0.0 disables STG. Recommended value is 1.0.',
        }),
      )
      .default(1),
    aspect_ratio: z.optional(
      z.enum(['16:9', '1:1', '9:16']).register(z.globalRegistry, {
        description: 'Aspect ratio to use for training.',
      }),
    ),
    trigger_phrase: z
      .optional(
        z.string().register(z.globalRegistry, {
          description:
            'A phrase that will trigger the LoRA style. Will be prepended to captions during training.',
        }),
      )
      .default(''),
    resolution: z.optional(
      z.enum(['low', 'medium', 'high']).register(z.globalRegistry, {
        description:
          'Resolution to use for training. Higher resolutions require more memory.',
      }),
    ),
    validation_frame_rate: z
      .optional(
        z.int().gte(8).lte(60).register(z.globalRegistry, {
          description: 'Target frames per second for validation videos.',
        }),
      )
      .default(25),
    split_input_into_scenes: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'If true, videos above a certain duration threshold will be split into scenes.',
        }),
      )
      .default(true),
    with_audio: z.optional(z.union([z.boolean(), z.unknown()])),
    generate_audio_in_validation: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description: 'Whether to generate audio in validation samples.',
        }),
      )
      .default(true),
    validation_resolution: z.optional(
      z.enum(['low', 'medium', 'high']).register(z.globalRegistry, {
        description: 'The resolution to use for validation.',
      }),
    ),
    validation_number_of_frames: z
      .optional(
        z.int().gte(9).lte(121).register(z.globalRegistry, {
          description: 'The number of frames in validation videos.',
        }),
      )
      .default(89),
    validation_aspect_ratio: z.optional(
      z.enum(['16:9', '1:1', '9:16']).register(z.globalRegistry, {
        description: 'The aspect ratio to use for validation.',
      }),
    ),
    validation_negative_prompt: z
      .optional(
        z.string().register(z.globalRegistry, {
          description: 'A negative prompt to use for validation.',
        }),
      )
      .default(
        'worst quality, inconsistent motion, blurry, jittery, distorted',
      ),
    auto_scale_input: z
      .optional(
        z.boolean().register(z.globalRegistry, {
          description:
            'If true, videos will be automatically scaled to the target frame count and fps. This option has no effect on image datasets.',
        }),
      )
      .default(false),
  })
  .register(z.globalRegistry, {
    description: 'Input configuration for LTX-2 text-to-video training.',
  })

/**
 * File
 */
export const zFalAiLtx2VideoTrainerFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * LTX2Output
 *
 * Output from LTX-2 training.
 */
export const zLtx2VideoTrainerOutput = z
  .object({
    lora_file: zFalAiLtx2VideoTrainerFile,
    config_file: zFalAiLtx2VideoTrainerFile,
    debug_dataset: z.optional(
      z.union([zFalAiLtx2VideoTrainerFile, z.unknown()]),
    ),
    video: z.union([zFalAiLtx2VideoTrainerFile, z.unknown()]),
  })
  .register(z.globalRegistry, {
    description: 'Output from LTX-2 training.',
  })

/**
 * InputImage
 */
export const zQwenImage2512TrainerInput = z.object({
  steps: z
    .optional(
      z.int().gte(100).lte(30000).register(z.globalRegistry, {
        description: 'Number of steps to train for',
      }),
    )
    .default(1000),
  image_data_url: z.string().register(z.globalRegistry, {
    description:
      '\n    URL to the input data zip archive for text-to-image training.\n\n    The zip should contain images with their corresponding text captions:\n\n    image.EXT and image.txt\n    For example:\n    photo.jpg and photo.txt\n\n    The text file contains the caption/prompt describing the target image.\n\n    If no text file is provided for an image, the default_caption will be used.\n\n    If no default_caption is provided and a text file is missing, the training will fail.\n    ',
  }),
  learning_rate: z
    .optional(
      z.number().register(z.globalRegistry, {
        description: 'Learning rate for LoRA parameters.',
      }),
    )
    .default(0.0005),
  default_caption: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * File
 */
export const zFalAiQwenImage2512TrainerFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * Output
 */
export const zQwenImage2512TrainerOutput = z.object({
  config_file: zFalAiQwenImage2512TrainerFile,
  diffusers_lora_file: zFalAiQwenImage2512TrainerFile,
})

/**
 * Input2511
 */
export const zQwenImageEdit2511TrainerInput = z.object({
  steps: z
    .optional(
      z.int().gte(100).lte(30000).register(z.globalRegistry, {
        description: 'Number of steps to train for',
      }),
    )
    .default(1000),
  image_data_url: z.string().register(z.globalRegistry, {
    description:
      '\n    URL to the input data zip archive.\n\n    The zip should contain pairs of images. The images should be named:\n\n    ROOT_start.EXT and ROOT_end.EXT\n    For example:\n    photo_start.jpg and photo_end.jpg\n\n    The zip can also contain more than one reference image for each image pair. The reference images should be named:\n    ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT\n    For example:\n    photo_start.jpg, photo_start2.jpg, photo_end.jpg\n\n    The Reference Image Count field should be set to the number of reference images.\n\n    The zip can also contain a text file for each image pair. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    ',
  }),
  learning_rate: z
    .optional(
      z.number().register(z.globalRegistry, {
        description: 'Learning rate for LoRA parameters.',
      }),
    )
    .default(0.0001),
  default_caption: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * File
 */
export const zFalAiQwenImageEdit2511TrainerFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * Output
 */
export const zQwenImageEdit2511TrainerOutput = z.object({
  config_file: zFalAiQwenImageEdit2511TrainerFile,
  diffusers_lora_file: zFalAiQwenImageEdit2511TrainerFile,
})

/**
 * Input
 */
export const zQwenImageLayeredTrainerInput = z.object({
  steps: z
    .optional(
      z.int().gte(100).lte(10000).register(z.globalRegistry, {
        description: 'Number of steps to train for',
      }),
    )
    .default(1000),
  image_data_url: z.string().register(z.globalRegistry, {
    description:
      '\n    URL to the input data zip archive.\n\n    The zip should contain groups of images. The images should be named:\n\n    ROOT_start.EXT, ROOT_end.EXT, ROOT_end2.EXT, ..., ROOT_endN.EXT\n    For example:\n    photo_start.png, photo_end.png, photo_end2.png, ..., photo_endN.png\n\n    The start image is the base image that will be decomposed into layers.\n    The end images are the layers that will be added to the base image.  ROOT_end.EXT is the first layer, ROOT_end2.EXT is the second layer, and so on.\n    You can have up to 8 layers.\n    All image groups must have the same number of output layers.\n\n    The end images can contain transparent regions. Only PNG and WebP images are supported since these are the only formats that support transparency.\n\n    The zip can also contain a text file for each image group. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify a description of the base image.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    ',
  }),
  learning_rate: z
    .optional(
      z.number().register(z.globalRegistry, {
        description: 'Learning rate for LoRA parameters.',
      }),
    )
    .default(0.0001),
  default_caption: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * File
 */
export const zFalAiQwenImageLayeredTrainerFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * Output
 */
export const zQwenImageLayeredTrainerOutput = z.object({
  config_file: zFalAiQwenImageLayeredTrainerFile,
  diffusers_lora_file: zFalAiQwenImageLayeredTrainerFile,
})

/**
 * InputPlus
 */
export const zQwenImageEdit2509TrainerInput = z.object({
  steps: z
    .optional(
      z.int().gte(100).lte(30000).register(z.globalRegistry, {
        description: 'Number of steps to train for',
      }),
    )
    .default(1000),
  image_data_url: z.string().register(z.globalRegistry, {
    description:
      '\n    URL to the input data zip archive.\n\n    The zip should contain pairs of images. The images should be named:\n\n    ROOT_start.EXT and ROOT_end.EXT\n    For example:\n    photo_start.jpg and photo_end.jpg\n\n    The zip can also contain more than one reference image for each image pair. The reference images should be named:\n    ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT\n    For example:\n    photo_start.jpg, photo_start2.jpg, photo_end.jpg\n\n    The Reference Image Count field should be set to the number of reference images.\n\n    The zip can also contain a text file for each image pair. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    ',
  }),
  learning_rate: z
    .optional(
      z.number().register(z.globalRegistry, {
        description: 'Learning rate for LoRA parameters.',
      }),
    )
    .default(0.0001),
  default_caption: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * File
 */
export const zFalAiQwenImageEdit2509TrainerFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * Output
 */
export const zQwenImageEdit2509TrainerOutput = z.object({
  config_file: zFalAiQwenImageEdit2509TrainerFile,
  diffusers_lora_file: zFalAiQwenImageEdit2509TrainerFile,
})

/**
 * Input
 */
export const zZImageTrainerInput = z.object({
  steps: z
    .optional(
      z.int().gte(100).lte(10000).register(z.globalRegistry, {
        description: 'Total number of training steps.',
      }),
    )
    .default(1000),
  image_data_url: z.string().register(z.globalRegistry, {
    description:
      '\n    URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.\n\n    The zip can also contain a text file for each image. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    ',
  }),
  training_type: z.optional(
    z.enum(['content', 'style', 'balanced']).register(z.globalRegistry, {
      description:
        "Type of training to perform. Use 'content' to focus on the content of the images, 'style' to focus on the style of the images, and 'balanced' to focus on a combination of both.",
    }),
  ),
  learning_rate: z
    .optional(
      z.number().register(z.globalRegistry, {
        description: 'Learning rate applied to trainable parameters.',
      }),
    )
    .default(0.0001),
  default_caption: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * File
 */
export const zFalAiZImageTrainerFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * Output
 */
export const zZImageTrainerOutput = z.object({
  config_file: zFalAiZImageTrainerFile,
  diffusers_lora_file: zFalAiZImageTrainerFile,
})

/**
 * InputEdit
 */
export const zFlux2TrainerEditInput = z.object({
  steps: z
    .optional(
      z.int().gte(100).lte(10000).register(z.globalRegistry, {
        description: 'Total number of training steps.',
      }),
    )
    .default(1000),
  image_data_url: z.string().register(z.globalRegistry, {
    description:
      '\n    URL to the input data zip archive.\n\n    The zip should contain pairs of images. The images should be named:\n\n    ROOT_start.EXT and ROOT_end.EXT\n    For example:\n    photo_start.jpg and photo_end.jpg\n\n    The zip can also contain up to four reference image for each image pair. The reference images should be named:\n    ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT\n    For example:\n    photo_start.jpg, photo_start2.jpg, photo_end.jpg\n\n    The zip can also contain a text file for each image pair. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    ',
  }),
  learning_rate: z
    .optional(
      z.number().register(z.globalRegistry, {
        description: 'Learning rate applied to trainable parameters.',
      }),
    )
    .default(0.00005),
  default_caption: z.optional(z.union([z.string(), z.unknown()])),
  output_lora_format: z.optional(
    z.enum(['fal', 'comfy']).register(z.globalRegistry, {
      description: 'Dictates the naming scheme for the output weights',
    }),
  ),
})

/**
 * File
 */
export const zFalAiFlux2TrainerEditFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * Output
 */
export const zFlux2TrainerEditOutput = z.object({
  config_file: zFalAiFlux2TrainerEditFile,
  diffusers_lora_file: zFalAiFlux2TrainerEditFile,
})

/**
 * InputT2I
 */
export const zFlux2TrainerInput = z.object({
  steps: z
    .optional(
      z.int().gte(100).lte(10000).register(z.globalRegistry, {
        description: 'Total number of training steps.',
      }),
    )
    .default(1000),
  image_data_url: z.string().register(z.globalRegistry, {
    description:
      '\n    URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.\n\n    The zip can also contain a text file for each image. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    ',
  }),
  learning_rate: z
    .optional(
      z.number().register(z.globalRegistry, {
        description: 'Learning rate applied to trainable parameters.',
      }),
    )
    .default(0.00005),
  default_caption: z.optional(z.union([z.string(), z.unknown()])),
  output_lora_format: z.optional(
    z.enum(['fal', 'comfy']).register(z.globalRegistry, {
      description: 'Dictates the naming scheme for the output weights',
    }),
  ),
})

/**
 * File
 */
export const zFalAiFlux2TrainerFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * Output
 */
export const zFlux2TrainerOutput = z.object({
  config_file: zFalAiFlux2TrainerFile,
  diffusers_lora_file: zFalAiFlux2TrainerFile,
})

/**
 * InputPlus
 */
export const zQwenImageEditPlusTrainerInput = z.object({
  steps: z
    .optional(
      z.int().gte(100).lte(30000).register(z.globalRegistry, {
        description: 'Number of steps to train for',
      }),
    )
    .default(1000),
  image_data_url: z.string().register(z.globalRegistry, {
    description:
      '\n    URL to the input data zip archive.\n\n    The zip should contain pairs of images. The images should be named:\n\n    ROOT_start.EXT and ROOT_end.EXT\n    For example:\n    photo_start.jpg and photo_end.jpg\n\n    The zip can also contain more than one reference image for each image pair. The reference images should be named:\n    ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT\n    For example:\n    photo_start.jpg, photo_start2.jpg, photo_end.jpg\n\n    The Reference Image Count field should be set to the number of reference images.\n\n    The zip can also contain a text file for each image pair. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    ',
  }),
  learning_rate: z
    .optional(
      z.number().register(z.globalRegistry, {
        description: 'Learning rate for LoRA parameters.',
      }),
    )
    .default(0.0001),
  default_caption: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * File
 */
export const zFalAiQwenImageEditPlusTrainerFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * Output
 */
export const zQwenImageEditPlusTrainerOutput = z.object({
  config_file: zFalAiQwenImageEditPlusTrainerFile,
  diffusers_lora_file: zFalAiQwenImageEditPlusTrainerFile,
})

/**
 * InputEdit
 */
export const zQwenImageEditTrainerInput = z.object({
  steps: z
    .optional(
      z.int().gte(100).lte(30000).register(z.globalRegistry, {
        description: 'Number of steps to train for',
      }),
    )
    .default(1000),
  image_data_url: z.string().register(z.globalRegistry, {
    description:
      '\n    URL to the input data zip archive.\n\n    The zip should contain pairs of images. The images should be named:\n\n    ROOT_start.EXT and ROOT_end.EXT\n    For example:\n    photo_start.jpg and photo_end.jpg\n\n    The zip can also contain a text file for each image pair. The text file should be named:\n    ROOT.txt\n    For example:\n    photo.txt\n\n    This text file can be used to specify the edit instructions for the image pair.\n\n    If no text file is provided, the default_caption will be used.\n\n    If no default_caption is provided, the training will fail.\n    ',
  }),
  learning_rate: z
    .optional(
      z.number().register(z.globalRegistry, {
        description: 'Learning rate for LoRA parameters.',
      }),
    )
    .default(0.0001),
  default_caption: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * File
 */
export const zFalAiQwenImageEditTrainerFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * Output
 */
export const zQwenImageEditTrainerOutput = z.object({
  config_file: zFalAiQwenImageEditTrainerFile,
  diffusers_lora_file: zFalAiQwenImageEditTrainerFile,
})

/**
 * PublicInput
 */
export const zQwenImageTrainerInput = z.object({
  steps: z
    .optional(
      z.int().gte(1).lte(8000).register(z.globalRegistry, {
        description:
          'Total number of training steps to perform. Default is 4000.',
      }),
    )
    .default(1000),
  image_data_url: z.string().register(z.globalRegistry, {
    description:
      '\n        URL to zip archive with images for training. The archive should contain images and corresponding text files with captions.\n        Each text file should have the same name as the image file it corresponds to (e.g., image1.jpg and image1.txt).\n        If text files are missing for some images, you can provide a trigger_phrase to automatically create them.\n        Supported image formats: PNG, JPG, JPEG, WEBP.\n        Try to use at least 10 images, although more is better.\n    ',
  }),
  learning_rate: z
    .optional(
      z.number().gte(0.000001).lte(0.01).register(z.globalRegistry, {
        description: 'Learning rate for training. Default is 5e-4',
      }),
    )
    .default(0.0005),
  trigger_phrase: z
    .optional(
      z.string().register(z.globalRegistry, {
        description:
          "Default caption to use for images that don't have corresponding text files. If provided, missing .txt files will be created automatically.",
      }),
    )
    .default(''),
})

/**
 * File
 */
export const zFalAiQwenImageTrainerFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * Output
 */
export const zQwenImageTrainerOutput = z.object({
  lora_file: zFalAiQwenImageTrainerFile,
  config_file: zFalAiQwenImageTrainerFile,
})

/**
 * BasicInput
 */
export const zWan22ImageTrainerInput = z.object({
  trigger_phrase: z.string().register(z.globalRegistry, {
    description: 'Trigger phrase for the model.',
  }),
  use_masks: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to use masks for the training data.',
      }),
    )
    .default(true),
  learning_rate: z
    .optional(
      z.number().gte(0.000001).lte(0.1).register(z.globalRegistry, {
        description: 'Learning rate for training.',
      }),
    )
    .default(0.0007),
  use_face_cropping: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to use face cropping for the training data. When enabled, images will be cropped to the face before resizing.',
      }),
    )
    .default(false),
  training_data_url: z.string().register(z.globalRegistry, {
    description: 'URL to the training data.',
  }),
  steps: z
    .optional(
      z.int().gte(10).lte(6000).register(z.globalRegistry, {
        description: 'Number of training steps.',
      }),
    )
    .default(1000),
  include_synthetic_captions: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to include synthetic captions.',
      }),
    )
    .default(false),
  is_style: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether the training data is style data. If true, face specific options like masking and face detection will be disabled.',
      }),
    )
    .default(false),
  use_face_detection: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to use face detection for the training data. When enabled, images will use the center of the face as the center of the image when resizing.',
      }),
    )
    .default(true),
})

/**
 * File
 */
export const zFalAiWan22ImageTrainerFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * WanTrainerResponse
 */
export const zWan22ImageTrainerOutput = z.object({
  config_file: zFalAiWan22ImageTrainerFile,
  high_noise_lora: zFalAiWan22ImageTrainerFile,
  diffusers_lora_file: zFalAiWan22ImageTrainerFile,
})

/**
 * Input
 */
export const zFluxKontextTrainerInput = z.object({
  steps: z
    .optional(
      z.int().gte(2).lte(10000).register(z.globalRegistry, {
        description: 'Number of steps to train for',
      }),
    )
    .default(1000),
  image_data_url: z.string().register(z.globalRegistry, {
    description:
      '\n        URL to the input data zip archive.\n\n        The zip should contain pairs of images. The images should be named:\n\n        ROOT_start.EXT and ROOT_end.EXT\n        For example:\n        photo_start.jpg and photo_end.jpg\n\n        The zip can also contain a text file for each image pair. The text file should be named:\n        ROOT.txt\n        For example:\n        photo.txt\n\n        This text file can be used to specify the edit instructions for the image pair.\n\n        If no text file is provided, the default_caption will be used.\n\n        If no default_caption is provided, the training will fail.\n        ',
  }),
  learning_rate: z.optional(z.number()).default(0.0001),
  default_caption: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'Default caption to use when caption files are missing. If None, missing captions will cause an error.',
    }),
  ),
  output_lora_format: z.optional(
    z.enum(['fal', 'comfy']).register(z.globalRegistry, {
      description: 'Dictates the naming scheme for the output weights',
    }),
  ),
})

/**
 * File
 */
export const zFalAiFluxKontextTrainerFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * Output
 */
export const zFluxKontextTrainerOutput = z.object({
  config_file: zFalAiFluxKontextTrainerFile,
  diffusers_lora_file: zFalAiFluxKontextTrainerFile,
})

/**
 * Input
 */
export const zWanTrainerT2vInput = z.object({
  number_of_steps: z
    .optional(
      z.int().gte(1).lte(20000).register(z.globalRegistry, {
        description: 'The number of steps to train for.',
      }),
    )
    .default(400),
  training_data_url: z.string().register(z.globalRegistry, {
    description:
      'URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.\n\n        In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.',
  }),
  trigger_phrase: z
    .optional(
      z.string().register(z.globalRegistry, {
        description:
          'The phrase that will trigger the model to generate an image.',
      }),
    )
    .default(''),
  learning_rate: z
    .optional(
      z.number().gte(0.000001).lte(1).register(z.globalRegistry, {
        description:
          'The rate at which the model learns. Higher values can lead to faster training, but over-fitting.',
      }),
    )
    .default(0.0002),
  auto_scale_input: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the input will be automatically scale the video to 81 frames at 16fps.',
      }),
    )
    .default(false),
})

/**
 * File
 */
export const zFalAiWanTrainerT2vFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * Output
 */
export const zWanTrainerT2vOutput = z.object({
  lora_file: zFalAiWanTrainerT2vFile,
  config_file: zFalAiWanTrainerT2vFile,
})

/**
 * Input
 */
export const zWanTrainerT2V14bInput = z.object({
  number_of_steps: z
    .optional(
      z.int().gte(1).lte(20000).register(z.globalRegistry, {
        description: 'The number of steps to train for.',
      }),
    )
    .default(400),
  training_data_url: z.string().register(z.globalRegistry, {
    description:
      'URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.\n\n        In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.',
  }),
  trigger_phrase: z
    .optional(
      z.string().register(z.globalRegistry, {
        description:
          'The phrase that will trigger the model to generate an image.',
      }),
    )
    .default(''),
  learning_rate: z
    .optional(
      z.number().gte(0.000001).lte(1).register(z.globalRegistry, {
        description:
          'The rate at which the model learns. Higher values can lead to faster training, but over-fitting.',
      }),
    )
    .default(0.0002),
  auto_scale_input: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the input will be automatically scale the video to 81 frames at 16fps.',
      }),
    )
    .default(false),
})

/**
 * File
 */
export const zFalAiWanTrainerT2V14bFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * Output
 */
export const zWanTrainerT2V14bOutput = z.object({
  lora_file: zFalAiWanTrainerT2V14bFile,
  config_file: zFalAiWanTrainerT2V14bFile,
})

/**
 * Input
 */
export const zWanTrainerI2V720pInput = z.object({
  number_of_steps: z
    .optional(
      z.int().gte(1).lte(20000).register(z.globalRegistry, {
        description: 'The number of steps to train for.',
      }),
    )
    .default(400),
  training_data_url: z.string().register(z.globalRegistry, {
    description:
      'URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.\n\n        In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.',
  }),
  trigger_phrase: z
    .optional(
      z.string().register(z.globalRegistry, {
        description:
          'The phrase that will trigger the model to generate an image.',
      }),
    )
    .default(''),
  learning_rate: z
    .optional(
      z.number().gte(0.000001).lte(1).register(z.globalRegistry, {
        description:
          'The rate at which the model learns. Higher values can lead to faster training, but over-fitting.',
      }),
    )
    .default(0.0002),
  auto_scale_input: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the input will be automatically scale the video to 81 frames at 16fps.',
      }),
    )
    .default(false),
})

/**
 * File
 */
export const zFalAiWanTrainerI2V720pFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * Output
 */
export const zWanTrainerI2V720pOutput = z.object({
  lora_file: zFalAiWanTrainerI2V720pFile,
  config_file: zFalAiWanTrainerI2V720pFile,
})

/**
 * Input
 */
export const zWanTrainerFlf2V720pInput = z.object({
  number_of_steps: z
    .optional(
      z.int().gte(1).lte(20000).register(z.globalRegistry, {
        description: 'The number of steps to train for.',
      }),
    )
    .default(400),
  training_data_url: z.string().register(z.globalRegistry, {
    description:
      'URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.\n\n        In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.',
  }),
  trigger_phrase: z
    .optional(
      z.string().register(z.globalRegistry, {
        description:
          'The phrase that will trigger the model to generate an image.',
      }),
    )
    .default(''),
  learning_rate: z
    .optional(
      z.number().gte(0.000001).lte(1).register(z.globalRegistry, {
        description:
          'The rate at which the model learns. Higher values can lead to faster training, but over-fitting.',
      }),
    )
    .default(0.0002),
  auto_scale_input: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the input will be automatically scale the video to 81 frames at 16fps.',
      }),
    )
    .default(false),
})

/**
 * File
 */
export const zFalAiWanTrainerFlf2V720pFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * Output
 */
export const zWanTrainerFlf2V720pOutput = z.object({
  lora_file: zFalAiWanTrainerFlf2V720pFile,
  config_file: zFalAiWanTrainerFlf2V720pFile,
})

/**
 * Validation
 */
export const zFalAiLtxVideoTrainerValidation = z.object({
  prompt: z.string().register(z.globalRegistry, {
    description: 'The prompt to use for validation.',
  }),
  image_url: z.optional(z.union([z.string(), z.unknown()])),
})

/**
 * Input
 */
export const zLtxVideoTrainerInput = z.object({
  number_of_steps: z
    .optional(
      z.int().gte(100).lte(20000).register(z.globalRegistry, {
        description: 'The number of steps to train for.',
      }),
    )
    .default(1000),
  frame_rate: z
    .optional(
      z.int().gte(8).lte(60).register(z.globalRegistry, {
        description: 'The target frames per second for the video.',
      }),
    )
    .default(25),
  learning_rate: z
    .optional(
      z.number().gte(0.000001).lte(1).register(z.globalRegistry, {
        description:
          'The rate at which the model learns. Higher values can lead to faster training, but over-fitting.',
      }),
    )
    .default(0.0002),
  validation: z
    .optional(
      z
        .array(zFalAiLtxVideoTrainerValidation)
        .max(2)
        .register(z.globalRegistry, {
          description:
            'A list of validation prompts to use during training. When providing an image, _all_ validation inputs must have an image.',
        }),
    )
    .default([]),
  number_of_frames: z
    .optional(
      z.int().gte(25).lte(121).register(z.globalRegistry, {
        description:
          'The number of frames to use for training. This is the number of frames per second multiplied by the number of seconds.',
      }),
    )
    .default(81),
  validation_reverse: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the validation videos will be reversed. This is useful for effects that are learned in reverse and then applied in reverse.',
      }),
    )
    .default(false),
  training_data_url: z.string().register(z.globalRegistry, {
    description:
      'URL to zip archive with videos or images. Try to use at least 10 files, although more is better.\n\n        **Supported video formats:** .mp4, .mov, .avi, .mkv\n        **Supported image formats:** .png, .jpg, .jpeg\n\n        Note: The dataset must contain ONLY videos OR ONLY images - mixed datasets are not supported.\n\n        The archive can also contain text files with captions. Each text file should have the same name as the media file it corresponds to.',
  }),
  split_input_duration_threshold: z
    .optional(
      z.number().gte(1).lte(60).register(z.globalRegistry, {
        description:
          'The duration threshold in seconds. If a video is longer than this, it will be split into scenes. If you provide captions for a split video, the caption will be applied to each scene. If you do not provide captions, scenes will be auto-captioned.',
      }),
    )
    .default(30),
  rank: z.optional(
    z
      .union([
        z.literal(8),
        z.literal(16),
        z.literal(32),
        z.literal(64),
        z.literal(128),
      ])
      .register(z.globalRegistry, {
        description: 'The rank of the LoRA.',
      }),
  ),
  aspect_ratio: z.optional(
    z.enum(['16:9', '1:1', '9:16']).register(z.globalRegistry, {
      description:
        'The aspect ratio to use for training. This is the aspect ratio of the video.',
    }),
  ),
  trigger_phrase: z
    .optional(
      z.string().register(z.globalRegistry, {
        description:
          'The phrase that will trigger the model to generate an image.',
      }),
    )
    .default(''),
  resolution: z.optional(
    z.enum(['low', 'medium', 'high']).register(z.globalRegistry, {
      description:
        'The resolution to use for training. This is the resolution of the video.',
    }),
  ),
  split_input_into_scenes: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, videos above a certain duration threshold will be split into scenes. If you provide captions for a split video, the caption will be applied to each scene. If you do not provide captions, scenes will be auto-captioned. This option has no effect on image datasets.',
      }),
    )
    .default(true),
  validation_resolution: z.optional(
    z.enum(['low', 'medium', 'high']).register(z.globalRegistry, {
      description: 'The resolution to use for validation.',
    }),
  ),
  validation_number_of_frames: z
    .optional(
      z.int().gte(8).lte(121).register(z.globalRegistry, {
        description: 'The number of frames to use for validation.',
      }),
    )
    .default(81),
  validation_aspect_ratio: z.optional(
    z.enum(['16:9', '1:1', '9:16']).register(z.globalRegistry, {
      description: 'The aspect ratio to use for validation.',
    }),
  ),
  validation_negative_prompt: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'A negative prompt to use for validation.',
      }),
    )
    .default('blurry, low quality, bad quality, out of focus'),
  auto_scale_input: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, videos will be automatically scaled to the target frame count and fps. This option has no effect on image datasets.',
      }),
    )
    .default(false),
})

/**
 * File
 */
export const zFalAiLtxVideoTrainerFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * TrainingOutput
 */
export const zLtxVideoTrainerOutput = z.object({
  lora_file: zFalAiLtxVideoTrainerFile,
  config_file: zFalAiLtxVideoTrainerFile,
  video: z.union([zFalAiLtxVideoTrainerFile, z.unknown()]),
})

/**
 * StyleReferenceInput
 */
export const zRecraftV3CreateStyleInput = z.object({
  images_data_url: z.string().register(z.globalRegistry, {
    description:
      'URL to zip archive with images, use PNG format. Maximum 5 images are allowed.',
  }),
  base_style: z.optional(
    z
      .enum([
        'any',
        'realistic_image',
        'digital_illustration',
        'vector_illustration',
        'realistic_image/b_and_w',
        'realistic_image/hard_flash',
        'realistic_image/hdr',
        'realistic_image/natural_light',
        'realistic_image/studio_portrait',
        'realistic_image/enterprise',
        'realistic_image/motion_blur',
        'realistic_image/evening_light',
        'realistic_image/faded_nostalgia',
        'realistic_image/forest_life',
        'realistic_image/mystic_naturalism',
        'realistic_image/natural_tones',
        'realistic_image/organic_calm',
        'realistic_image/real_life_glow',
        'realistic_image/retro_realism',
        'realistic_image/retro_snapshot',
        'realistic_image/urban_drama',
        'realistic_image/village_realism',
        'realistic_image/warm_folk',
        'digital_illustration/pixel_art',
        'digital_illustration/hand_drawn',
        'digital_illustration/grain',
        'digital_illustration/infantile_sketch',
        'digital_illustration/2d_art_poster',
        'digital_illustration/handmade_3d',
        'digital_illustration/hand_drawn_outline',
        'digital_illustration/engraving_color',
        'digital_illustration/2d_art_poster_2',
        'digital_illustration/antiquarian',
        'digital_illustration/bold_fantasy',
        'digital_illustration/child_book',
        'digital_illustration/child_books',
        'digital_illustration/cover',
        'digital_illustration/crosshatch',
        'digital_illustration/digital_engraving',
        'digital_illustration/expressionism',
        'digital_illustration/freehand_details',
        'digital_illustration/grain_20',
        'digital_illustration/graphic_intensity',
        'digital_illustration/hard_comics',
        'digital_illustration/long_shadow',
        'digital_illustration/modern_folk',
        'digital_illustration/multicolor',
        'digital_illustration/neon_calm',
        'digital_illustration/noir',
        'digital_illustration/nostalgic_pastel',
        'digital_illustration/outline_details',
        'digital_illustration/pastel_gradient',
        'digital_illustration/pastel_sketch',
        'digital_illustration/pop_art',
        'digital_illustration/pop_renaissance',
        'digital_illustration/street_art',
        'digital_illustration/tablet_sketch',
        'digital_illustration/urban_glow',
        'digital_illustration/urban_sketching',
        'digital_illustration/vanilla_dreams',
        'digital_illustration/young_adult_book',
        'digital_illustration/young_adult_book_2',
        'vector_illustration/bold_stroke',
        'vector_illustration/chemistry',
        'vector_illustration/colored_stencil',
        'vector_illustration/contour_pop_art',
        'vector_illustration/cosmics',
        'vector_illustration/cutout',
        'vector_illustration/depressive',
        'vector_illustration/editorial',
        'vector_illustration/emotional_flat',
        'vector_illustration/infographical',
        'vector_illustration/marker_outline',
        'vector_illustration/mosaic',
        'vector_illustration/naivector',
        'vector_illustration/roundish_flat',
        'vector_illustration/segmented_colors',
        'vector_illustration/sharp_contrast',
        'vector_illustration/thin',
        'vector_illustration/vector_photo',
        'vector_illustration/vivid_shapes',
        'vector_illustration/engraving',
        'vector_illustration/line_art',
        'vector_illustration/line_circuit',
        'vector_illustration/linocut',
      ])
      .register(z.globalRegistry, {
        description:
          'The base style of the generated images, this topic is covered above.',
      }),
  ),
})

/**
 * StyleReferenceOutput
 */
export const zRecraftV3CreateStyleOutput = z.object({
  style_id: z.string().register(z.globalRegistry, {
    description:
      'The ID of the created style, this ID can be used to reference the style in the future.',
  }),
})

/**
 * Input
 */
export const zTurboFluxTrainerInput = z.object({
  images_data_url: z.string().register(z.globalRegistry, {
    description:
      '\n        URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.\n        ',
  }),
  trigger_phrase: z
    .optional(
      z.string().register(z.globalRegistry, {
        description:
          'Trigger phrase to be used in the captions. If None, a trigger word will not be used.\n        If no captions are provide the trigger_work will be used instead of captions. If captions are provided, the trigger word will replace the `[trigger]` string in the captions.\n        ',
      }),
    )
    .default('ohwx'),
  steps: z
    .optional(
      z.int().gte(1).lte(10000).register(z.globalRegistry, {
        description: 'Number of steps to train the LoRA on.',
      }),
    )
    .default(1000),
  learning_rate: z
    .optional(
      z.number().gte(1e-7).lte(0.01).register(z.globalRegistry, {
        description: 'Learning rate for the training.',
      }),
    )
    .default(0.00115),
  training_style: z.optional(
    z.enum(['subject', 'style']).register(z.globalRegistry, {
      description: 'Training style to use.',
    }),
  ),
  face_crop: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'Whether to try to detect the face and crop the images to the face.',
      }),
    )
    .default(true),
})

/**
 * File
 */
export const zFalAiTurboFluxTrainerFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * Output
 */
export const zTurboFluxTrainerOutput = z.object({
  config_file: zFalAiTurboFluxTrainerFile,
  diffusers_lora_file: zFalAiTurboFluxTrainerFile,
})

/**
 * Input
 */
export const zWanTrainerInput = z.object({
  number_of_steps: z
    .optional(
      z.int().gte(1).lte(20000).register(z.globalRegistry, {
        description: 'The number of steps to train for.',
      }),
    )
    .default(400),
  training_data_url: z.string().register(z.globalRegistry, {
    description:
      'URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.\n\n        In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.',
  }),
  trigger_phrase: z
    .optional(
      z.string().register(z.globalRegistry, {
        description:
          'The phrase that will trigger the model to generate an image.',
      }),
    )
    .default(''),
  learning_rate: z
    .optional(
      z.number().gte(0.000001).lte(1).register(z.globalRegistry, {
        description:
          'The rate at which the model learns. Higher values can lead to faster training, but over-fitting.',
      }),
    )
    .default(0.0002),
  auto_scale_input: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description:
          'If true, the input will be automatically scale the video to 81 frames at 16fps.',
      }),
    )
    .default(false),
})

/**
 * File
 */
export const zFalAiWanTrainerFile = z.object({
  file_size: z.optional(
    z.int().register(z.globalRegistry, {
      description: 'The size of the file in bytes.',
    }),
  ),
  file_name: z.optional(
    z.string().register(z.globalRegistry, {
      description:
        'The name of the file. It will be auto-generated if not provided.',
    }),
  ),
  content_type: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'The mime type of the file.',
    }),
  ),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
  file_data: z.optional(
    z.string().register(z.globalRegistry, {
      description: 'File data',
    }),
  ),
})

/**
 * Output
 */
export const zWanTrainerOutput = z.object({
  lora_file: zFalAiWanTrainerFile,
  config_file: zFalAiWanTrainerFile,
})

/**
 * PublicInput
 */
export const zHunyuanVideoLoraTrainingInput = z.object({
  trigger_word: z
    .optional(
      z.string().register(z.globalRegistry, {
        description: 'The trigger word to use.',
      }),
    )
    .default(''),
  images_data_url: z.string().register(z.globalRegistry, {
    description:
      '\n        URL to zip archive with images. Try to use at least 4 images in general the more the better.\n\n        In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.\n    ',
  }),
  steps: z.int().gte(1).lte(5000).register(z.globalRegistry, {
    description: 'Number of steps to train the LoRA on.',
  }),
  data_archive_format: z.optional(z.union([z.string(), z.unknown(), z.null()])),
  learning_rate: z
    .optional(
      z.number().register(z.globalRegistry, {
        description: 'Learning rate to use for training.',
      }),
    )
    .default(0.0001),
  do_caption: z
    .optional(
      z.boolean().register(z.globalRegistry, {
        description: 'Whether to generate captions for the images.',
      }),
    )
    .default(true),
})

/**
 * File
 */
export const zFalAiHunyuanVideoLoraTrainingFile = z.object({
  file_size: z.optional(z.union([z.int(), z.unknown()])),
  file_name: z.optional(z.union([z.string(), z.unknown()])),
  content_type: z.optional(z.union([z.string(), z.unknown()])),
  url: z.string().register(z.globalRegistry, {
    description: 'The URL where the file can be downloaded from.',
  }),
})

/**
 * Output
 */
export const zHunyuanVideoLoraTrainingOutput = z.object({
  config_file: zFalAiHunyuanVideoLoraTrainingFile,
  diffusers_lora_file: zFalAiHunyuanVideoLoraTrainingFile,
})
