// This file is auto-generated by @hey-api/openapi-ts

export type ClientOptions = {
    baseUrl: `${string}://${string}` | (string & {});
};

export type File = {
    url: string;
    content_type?: string;
    file_name?: string;
    file_size?: number;
};

export type QueueStatus = {
    status: 'IN_PROGRESS' | 'COMPLETED' | 'FAILED';
    response_url?: string;
};

/**
 * PublicInput
 */
export type FluxKreaTrainerInput = {
    /**
     * Images Data Url
     *
     *
     * URL to zip archive with images. Try to use at least 4 images in general the more the better.
     *
     * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.
     *
     */
    images_data_url: string;
    /**
     * Is Input Format Already Preprocessed
     *
     * Specifies whether the input data is already in a processed format. When set to False (default), the system expects raw input where image files and their corresponding caption files share the same name (e.g., 'photo.jpg' and 'photo.txt'). Set to True if your data is already in a preprocessed format.
     */
    is_input_format_already_preprocessed?: boolean;
    /**
     * Trigger Word
     *
     * Trigger word to be used in the captions. If None, a trigger word will not be used.
     * If no captions are provide the trigger_word will be used instead of captions. If captions are the trigger word will not be used.
     *
     */
    trigger_word?: string | null;
    /**
     * Steps
     *
     * Number of steps to train the LoRA on.
     */
    steps?: number;
    /**
     * Data Archive Format
     *
     * The format of the archive. If not specified, the format will be inferred from the URL.
     */
    data_archive_format?: string | null;
    /**
     * Is Style
     *
     * If True, the training will be for a style. This will deactivate segmentation, captioning and will use trigger word instead. Use the trigger word to specify the style.
     */
    is_style?: boolean;
    /**
     * Create Masks
     *
     * If True segmentation masks will be used in the weight the training loss. For people a face mask is used if possible.
     */
    create_masks?: boolean;
};

/**
 * Output
 */
export type FluxKreaTrainerOutput = {
    /**
     * Config File
     *
     * URL to the training configuration file.
     */
    config_file: FalAiFluxKreaTrainerFile;
    /**
     * Debug Preprocessed Output
     *
     * URL to the preprocessed images.
     */
    debug_preprocessed_output?: FalAiFluxKreaTrainerFile;
    /**
     * Diffusers Lora File
     *
     * URL to the trained diffusers lora weights.
     */
    diffusers_lora_file: FalAiFluxKreaTrainerFile;
};

/**
 * File
 */
export type FalAiFluxKreaTrainerFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
    /**
     * File Data
     *
     * File data
     */
    file_data?: Blob | File;
};

/**
 * PublicInput
 */
export type FluxLoraFastTrainingInput = {
    /**
     * Images Data Url
     *
     *
     * URL to zip archive with images. Try to use at least 4 images in general the more the better.
     *
     * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.
     *
     */
    images_data_url: string;
    /**
     * Is Input Format Already Preprocessed
     *
     * Specifies whether the input data is already in a processed format. When set to False (default), the system expects raw input where image files and their corresponding caption files share the same name (e.g., 'photo.jpg' and 'photo.txt'). Set to True if your data is already in a preprocessed format.
     */
    is_input_format_already_preprocessed?: boolean;
    /**
     * Trigger Word
     *
     * Trigger word to be used in the captions. If None, a trigger word will not be used.
     * If no captions are provide the trigger_word will be used instead of captions. If captions are the trigger word will not be used.
     *
     */
    trigger_word?: string | null;
    /**
     * Steps
     *
     * Number of steps to train the LoRA on.
     */
    steps?: number;
    /**
     * Data Archive Format
     *
     * The format of the archive. If not specified, the format will be inferred from the URL.
     */
    data_archive_format?: string | null;
    /**
     * Is Style
     *
     * If True, the training will be for a style. This will deactivate segmentation, captioning and will use trigger word instead. Use the trigger word to specify the style.
     */
    is_style?: boolean;
    /**
     * Create Masks
     *
     * If True segmentation masks will be used in the weight the training loss. For people a face mask is used if possible.
     */
    create_masks?: boolean;
};

/**
 * Output
 */
export type FluxLoraFastTrainingOutput = {
    /**
     * Config File
     *
     * URL to the training configuration file.
     */
    config_file: FalAiFluxLoraFastTrainingFile;
    /**
     * Debug Preprocessed Output
     *
     * URL to the preprocessed images.
     */
    debug_preprocessed_output?: FalAiFluxLoraFastTrainingFile;
    /**
     * Diffusers Lora File
     *
     * URL to the trained diffusers lora weights.
     */
    diffusers_lora_file: FalAiFluxLoraFastTrainingFile;
};

/**
 * File
 */
export type FalAiFluxLoraFastTrainingFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
    /**
     * File Data
     *
     * File data
     */
    file_data?: Blob | File;
};

/**
 * PublicInput
 */
export type FluxLoraPortraitTrainerInput = {
    /**
     * Images Data Url
     *
     *
     * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
     *
     * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.
     *
     * The captions can include a special string `[trigger]`. If a trigger_word is specified, it will replace `[trigger]` in the captions.
     *
     */
    images_data_url: string;
    /**
     * Trigger Phrase
     *
     * Trigger phrase to be used in the captions. If None, a trigger word will not be used.
     * If no captions are provide the trigger_work will be used instead of captions. If captions are provided, the trigger word will replace the `[trigger]` string in the captions.
     *
     */
    trigger_phrase?: string | null;
    /**
     * Resume From Checkpoint
     *
     * URL to a checkpoint to resume training from.
     */
    resume_from_checkpoint?: string;
    /**
     * Subject Crop
     *
     * If True, the subject will be cropped from the image.
     */
    subject_crop?: boolean;
    /**
     * Learning Rate
     *
     * Learning rate to use for training.
     */
    learning_rate?: number;
    /**
     * Multiresolution Training
     *
     * If True, multiresolution training will be used.
     */
    multiresolution_training?: boolean;
    /**
     * Steps
     *
     * Number of steps to train the LoRA on.
     */
    steps?: number;
    /**
     * Data Archive Format
     *
     * The format of the archive. If not specified, the format will be inferred from the URL.
     */
    data_archive_format?: string | null;
    /**
     * Create Masks
     *
     * If True, masks will be created for the subject.
     */
    create_masks?: boolean;
};

/**
 * Output
 */
export type FluxLoraPortraitTrainerOutput = {
    /**
     * Config File
     *
     * URL to the training configuration file.
     */
    config_file: FalAiFluxLoraPortraitTrainerFile;
    /**
     * Diffusers Lora File
     *
     * URL to the trained diffusers lora weights.
     */
    diffusers_lora_file: FalAiFluxLoraPortraitTrainerFile;
};

/**
 * File
 */
export type FalAiFluxLoraPortraitTrainerFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
    /**
     * File Data
     *
     * File data
     */
    file_data?: Blob | File;
};

/**
 * InputEditV2
 */
export type Flux2Klein9bBaseTrainerEditInput = {
    /**
     * Steps
     *
     * Total number of training steps.
     */
    steps?: number;
    /**
     * Image Data Url
     *
     *
     * URL to the input data zip archive.
     *
     * The zip should contain pairs of images. The images should be named:
     *
     * ROOT_start.EXT and ROOT_end.EXT
     * For example:
     * photo_start.jpg and photo_end.jpg
     *
     * The zip can also contain up to four reference image for each image pair. The reference images should be named:
     * ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT
     * For example:
     * photo_start.jpg, photo_start2.jpg, photo_end.jpg
     *
     * The zip can also contain a text file for each image pair. The text file should be named:
     * ROOT.txt
     * For example:
     * photo.txt
     *
     * This text file can be used to specify the edit instructions for the image pair.
     *
     * If no text file is provided, the default_caption will be used.
     *
     * If no default_caption is provided, the training will fail.
     *
     */
    image_data_url: string;
    /**
     * Learning Rate
     *
     * Learning rate applied to trainable parameters.
     */
    learning_rate?: number;
    /**
     * Default Caption
     *
     * Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string | unknown;
    /**
     * Output Lora Format
     *
     * Dictates the naming scheme for the output weights
     */
    output_lora_format?: 'fal' | 'comfy';
};

/**
 * Output
 */
export type Flux2Klein9bBaseTrainerEditOutput = {
    config_file: FalAiFlux2Klein9bBaseTrainerEditFile;
    diffusers_lora_file: FalAiFlux2Klein9bBaseTrainerEditFile;
};

/**
 * File
 */
export type FalAiFlux2Klein9bBaseTrainerEditFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number | unknown;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string | unknown;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string | unknown;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
};

/**
 * InputT2IV2
 *
 * V2 input with multi-resolution bucketing.
 */
export type Flux2Klein9bBaseTrainerInput = {
    /**
     * Steps
     *
     * Total number of training steps.
     */
    steps?: number;
    /**
     * Image Data Url
     *
     *
     * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
     *
     * The zip can also contain a text file for each image. The text file should be named:
     * ROOT.txt
     * For example:
     * photo.txt
     *
     * This text file can be used to specify the edit instructions for the image pair.
     *
     * If no text file is provided, the default_caption will be used.
     *
     * If no default_caption is provided, the training will fail.
     *
     */
    image_data_url: string;
    /**
     * Learning Rate
     *
     * Learning rate applied to trainable parameters.
     */
    learning_rate?: number;
    /**
     * Default Caption
     *
     * Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string | unknown;
    /**
     * Output Lora Format
     *
     * Dictates the naming scheme for the output weights
     */
    output_lora_format?: 'fal' | 'comfy';
};

/**
 * Output
 */
export type Flux2Klein9bBaseTrainerOutput = {
    config_file: FalAiFlux2Klein9bBaseTrainerFile;
    diffusers_lora_file: FalAiFlux2Klein9bBaseTrainerFile;
};

/**
 * File
 */
export type FalAiFlux2Klein9bBaseTrainerFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number | unknown;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string | unknown;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string | unknown;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
};

/**
 * InputT2IV2
 *
 * V2 input with multi-resolution bucketing.
 */
export type Flux2Klein4bBaseTrainerInput = {
    /**
     * Steps
     *
     * Total number of training steps.
     */
    steps?: number;
    /**
     * Image Data Url
     *
     *
     * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
     *
     * The zip can also contain a text file for each image. The text file should be named:
     * ROOT.txt
     * For example:
     * photo.txt
     *
     * This text file can be used to specify the edit instructions for the image pair.
     *
     * If no text file is provided, the default_caption will be used.
     *
     * If no default_caption is provided, the training will fail.
     *
     */
    image_data_url: string;
    /**
     * Learning Rate
     *
     * Learning rate applied to trainable parameters.
     */
    learning_rate?: number;
    /**
     * Default Caption
     *
     * Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string | unknown;
    /**
     * Output Lora Format
     *
     * Dictates the naming scheme for the output weights
     */
    output_lora_format?: 'fal' | 'comfy';
};

/**
 * Output
 */
export type Flux2Klein4bBaseTrainerOutput = {
    config_file: FalAiFlux2Klein4bBaseTrainerFile;
    diffusers_lora_file: FalAiFlux2Klein4bBaseTrainerFile;
};

/**
 * File
 */
export type FalAiFlux2Klein4bBaseTrainerFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number | unknown;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string | unknown;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string | unknown;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
};

/**
 * InputEditV2
 */
export type Flux2Klein4bBaseTrainerEditInput = {
    /**
     * Steps
     *
     * Total number of training steps.
     */
    steps?: number;
    /**
     * Image Data Url
     *
     *
     * URL to the input data zip archive.
     *
     * The zip should contain pairs of images. The images should be named:
     *
     * ROOT_start.EXT and ROOT_end.EXT
     * For example:
     * photo_start.jpg and photo_end.jpg
     *
     * The zip can also contain up to four reference image for each image pair. The reference images should be named:
     * ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT
     * For example:
     * photo_start.jpg, photo_start2.jpg, photo_end.jpg
     *
     * The zip can also contain a text file for each image pair. The text file should be named:
     * ROOT.txt
     * For example:
     * photo.txt
     *
     * This text file can be used to specify the edit instructions for the image pair.
     *
     * If no text file is provided, the default_caption will be used.
     *
     * If no default_caption is provided, the training will fail.
     *
     */
    image_data_url: string;
    /**
     * Learning Rate
     *
     * Learning rate applied to trainable parameters.
     */
    learning_rate?: number;
    /**
     * Default Caption
     *
     * Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string | unknown;
    /**
     * Output Lora Format
     *
     * Dictates the naming scheme for the output weights
     */
    output_lora_format?: 'fal' | 'comfy';
};

/**
 * Output
 */
export type Flux2Klein4bBaseTrainerEditOutput = {
    config_file: FalAiFlux2Klein4bBaseTrainerEditFile;
    diffusers_lora_file: FalAiFlux2Klein4bBaseTrainerEditFile;
};

/**
 * File
 */
export type FalAiFlux2Klein4bBaseTrainerEditFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number | unknown;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string | unknown;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string | unknown;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
};

/**
 * Input
 */
export type QwenImage2512TrainerV2Input = {
    /**
     * Steps
     *
     * Number of steps to train for
     */
    steps?: number;
    /**
     * Image Data Url
     *
     *
     * URL to the input data zip archive.
     *
     * The zip should contain pairs of images and corresponding captions.
     *
     * The images should be named: ROOT.EXT. For example: 001.jpg
     *
     * The corresponding captions should be named: ROOT.txt. For example: 001.txt
     *
     * If no text file is provided for an image, the default_caption will be used.
     *
     */
    image_data_url: string;
    /**
     * Learning Rate
     *
     * Learning rate.
     */
    learning_rate?: number;
    /**
     * Default Caption
     *
     * Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string | unknown;
};

/**
 * Output
 */
export type QwenImage2512TrainerV2Output = {
    config_file: FalAiQwenImage2512TrainerV2File;
    diffusers_lora_file: FalAiQwenImage2512TrainerV2File;
};

/**
 * File
 */
export type FalAiQwenImage2512TrainerV2File = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number | unknown;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string | unknown;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string | unknown;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
};

/**
 * InputEditV2
 */
export type Flux2TrainerV2EditInput = {
    /**
     * Steps
     *
     * Total number of training steps.
     */
    steps?: number;
    /**
     * Image Data Url
     *
     *
     * URL to the input data zip archive.
     *
     * The zip should contain pairs of images. The images should be named:
     *
     * ROOT_start.EXT and ROOT_end.EXT
     * For example:
     * photo_start.jpg and photo_end.jpg
     *
     * The zip can also contain up to four reference image for each image pair. The reference images should be named:
     * ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT
     * For example:
     * photo_start.jpg, photo_start2.jpg, photo_end.jpg
     *
     * The zip can also contain a text file for each image pair. The text file should be named:
     * ROOT.txt
     * For example:
     * photo.txt
     *
     * This text file can be used to specify the edit instructions for the image pair.
     *
     * If no text file is provided, the default_caption will be used.
     *
     * If no default_caption is provided, the training will fail.
     *
     */
    image_data_url: string;
    /**
     * Learning Rate
     *
     * Learning rate applied to trainable parameters.
     */
    learning_rate?: number;
    /**
     * Default Caption
     *
     * Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string | unknown;
    /**
     * Output Lora Format
     *
     * Dictates the naming scheme for the output weights
     */
    output_lora_format?: 'fal' | 'comfy';
};

/**
 * Output
 */
export type Flux2TrainerV2EditOutput = {
    config_file: FalAiFlux2TrainerV2EditFile;
    diffusers_lora_file: FalAiFlux2TrainerV2EditFile;
};

/**
 * File
 */
export type FalAiFlux2TrainerV2EditFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number | unknown;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string | unknown;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string | unknown;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
};

/**
 * InputT2IV2
 *
 * V2 input with multi-resolution bucketing.
 */
export type Flux2TrainerV2Input = {
    /**
     * Steps
     *
     * Total number of training steps.
     */
    steps?: number;
    /**
     * Image Data Url
     *
     *
     * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
     *
     * The zip can also contain a text file for each image. The text file should be named:
     * ROOT.txt
     * For example:
     * photo.txt
     *
     * This text file can be used to specify the edit instructions for the image pair.
     *
     * If no text file is provided, the default_caption will be used.
     *
     * If no default_caption is provided, the training will fail.
     *
     */
    image_data_url: string;
    /**
     * Learning Rate
     *
     * Learning rate applied to trainable parameters.
     */
    learning_rate?: number;
    /**
     * Default Caption
     *
     * Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string | unknown;
    /**
     * Output Lora Format
     *
     * Dictates the naming scheme for the output weights
     */
    output_lora_format?: 'fal' | 'comfy';
};

/**
 * Output
 */
export type Flux2TrainerV2Output = {
    config_file: FalAiFlux2TrainerV2File;
    diffusers_lora_file: FalAiFlux2TrainerV2File;
};

/**
 * File
 */
export type FalAiFlux2TrainerV2File = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number | unknown;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string | unknown;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string | unknown;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
};

/**
 * LTX2V2VInput
 *
 * Input configuration for LTX-2 video-to-video (IC-LoRA) training.
 */
export type Ltx2V2vTrainerInput = {
    /**
     * Number Of Steps
     *
     * The number of training steps.
     */
    number_of_steps?: number;
    /**
     * Frame Rate
     *
     * Target frames per second for the video.
     */
    frame_rate?: number;
    /**
     * Learning Rate
     *
     * Learning rate for optimization. Higher values can lead to faster training but may cause overfitting.
     */
    learning_rate?: number;
    /**
     * Validation
     *
     * A list of validation inputs with prompts and reference videos.
     */
    validation?: Array<V2vValidation>;
    /**
     * Number Of Frames
     *
     * Number of frames per training sample. Must satisfy frames % 8 == 1 (e.g., 1, 9, 17, 25, 33, 41, 49, 57, 65, 73, 81, 89, 97).
     */
    number_of_frames?: number;
    /**
     * Training Data Url
     *
     * URL to zip archive with videos or images. Try to use at least 10 files, although more is better.
     *
     * **Supported video formats:** .mp4, .mov, .avi, .mkv
     * **Supported image formats:** .png, .jpg, .jpeg
     *
     * Note: The dataset must contain ONLY videos OR ONLY images - mixed datasets are not supported.
     *
     * The archive can also contain text files with captions. Each text file should have the same name as the media file it corresponds to.
     */
    training_data_url: string;
    /**
     * Split Input Duration Threshold
     *
     * The duration threshold in seconds. If a video is longer than this, it will be split into scenes.
     */
    split_input_duration_threshold?: number;
    /**
     * Rank
     *
     * The rank of the LoRA adaptation. Higher values increase capacity but use more memory.
     */
    rank?: 8 | 16 | 32 | 64 | 128;
    /**
     * First Frame Conditioning P
     *
     * Probability of conditioning on the first frame during training. Lower values work better for video-to-video transformation.
     */
    first_frame_conditioning_p?: number;
    /**
     * Stg Scale
     *
     * STG (Spatio-Temporal Guidance) scale. 0.0 disables STG. Recommended value is 1.0.
     */
    stg_scale?: number;
    /**
     * Resolution
     *
     * Resolution to use for training. Higher resolutions require more memory.
     */
    resolution?: 'low' | 'medium' | 'high';
    /**
     * Split Input Into Scenes
     *
     * If true, videos above a certain duration threshold will be split into scenes.
     */
    split_input_into_scenes?: boolean;
    /**
     * Trigger Phrase
     *
     * A phrase that will trigger the LoRA style. Will be prepended to captions during training.
     */
    trigger_phrase?: string;
    /**
     * Validation Frame Rate
     *
     * Target frames per second for validation videos.
     */
    validation_frame_rate?: number;
    /**
     * Aspect Ratio
     *
     * Aspect ratio to use for training.
     */
    aspect_ratio?: '16:9' | '1:1' | '9:16';
    /**
     * Validation Resolution
     *
     * The resolution to use for validation.
     */
    validation_resolution?: 'low' | 'medium' | 'high';
    /**
     * Validation Number Of Frames
     *
     * The number of frames in validation videos.
     */
    validation_number_of_frames?: number;
    /**
     * Validation Aspect Ratio
     *
     * The aspect ratio to use for validation.
     */
    validation_aspect_ratio?: '16:9' | '1:1' | '9:16';
    /**
     * Validation Negative Prompt
     *
     * A negative prompt to use for validation.
     */
    validation_negative_prompt?: string;
    /**
     * Auto Scale Input
     *
     * If true, videos will be automatically scaled to the target frame count and fps. This option has no effect on image datasets.
     */
    auto_scale_input?: boolean;
};

/**
 * LTX2V2VOutput
 *
 * Output from LTX-2 video-to-video training.
 */
export type Ltx2V2vTrainerOutput = {
    lora_file: FalAiLtx2V2vTrainerFile;
    config_file: FalAiLtx2V2vTrainerFile;
    /**
     * URL to the debug dataset archive containing decoded videos.
     */
    debug_dataset?: FalAiLtx2V2vTrainerFile | unknown;
    /**
     * The URL to the validation videos (with reference videos side-by-side), if any.
     */
    video: FalAiLtx2V2vTrainerFile | unknown;
};

/**
 * V2VValidation
 *
 * Validation input for video-to-video training.
 */
export type V2vValidation = {
    /**
     * Prompt
     *
     * The prompt to use for validation.
     */
    prompt: string;
    /**
     * Reference Video Url
     *
     * URL to reference video for IC-LoRA validation. This is the input video that will be transformed.
     */
    reference_video_url: string;
};

/**
 * File
 */
export type FalAiLtx2V2vTrainerFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number | unknown;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string | unknown;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string | unknown;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
};

/**
 * LTX2Input
 *
 * Input configuration for LTX-2 text-to-video training.
 */
export type Ltx2VideoTrainerInput = {
    /**
     * Audio Normalize
     *
     * Normalize audio peak amplitude to a consistent level. Recommended for consistent audio levels across the dataset.
     */
    audio_normalize?: boolean;
    /**
     * Audio Preserve Pitch
     *
     * When audio duration doesn't match video duration, stretch/compress audio without changing pitch. If disabled, audio is trimmed or padded with silence.
     */
    audio_preserve_pitch?: boolean;
    /**
     * Frame Rate
     *
     * Target frames per second for the video.
     */
    frame_rate?: number;
    /**
     * Number Of Steps
     *
     * The number of training steps.
     */
    number_of_steps?: number;
    /**
     * Learning Rate
     *
     * Learning rate for optimization. Higher values can lead to faster training but may cause overfitting.
     */
    learning_rate?: number;
    /**
     * Validation
     *
     * A list of validation prompts to use during training. When providing an image, _all_ validation inputs must have an image.
     */
    validation?: Array<Validation>;
    /**
     * Number Of Frames
     *
     * Number of frames per training sample. Must satisfy frames % 8 == 1 (e.g., 1, 9, 17, 25, 33, 41, 49, 57, 65, 73, 81, 89, 97).
     */
    number_of_frames?: number;
    /**
     * Training Data Url
     *
     * URL to zip archive with videos or images. Try to use at least 10 files, although more is better.
     *
     * **Supported video formats:** .mp4, .mov, .avi, .mkv
     * **Supported image formats:** .png, .jpg, .jpeg
     *
     * Note: The dataset must contain ONLY videos OR ONLY images - mixed datasets are not supported.
     *
     * The archive can also contain text files with captions. Each text file should have the same name as the media file it corresponds to.
     */
    training_data_url: string;
    /**
     * Split Input Duration Threshold
     *
     * The duration threshold in seconds. If a video is longer than this, it will be split into scenes.
     */
    split_input_duration_threshold?: number;
    /**
     * Rank
     *
     * The rank of the LoRA adaptation. Higher values increase capacity but use more memory.
     */
    rank?: 8 | 16 | 32 | 64 | 128;
    /**
     * First Frame Conditioning P
     *
     * Probability of conditioning on the first frame during training. Higher values improve image-to-video performance.
     */
    first_frame_conditioning_p?: number;
    /**
     * Stg Scale
     *
     * STG (Spatio-Temporal Guidance) scale. 0.0 disables STG. Recommended value is 1.0.
     */
    stg_scale?: number;
    /**
     * Aspect Ratio
     *
     * Aspect ratio to use for training.
     */
    aspect_ratio?: '16:9' | '1:1' | '9:16';
    /**
     * Trigger Phrase
     *
     * A phrase that will trigger the LoRA style. Will be prepended to captions during training.
     */
    trigger_phrase?: string;
    /**
     * Resolution
     *
     * Resolution to use for training. Higher resolutions require more memory.
     */
    resolution?: 'low' | 'medium' | 'high';
    /**
     * Validation Frame Rate
     *
     * Target frames per second for validation videos.
     */
    validation_frame_rate?: number;
    /**
     * Split Input Into Scenes
     *
     * If true, videos above a certain duration threshold will be split into scenes.
     */
    split_input_into_scenes?: boolean;
    /**
     * With Audio
     *
     * Enable joint audio-video training. If None (default), automatically detects whether input videos have audio. Set to True to force audio training, or False to disable.
     */
    with_audio?: boolean | unknown;
    /**
     * Generate Audio In Validation
     *
     * Whether to generate audio in validation samples.
     */
    generate_audio_in_validation?: boolean;
    /**
     * Validation Resolution
     *
     * The resolution to use for validation.
     */
    validation_resolution?: 'low' | 'medium' | 'high';
    /**
     * Validation Number Of Frames
     *
     * The number of frames in validation videos.
     */
    validation_number_of_frames?: number;
    /**
     * Validation Aspect Ratio
     *
     * The aspect ratio to use for validation.
     */
    validation_aspect_ratio?: '16:9' | '1:1' | '9:16';
    /**
     * Validation Negative Prompt
     *
     * A negative prompt to use for validation.
     */
    validation_negative_prompt?: string;
    /**
     * Auto Scale Input
     *
     * If true, videos will be automatically scaled to the target frame count and fps. This option has no effect on image datasets.
     */
    auto_scale_input?: boolean;
};

/**
 * LTX2Output
 *
 * Output from LTX-2 training.
 */
export type Ltx2VideoTrainerOutput = {
    lora_file: FalAiLtx2VideoTrainerFile;
    config_file: FalAiLtx2VideoTrainerFile;
    /**
     * URL to the debug dataset archive containing decoded videos and audio.
     */
    debug_dataset?: FalAiLtx2VideoTrainerFile | unknown;
    /**
     * The URL to the validation videos, if any.
     */
    video: FalAiLtx2VideoTrainerFile | unknown;
};

/**
 * Validation
 */
export type Validation = {
    /**
     * Prompt
     *
     * The prompt to use for validation.
     */
    prompt: string;
    /**
     * Image Url
     *
     * An image to use for image-to-video validation. If provided for one validation, _all_ validation inputs must have an image.
     */
    image_url?: string | unknown;
};

/**
 * File
 */
export type FalAiLtx2VideoTrainerFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number | unknown;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string | unknown;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string | unknown;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
};

/**
 * InputImage
 */
export type QwenImage2512TrainerInput = {
    /**
     * Steps
     *
     * Number of steps to train for
     */
    steps?: number;
    /**
     * Image Data Url
     *
     *
     * URL to the input data zip archive for text-to-image training.
     *
     * The zip should contain images with their corresponding text captions:
     *
     * image.EXT and image.txt
     * For example:
     * photo.jpg and photo.txt
     *
     * The text file contains the caption/prompt describing the target image.
     *
     * If no text file is provided for an image, the default_caption will be used.
     *
     * If no default_caption is provided and a text file is missing, the training will fail.
     *
     */
    image_data_url: string;
    /**
     * Learning Rate
     *
     * Learning rate for LoRA parameters.
     */
    learning_rate?: number;
    /**
     * Default Caption
     *
     * Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string | unknown;
};

/**
 * Output
 */
export type QwenImage2512TrainerOutput = {
    config_file: FalAiQwenImage2512TrainerFile;
    diffusers_lora_file: FalAiQwenImage2512TrainerFile;
};

/**
 * File
 */
export type FalAiQwenImage2512TrainerFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number | unknown;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string | unknown;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string | unknown;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
};

/**
 * Input2511
 */
export type QwenImageEdit2511TrainerInput = {
    /**
     * Steps
     *
     * Number of steps to train for
     */
    steps?: number;
    /**
     * Image Data Url
     *
     *
     * URL to the input data zip archive.
     *
     * The zip should contain pairs of images. The images should be named:
     *
     * ROOT_start.EXT and ROOT_end.EXT
     * For example:
     * photo_start.jpg and photo_end.jpg
     *
     * The zip can also contain more than one reference image for each image pair. The reference images should be named:
     * ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT
     * For example:
     * photo_start.jpg, photo_start2.jpg, photo_end.jpg
     *
     * The Reference Image Count field should be set to the number of reference images.
     *
     * The zip can also contain a text file for each image pair. The text file should be named:
     * ROOT.txt
     * For example:
     * photo.txt
     *
     * This text file can be used to specify the edit instructions for the image pair.
     *
     * If no text file is provided, the default_caption will be used.
     *
     * If no default_caption is provided, the training will fail.
     *
     */
    image_data_url: string;
    /**
     * Learning Rate
     *
     * Learning rate for LoRA parameters.
     */
    learning_rate?: number;
    /**
     * Default Caption
     *
     * Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string | unknown;
};

/**
 * Output
 */
export type QwenImageEdit2511TrainerOutput = {
    config_file: FalAiQwenImageEdit2511TrainerFile;
    diffusers_lora_file: FalAiQwenImageEdit2511TrainerFile;
};

/**
 * File
 */
export type FalAiQwenImageEdit2511TrainerFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number | unknown;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string | unknown;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string | unknown;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
};

/**
 * Input
 */
export type QwenImageLayeredTrainerInput = {
    /**
     * Steps
     *
     * Number of steps to train for
     */
    steps?: number;
    /**
     * Image Data Url
     *
     *
     * URL to the input data zip archive.
     *
     * The zip should contain groups of images. The images should be named:
     *
     * ROOT_start.EXT, ROOT_end.EXT, ROOT_end2.EXT, ..., ROOT_endN.EXT
     * For example:
     * photo_start.png, photo_end.png, photo_end2.png, ..., photo_endN.png
     *
     * The start image is the base image that will be decomposed into layers.
     * The end images are the layers that will be added to the base image.  ROOT_end.EXT is the first layer, ROOT_end2.EXT is the second layer, and so on.
     * You can have up to 8 layers.
     * All image groups must have the same number of output layers.
     *
     * The end images can contain transparent regions. Only PNG and WebP images are supported since these are the only formats that support transparency.
     *
     * The zip can also contain a text file for each image group. The text file should be named:
     * ROOT.txt
     * For example:
     * photo.txt
     *
     * This text file can be used to specify a description of the base image.
     *
     * If no text file is provided, the default_caption will be used.
     *
     * If no default_caption is provided, the training will fail.
     *
     */
    image_data_url: string;
    /**
     * Learning Rate
     *
     * Learning rate for LoRA parameters.
     */
    learning_rate?: number;
    /**
     * Default Caption
     *
     * Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string | unknown;
};

/**
 * Output
 */
export type QwenImageLayeredTrainerOutput = {
    config_file: FalAiQwenImageLayeredTrainerFile;
    diffusers_lora_file: FalAiQwenImageLayeredTrainerFile;
};

/**
 * File
 */
export type FalAiQwenImageLayeredTrainerFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number | unknown;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string | unknown;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string | unknown;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
};

/**
 * InputPlus
 */
export type QwenImageEdit2509TrainerInput = {
    /**
     * Steps
     *
     * Number of steps to train for
     */
    steps?: number;
    /**
     * Image Data Url
     *
     *
     * URL to the input data zip archive.
     *
     * The zip should contain pairs of images. The images should be named:
     *
     * ROOT_start.EXT and ROOT_end.EXT
     * For example:
     * photo_start.jpg and photo_end.jpg
     *
     * The zip can also contain more than one reference image for each image pair. The reference images should be named:
     * ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT
     * For example:
     * photo_start.jpg, photo_start2.jpg, photo_end.jpg
     *
     * The Reference Image Count field should be set to the number of reference images.
     *
     * The zip can also contain a text file for each image pair. The text file should be named:
     * ROOT.txt
     * For example:
     * photo.txt
     *
     * This text file can be used to specify the edit instructions for the image pair.
     *
     * If no text file is provided, the default_caption will be used.
     *
     * If no default_caption is provided, the training will fail.
     *
     */
    image_data_url: string;
    /**
     * Learning Rate
     *
     * Learning rate for LoRA parameters.
     */
    learning_rate?: number;
    /**
     * Default Caption
     *
     * Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string | unknown;
};

/**
 * Output
 */
export type QwenImageEdit2509TrainerOutput = {
    config_file: FalAiQwenImageEdit2509TrainerFile;
    diffusers_lora_file: FalAiQwenImageEdit2509TrainerFile;
};

/**
 * File
 */
export type FalAiQwenImageEdit2509TrainerFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number | unknown;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string | unknown;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string | unknown;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
};

/**
 * Input
 */
export type ZImageTrainerInput = {
    /**
     * Steps
     *
     * Total number of training steps.
     */
    steps?: number;
    /**
     * Image Data Url
     *
     *
     * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
     *
     * The zip can also contain a text file for each image. The text file should be named:
     * ROOT.txt
     * For example:
     * photo.txt
     *
     * This text file can be used to specify the edit instructions for the image pair.
     *
     * If no text file is provided, the default_caption will be used.
     *
     * If no default_caption is provided, the training will fail.
     *
     */
    image_data_url: string;
    /**
     * Training Type
     *
     * Type of training to perform. Use 'content' to focus on the content of the images, 'style' to focus on the style of the images, and 'balanced' to focus on a combination of both.
     */
    training_type?: 'content' | 'style' | 'balanced';
    /**
     * Learning Rate
     *
     * Learning rate applied to trainable parameters.
     */
    learning_rate?: number;
    /**
     * Default Caption
     *
     * Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string | unknown;
};

/**
 * Output
 */
export type ZImageTrainerOutput = {
    config_file: FalAiZImageTrainerFile;
    diffusers_lora_file: FalAiZImageTrainerFile;
};

/**
 * File
 */
export type FalAiZImageTrainerFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number | unknown;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string | unknown;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string | unknown;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
};

/**
 * InputEdit
 */
export type Flux2TrainerEditInput = {
    /**
     * Steps
     *
     * Total number of training steps.
     */
    steps?: number;
    /**
     * Image Data Url
     *
     *
     * URL to the input data zip archive.
     *
     * The zip should contain pairs of images. The images should be named:
     *
     * ROOT_start.EXT and ROOT_end.EXT
     * For example:
     * photo_start.jpg and photo_end.jpg
     *
     * The zip can also contain up to four reference image for each image pair. The reference images should be named:
     * ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT
     * For example:
     * photo_start.jpg, photo_start2.jpg, photo_end.jpg
     *
     * The zip can also contain a text file for each image pair. The text file should be named:
     * ROOT.txt
     * For example:
     * photo.txt
     *
     * This text file can be used to specify the edit instructions for the image pair.
     *
     * If no text file is provided, the default_caption will be used.
     *
     * If no default_caption is provided, the training will fail.
     *
     */
    image_data_url: string;
    /**
     * Learning Rate
     *
     * Learning rate applied to trainable parameters.
     */
    learning_rate?: number;
    /**
     * Default Caption
     *
     * Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string | unknown;
    /**
     * Output Lora Format
     *
     * Dictates the naming scheme for the output weights
     */
    output_lora_format?: 'fal' | 'comfy';
};

/**
 * Output
 */
export type Flux2TrainerEditOutput = {
    config_file: FalAiFlux2TrainerEditFile;
    diffusers_lora_file: FalAiFlux2TrainerEditFile;
};

/**
 * File
 */
export type FalAiFlux2TrainerEditFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number | unknown;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string | unknown;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string | unknown;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
};

/**
 * InputT2I
 */
export type Flux2TrainerInput = {
    /**
     * Steps
     *
     * Total number of training steps.
     */
    steps?: number;
    /**
     * Image Data Url
     *
     *
     * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
     *
     * The zip can also contain a text file for each image. The text file should be named:
     * ROOT.txt
     * For example:
     * photo.txt
     *
     * This text file can be used to specify the edit instructions for the image pair.
     *
     * If no text file is provided, the default_caption will be used.
     *
     * If no default_caption is provided, the training will fail.
     *
     */
    image_data_url: string;
    /**
     * Learning Rate
     *
     * Learning rate applied to trainable parameters.
     */
    learning_rate?: number;
    /**
     * Default Caption
     *
     * Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string | unknown;
    /**
     * Output Lora Format
     *
     * Dictates the naming scheme for the output weights
     */
    output_lora_format?: 'fal' | 'comfy';
};

/**
 * Output
 */
export type Flux2TrainerOutput = {
    config_file: FalAiFlux2TrainerFile;
    diffusers_lora_file: FalAiFlux2TrainerFile;
};

/**
 * File
 */
export type FalAiFlux2TrainerFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number | unknown;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string | unknown;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string | unknown;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
};

/**
 * InputPlus
 */
export type QwenImageEditPlusTrainerInput = {
    /**
     * Steps
     *
     * Number of steps to train for
     */
    steps?: number;
    /**
     * Image Data Url
     *
     *
     * URL to the input data zip archive.
     *
     * The zip should contain pairs of images. The images should be named:
     *
     * ROOT_start.EXT and ROOT_end.EXT
     * For example:
     * photo_start.jpg and photo_end.jpg
     *
     * The zip can also contain more than one reference image for each image pair. The reference images should be named:
     * ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT
     * For example:
     * photo_start.jpg, photo_start2.jpg, photo_end.jpg
     *
     * The Reference Image Count field should be set to the number of reference images.
     *
     * The zip can also contain a text file for each image pair. The text file should be named:
     * ROOT.txt
     * For example:
     * photo.txt
     *
     * This text file can be used to specify the edit instructions for the image pair.
     *
     * If no text file is provided, the default_caption will be used.
     *
     * If no default_caption is provided, the training will fail.
     *
     */
    image_data_url: string;
    /**
     * Learning Rate
     *
     * Learning rate for LoRA parameters.
     */
    learning_rate?: number;
    /**
     * Default Caption
     *
     * Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string | unknown;
};

/**
 * Output
 */
export type QwenImageEditPlusTrainerOutput = {
    config_file: FalAiQwenImageEditPlusTrainerFile;
    diffusers_lora_file: FalAiQwenImageEditPlusTrainerFile;
};

/**
 * File
 */
export type FalAiQwenImageEditPlusTrainerFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number | unknown;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string | unknown;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string | unknown;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
};

/**
 * InputEdit
 */
export type QwenImageEditTrainerInput = {
    /**
     * Steps
     *
     * Number of steps to train for
     */
    steps?: number;
    /**
     * Image Data Url
     *
     *
     * URL to the input data zip archive.
     *
     * The zip should contain pairs of images. The images should be named:
     *
     * ROOT_start.EXT and ROOT_end.EXT
     * For example:
     * photo_start.jpg and photo_end.jpg
     *
     * The zip can also contain a text file for each image pair. The text file should be named:
     * ROOT.txt
     * For example:
     * photo.txt
     *
     * This text file can be used to specify the edit instructions for the image pair.
     *
     * If no text file is provided, the default_caption will be used.
     *
     * If no default_caption is provided, the training will fail.
     *
     */
    image_data_url: string;
    /**
     * Learning Rate
     *
     * Learning rate for LoRA parameters.
     */
    learning_rate?: number;
    /**
     * Default Caption
     *
     * Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string | unknown;
};

/**
 * Output
 */
export type QwenImageEditTrainerOutput = {
    config_file: FalAiQwenImageEditTrainerFile;
    diffusers_lora_file: FalAiQwenImageEditTrainerFile;
};

/**
 * File
 */
export type FalAiQwenImageEditTrainerFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number | unknown;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string | unknown;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string | unknown;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
};

/**
 * PublicInput
 */
export type QwenImageTrainerInput = {
    /**
     * Steps
     *
     * Total number of training steps to perform. Default is 4000.
     */
    steps?: number;
    /**
     * Image Data Url
     *
     *
     * URL to zip archive with images for training. The archive should contain images and corresponding text files with captions.
     * Each text file should have the same name as the image file it corresponds to (e.g., image1.jpg and image1.txt).
     * If text files are missing for some images, you can provide a trigger_phrase to automatically create them.
     * Supported image formats: PNG, JPG, JPEG, WEBP.
     * Try to use at least 10 images, although more is better.
     *
     */
    image_data_url: string;
    /**
     * Learning Rate
     *
     * Learning rate for training. Default is 5e-4
     */
    learning_rate?: number;
    /**
     * Trigger Phrase
     *
     * Default caption to use for images that don't have corresponding text files. If provided, missing .txt files will be created automatically.
     */
    trigger_phrase?: string;
};

/**
 * Output
 */
export type QwenImageTrainerOutput = {
    /**
     * Lora File
     *
     * URL to the trained LoRA weights file.
     */
    lora_file: FalAiQwenImageTrainerFile;
    /**
     * Config File
     *
     * URL to the training configuration file.
     */
    config_file: FalAiQwenImageTrainerFile;
};

/**
 * File
 */
export type FalAiQwenImageTrainerFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
    /**
     * File Data
     *
     * File data
     */
    file_data?: Blob | File;
};

/**
 * BasicInput
 */
export type Wan22ImageTrainerInput = {
    /**
     * Trigger Phrase
     *
     * Trigger phrase for the model.
     */
    trigger_phrase: string;
    /**
     * Use Masks
     *
     * Whether to use masks for the training data.
     */
    use_masks?: boolean;
    /**
     * Learning Rate
     *
     * Learning rate for training.
     */
    learning_rate?: number;
    /**
     * Use Face Cropping
     *
     * Whether to use face cropping for the training data. When enabled, images will be cropped to the face before resizing.
     */
    use_face_cropping?: boolean;
    /**
     * Training Data URL
     *
     * URL to the training data.
     */
    training_data_url: string;
    /**
     * Number of Steps
     *
     * Number of training steps.
     */
    steps?: number;
    /**
     * Include Synthetic Captions
     *
     * Whether to include synthetic captions.
     */
    include_synthetic_captions?: boolean;
    /**
     * Is Style
     *
     * Whether the training data is style data. If true, face specific options like masking and face detection will be disabled.
     */
    is_style?: boolean;
    /**
     * Use Face Detection
     *
     * Whether to use face detection for the training data. When enabled, images will use the center of the face as the center of the image when resizing.
     */
    use_face_detection?: boolean;
};

/**
 * WanTrainerResponse
 */
export type Wan22ImageTrainerOutput = {
    /**
     * Config File
     *
     * Config file helping inference endpoints after training.
     */
    config_file: FalAiWan22ImageTrainerFile;
    /**
     * High Noise LoRA
     *
     * High noise LoRA file.
     */
    high_noise_lora: FalAiWan22ImageTrainerFile;
    /**
     * Low Noise LoRA
     *
     * Low noise LoRA file.
     */
    diffusers_lora_file: FalAiWan22ImageTrainerFile;
};

/**
 * File
 */
export type FalAiWan22ImageTrainerFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
    /**
     * File Data
     *
     * File data
     */
    file_data?: Blob | File;
};

/**
 * Input
 */
export type FluxKontextTrainerInput = {
    /**
     * Steps
     *
     * Number of steps to train for
     */
    steps?: number;
    /**
     * Image Data Url
     *
     *
     * URL to the input data zip archive.
     *
     * The zip should contain pairs of images. The images should be named:
     *
     * ROOT_start.EXT and ROOT_end.EXT
     * For example:
     * photo_start.jpg and photo_end.jpg
     *
     * The zip can also contain a text file for each image pair. The text file should be named:
     * ROOT.txt
     * For example:
     * photo.txt
     *
     * This text file can be used to specify the edit instructions for the image pair.
     *
     * If no text file is provided, the default_caption will be used.
     *
     * If no default_caption is provided, the training will fail.
     *
     */
    image_data_url: string;
    /**
     * Learning Rate
     */
    learning_rate?: number;
    /**
     * Default Caption
     *
     * Default caption to use when caption files are missing. If None, missing captions will cause an error.
     */
    default_caption?: string;
    /**
     * Output Lora Format
     *
     * Dictates the naming scheme for the output weights
     */
    output_lora_format?: 'fal' | 'comfy';
};

/**
 * Output
 */
export type FluxKontextTrainerOutput = {
    /**
     * Config File
     *
     * URL to the configuration file for the trained model.
     */
    config_file: FalAiFluxKontextTrainerFile;
    /**
     * Diffusers Lora File
     *
     * URL to the trained diffusers lora weights.
     */
    diffusers_lora_file: FalAiFluxKontextTrainerFile;
};

/**
 * File
 */
export type FalAiFluxKontextTrainerFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
    /**
     * File Data
     *
     * File data
     */
    file_data?: Blob | File;
};

/**
 * Input
 */
export type WanTrainerT2vInput = {
    /**
     * Number Of Steps
     *
     * The number of steps to train for.
     */
    number_of_steps?: number;
    /**
     * Training Data URL
     *
     * URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.
     *
     * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.
     */
    training_data_url: string;
    /**
     * Trigger Phrase
     *
     * The phrase that will trigger the model to generate an image.
     */
    trigger_phrase?: string;
    /**
     * Learning Rate
     *
     * The rate at which the model learns. Higher values can lead to faster training, but over-fitting.
     */
    learning_rate?: number;
    /**
     * Auto-Scale Input
     *
     * If true, the input will be automatically scale the video to 81 frames at 16fps.
     */
    auto_scale_input?: boolean;
};

/**
 * Output
 */
export type WanTrainerT2vOutput = {
    /**
     * Lora File
     *
     * URL to the trained LoRA weights.
     */
    lora_file: FalAiWanTrainerT2vFile;
    /**
     * Config File
     *
     * Configuration used for setting up the inference endpoints.
     */
    config_file: FalAiWanTrainerT2vFile;
};

/**
 * File
 */
export type FalAiWanTrainerT2vFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
    /**
     * File Data
     *
     * File data
     */
    file_data?: Blob | File;
};

/**
 * Input
 */
export type WanTrainerT2V14bInput = {
    /**
     * Number Of Steps
     *
     * The number of steps to train for.
     */
    number_of_steps?: number;
    /**
     * Training Data URL
     *
     * URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.
     *
     * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.
     */
    training_data_url: string;
    /**
     * Trigger Phrase
     *
     * The phrase that will trigger the model to generate an image.
     */
    trigger_phrase?: string;
    /**
     * Learning Rate
     *
     * The rate at which the model learns. Higher values can lead to faster training, but over-fitting.
     */
    learning_rate?: number;
    /**
     * Auto-Scale Input
     *
     * If true, the input will be automatically scale the video to 81 frames at 16fps.
     */
    auto_scale_input?: boolean;
};

/**
 * Output
 */
export type WanTrainerT2V14bOutput = {
    /**
     * Lora File
     *
     * URL to the trained LoRA weights.
     */
    lora_file: FalAiWanTrainerT2V14bFile;
    /**
     * Config File
     *
     * Configuration used for setting up the inference endpoints.
     */
    config_file: FalAiWanTrainerT2V14bFile;
};

/**
 * File
 */
export type FalAiWanTrainerT2V14bFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
    /**
     * File Data
     *
     * File data
     */
    file_data?: Blob | File;
};

/**
 * Input
 */
export type WanTrainerI2V720pInput = {
    /**
     * Number Of Steps
     *
     * The number of steps to train for.
     */
    number_of_steps?: number;
    /**
     * Training Data URL
     *
     * URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.
     *
     * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.
     */
    training_data_url: string;
    /**
     * Trigger Phrase
     *
     * The phrase that will trigger the model to generate an image.
     */
    trigger_phrase?: string;
    /**
     * Learning Rate
     *
     * The rate at which the model learns. Higher values can lead to faster training, but over-fitting.
     */
    learning_rate?: number;
    /**
     * Auto-Scale Input
     *
     * If true, the input will be automatically scale the video to 81 frames at 16fps.
     */
    auto_scale_input?: boolean;
};

/**
 * Output
 */
export type WanTrainerI2V720pOutput = {
    /**
     * Lora File
     *
     * URL to the trained LoRA weights.
     */
    lora_file: FalAiWanTrainerI2V720pFile;
    /**
     * Config File
     *
     * Configuration used for setting up the inference endpoints.
     */
    config_file: FalAiWanTrainerI2V720pFile;
};

/**
 * File
 */
export type FalAiWanTrainerI2V720pFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
    /**
     * File Data
     *
     * File data
     */
    file_data?: Blob | File;
};

/**
 * Input
 */
export type WanTrainerFlf2V720pInput = {
    /**
     * Number Of Steps
     *
     * The number of steps to train for.
     */
    number_of_steps?: number;
    /**
     * Training Data URL
     *
     * URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.
     *
     * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.
     */
    training_data_url: string;
    /**
     * Trigger Phrase
     *
     * The phrase that will trigger the model to generate an image.
     */
    trigger_phrase?: string;
    /**
     * Learning Rate
     *
     * The rate at which the model learns. Higher values can lead to faster training, but over-fitting.
     */
    learning_rate?: number;
    /**
     * Auto-Scale Input
     *
     * If true, the input will be automatically scale the video to 81 frames at 16fps.
     */
    auto_scale_input?: boolean;
};

/**
 * Output
 */
export type WanTrainerFlf2V720pOutput = {
    /**
     * Lora File
     *
     * URL to the trained LoRA weights.
     */
    lora_file: FalAiWanTrainerFlf2V720pFile;
    /**
     * Config File
     *
     * Configuration used for setting up the inference endpoints.
     */
    config_file: FalAiWanTrainerFlf2V720pFile;
};

/**
 * File
 */
export type FalAiWanTrainerFlf2V720pFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
    /**
     * File Data
     *
     * File data
     */
    file_data?: Blob | File;
};

/**
 * Input
 */
export type LtxVideoTrainerInput = {
    /**
     * Number Of Steps
     *
     * The number of steps to train for.
     */
    number_of_steps?: number;
    /**
     * Frame Rate
     *
     * The target frames per second for the video.
     */
    frame_rate?: number;
    /**
     * Learning Rate
     *
     * The rate at which the model learns. Higher values can lead to faster training, but over-fitting.
     */
    learning_rate?: number;
    /**
     * Validation
     *
     * A list of validation prompts to use during training. When providing an image, _all_ validation inputs must have an image.
     */
    validation?: Array<FalAiLtxVideoTrainerValidation>;
    /**
     * Number Of Frames
     *
     * The number of frames to use for training. This is the number of frames per second multiplied by the number of seconds.
     */
    number_of_frames?: number;
    /**
     * Validation Reverse
     *
     * If true, the validation videos will be reversed. This is useful for effects that are learned in reverse and then applied in reverse.
     */
    validation_reverse?: boolean;
    /**
     * Training Data Url
     *
     * URL to zip archive with videos or images. Try to use at least 10 files, although more is better.
     *
     * **Supported video formats:** .mp4, .mov, .avi, .mkv
     * **Supported image formats:** .png, .jpg, .jpeg
     *
     * Note: The dataset must contain ONLY videos OR ONLY images - mixed datasets are not supported.
     *
     * The archive can also contain text files with captions. Each text file should have the same name as the media file it corresponds to.
     */
    training_data_url: string;
    /**
     * Split Input Duration Threshold
     *
     * The duration threshold in seconds. If a video is longer than this, it will be split into scenes. If you provide captions for a split video, the caption will be applied to each scene. If you do not provide captions, scenes will be auto-captioned.
     */
    split_input_duration_threshold?: number;
    /**
     * Rank
     *
     * The rank of the LoRA.
     */
    rank?: 8 | 16 | 32 | 64 | 128;
    /**
     * Aspect Ratio
     *
     * The aspect ratio to use for training. This is the aspect ratio of the video.
     */
    aspect_ratio?: '16:9' | '1:1' | '9:16';
    /**
     * Trigger Phrase
     *
     * The phrase that will trigger the model to generate an image.
     */
    trigger_phrase?: string;
    /**
     * Resolution
     *
     * The resolution to use for training. This is the resolution of the video.
     */
    resolution?: 'low' | 'medium' | 'high';
    /**
     * Split Input Into Scenes
     *
     * If true, videos above a certain duration threshold will be split into scenes. If you provide captions for a split video, the caption will be applied to each scene. If you do not provide captions, scenes will be auto-captioned. This option has no effect on image datasets.
     */
    split_input_into_scenes?: boolean;
    /**
     * Validation Resolution
     *
     * The resolution to use for validation.
     */
    validation_resolution?: 'low' | 'medium' | 'high';
    /**
     * Validation Number Of Frames
     *
     * The number of frames to use for validation.
     */
    validation_number_of_frames?: number;
    /**
     * Validation Aspect Ratio
     *
     * The aspect ratio to use for validation.
     */
    validation_aspect_ratio?: '16:9' | '1:1' | '9:16';
    /**
     * Validation Negative Prompt
     *
     * A negative prompt to use for validation.
     */
    validation_negative_prompt?: string;
    /**
     * Auto Scale Input
     *
     * If true, videos will be automatically scaled to the target frame count and fps. This option has no effect on image datasets.
     */
    auto_scale_input?: boolean;
};

/**
 * TrainingOutput
 */
export type LtxVideoTrainerOutput = {
    lora_file: FalAiLtxVideoTrainerFile;
    config_file: FalAiLtxVideoTrainerFile;
    /**
     * The URL to the validations video.
     */
    video: FalAiLtxVideoTrainerFile | unknown;
};

/**
 * Validation
 */
export type FalAiLtxVideoTrainerValidation = {
    /**
     * Prompt
     *
     * The prompt to use for validation.
     */
    prompt: string;
    /**
     * Image Url
     *
     * An image to use for image-to-video validation. If provided for one validation, _all_ validation inputs must have an image.
     */
    image_url?: string | unknown;
};

/**
 * File
 */
export type FalAiLtxVideoTrainerFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number | unknown;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string | unknown;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string | unknown;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
};

/**
 * StyleReferenceInput
 */
export type RecraftV3CreateStyleInput = {
    /**
     * Images Data Url
     *
     * URL to zip archive with images, use PNG format. Maximum 5 images are allowed.
     */
    images_data_url: string;
    /**
     * Base Style
     *
     * The base style of the generated images, this topic is covered above.
     */
    base_style?: 'any' | 'realistic_image' | 'digital_illustration' | 'vector_illustration' | 'realistic_image/b_and_w' | 'realistic_image/hard_flash' | 'realistic_image/hdr' | 'realistic_image/natural_light' | 'realistic_image/studio_portrait' | 'realistic_image/enterprise' | 'realistic_image/motion_blur' | 'realistic_image/evening_light' | 'realistic_image/faded_nostalgia' | 'realistic_image/forest_life' | 'realistic_image/mystic_naturalism' | 'realistic_image/natural_tones' | 'realistic_image/organic_calm' | 'realistic_image/real_life_glow' | 'realistic_image/retro_realism' | 'realistic_image/retro_snapshot' | 'realistic_image/urban_drama' | 'realistic_image/village_realism' | 'realistic_image/warm_folk' | 'digital_illustration/pixel_art' | 'digital_illustration/hand_drawn' | 'digital_illustration/grain' | 'digital_illustration/infantile_sketch' | 'digital_illustration/2d_art_poster' | 'digital_illustration/handmade_3d' | 'digital_illustration/hand_drawn_outline' | 'digital_illustration/engraving_color' | 'digital_illustration/2d_art_poster_2' | 'digital_illustration/antiquarian' | 'digital_illustration/bold_fantasy' | 'digital_illustration/child_book' | 'digital_illustration/child_books' | 'digital_illustration/cover' | 'digital_illustration/crosshatch' | 'digital_illustration/digital_engraving' | 'digital_illustration/expressionism' | 'digital_illustration/freehand_details' | 'digital_illustration/grain_20' | 'digital_illustration/graphic_intensity' | 'digital_illustration/hard_comics' | 'digital_illustration/long_shadow' | 'digital_illustration/modern_folk' | 'digital_illustration/multicolor' | 'digital_illustration/neon_calm' | 'digital_illustration/noir' | 'digital_illustration/nostalgic_pastel' | 'digital_illustration/outline_details' | 'digital_illustration/pastel_gradient' | 'digital_illustration/pastel_sketch' | 'digital_illustration/pop_art' | 'digital_illustration/pop_renaissance' | 'digital_illustration/street_art' | 'digital_illustration/tablet_sketch' | 'digital_illustration/urban_glow' | 'digital_illustration/urban_sketching' | 'digital_illustration/vanilla_dreams' | 'digital_illustration/young_adult_book' | 'digital_illustration/young_adult_book_2' | 'vector_illustration/bold_stroke' | 'vector_illustration/chemistry' | 'vector_illustration/colored_stencil' | 'vector_illustration/contour_pop_art' | 'vector_illustration/cosmics' | 'vector_illustration/cutout' | 'vector_illustration/depressive' | 'vector_illustration/editorial' | 'vector_illustration/emotional_flat' | 'vector_illustration/infographical' | 'vector_illustration/marker_outline' | 'vector_illustration/mosaic' | 'vector_illustration/naivector' | 'vector_illustration/roundish_flat' | 'vector_illustration/segmented_colors' | 'vector_illustration/sharp_contrast' | 'vector_illustration/thin' | 'vector_illustration/vector_photo' | 'vector_illustration/vivid_shapes' | 'vector_illustration/engraving' | 'vector_illustration/line_art' | 'vector_illustration/line_circuit' | 'vector_illustration/linocut';
};

/**
 * StyleReferenceOutput
 */
export type RecraftV3CreateStyleOutput = {
    /**
     * Style Id
     *
     * The ID of the created style, this ID can be used to reference the style in the future.
     */
    style_id: string;
};

/**
 * Input
 */
export type TurboFluxTrainerInput = {
    /**
     * Images Data Url
     *
     *
     * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
     *
     */
    images_data_url: string;
    /**
     * Trigger Phrase
     *
     * Trigger phrase to be used in the captions. If None, a trigger word will not be used.
     * If no captions are provide the trigger_work will be used instead of captions. If captions are provided, the trigger word will replace the `[trigger]` string in the captions.
     *
     */
    trigger_phrase?: string;
    /**
     * Steps
     *
     * Number of steps to train the LoRA on.
     */
    steps?: number;
    /**
     * Learning Rate
     *
     * Learning rate for the training.
     */
    learning_rate?: number;
    /**
     * Training Style
     *
     * Training style to use.
     */
    training_style?: 'subject' | 'style';
    /**
     * Face Crop
     *
     * Whether to try to detect the face and crop the images to the face.
     */
    face_crop?: boolean;
};

/**
 * Output
 */
export type TurboFluxTrainerOutput = {
    /**
     * Config File
     *
     * URL to the trained diffusers config file.
     */
    config_file: FalAiTurboFluxTrainerFile;
    /**
     * Diffusers Lora File
     *
     * URL to the trained diffusers lora weights.
     */
    diffusers_lora_file: FalAiTurboFluxTrainerFile;
};

/**
 * File
 */
export type FalAiTurboFluxTrainerFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
    /**
     * File Data
     *
     * File data
     */
    file_data?: Blob | File;
};

/**
 * Input
 */
export type WanTrainerInput = {
    /**
     * Number Of Steps
     *
     * The number of steps to train for.
     */
    number_of_steps?: number;
    /**
     * Training Data URL
     *
     * URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.
     *
     * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.
     */
    training_data_url: string;
    /**
     * Trigger Phrase
     *
     * The phrase that will trigger the model to generate an image.
     */
    trigger_phrase?: string;
    /**
     * Learning Rate
     *
     * The rate at which the model learns. Higher values can lead to faster training, but over-fitting.
     */
    learning_rate?: number;
    /**
     * Auto-Scale Input
     *
     * If true, the input will be automatically scale the video to 81 frames at 16fps.
     */
    auto_scale_input?: boolean;
};

/**
 * Output
 */
export type WanTrainerOutput = {
    /**
     * Lora File
     *
     * URL to the trained LoRA weights.
     */
    lora_file: FalAiWanTrainerFile;
    /**
     * Config File
     *
     * Configuration used for setting up the inference endpoints.
     */
    config_file: FalAiWanTrainerFile;
};

/**
 * File
 */
export type FalAiWanTrainerFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
    /**
     * File Data
     *
     * File data
     */
    file_data?: Blob | File;
};

/**
 * PublicInput
 */
export type HunyuanVideoLoraTrainingInput = {
    /**
     * Trigger Word
     *
     * The trigger word to use.
     */
    trigger_word?: string;
    /**
     * Images Data Url
     *
     *
     * URL to zip archive with images. Try to use at least 4 images in general the more the better.
     *
     * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.
     *
     */
    images_data_url: string;
    /**
     * Steps
     *
     * Number of steps to train the LoRA on.
     */
    steps: number;
    /**
     * Data Archive Format
     *
     * The format of the archive. If not specified, the format will be inferred from the URL.
     */
    data_archive_format?: string | unknown | null;
    /**
     * Learning Rate
     *
     * Learning rate to use for training.
     */
    learning_rate?: number;
    /**
     * Do Caption
     *
     * Whether to generate captions for the images.
     */
    do_caption?: boolean;
};

/**
 * Output
 */
export type HunyuanVideoLoraTrainingOutput = {
    config_file: FalAiHunyuanVideoLoraTrainingFile;
    diffusers_lora_file: FalAiHunyuanVideoLoraTrainingFile;
};

/**
 * File
 */
export type FalAiHunyuanVideoLoraTrainingFile = {
    /**
     * File Size
     *
     * The size of the file in bytes.
     */
    file_size?: number | unknown;
    /**
     * File Name
     *
     * The name of the file. It will be auto-generated if not provided.
     */
    file_name?: string | unknown;
    /**
     * Content Type
     *
     * The mime type of the file.
     */
    content_type?: string | unknown;
    /**
     * Url
     *
     * The URL where the file can be downloaded from.
     */
    url: string;
};
