// This file is auto-generated by @hey-api/openapi-ts

export type ClientOptions = {
    baseUrl: `${string}://${string}` | (string & {});
};

export type File = {
    url: string;
    content_type?: string;
    file_name?: string;
    file_size?: number;
};

export type QueueStatus = {
    status: 'IN_PROGRESS' | 'COMPLETED' | 'FAILED';
    response_url?: string;
};

export type RouterOpenaiV1ResponsesOutput = unknown;

export type RouterOpenaiV1EmbeddingsOutput = unknown;

/**
 * ChatInput
 */
export type RouterInput = {
    /**
     * Model
     *
     * Name of the model to use. Charged based on actual token usage.
     */
    model: string;
    /**
     * Prompt
     *
     * Prompt to be used for the chat completion
     */
    prompt: string;
    /**
     * Max Tokens
     *
     * This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.
     */
    max_tokens?: number;
    /**
     * Temperature
     *
     * This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input.
     */
    temperature?: number;
    /**
     * System Prompt
     *
     * System prompt to provide context or instructions to the model
     */
    system_prompt?: string;
    /**
     * Reasoning
     *
     * Should reasoning be the part of the final answer.
     */
    reasoning?: boolean;
};

/**
 * ChatOutput
 */
export type RouterOutput = {
    /**
     * Usage
     *
     * Token usage information
     */
    usage?: UsageInfo;
    /**
     * Error
     *
     * Error message if an error occurred
     */
    error?: string;
    /**
     * Partial
     *
     * Whether the output is partial
     */
    partial?: boolean;
    /**
     * Reasoning
     *
     * Generated reasoning for the final answer
     */
    reasoning?: string;
    /**
     * Output
     *
     * Generated output
     */
    output: string;
};

/**
 * UsageInfo
 */
export type UsageInfo = {
    /**
     * Prompt Tokens
     */
    prompt_tokens?: number;
    /**
     * Total Tokens
     */
    total_tokens?: number;
    /**
     * Completion Tokens
     */
    completion_tokens?: number;
    /**
     * Cost
     */
    cost: number;
};

export type RouterOpenaiV1ChatCompletionsOutput = unknown;

/**
 * Qwen3GuardInput
 */
export type Qwen3GuardInput = {
    /**
     * Prompt
     *
     * The input text to be classified
     */
    prompt: string;
};

/**
 * Qwen3GuardOutput
 */
export type Qwen3GuardOutput = {
    /**
     * Categories
     *
     * The confidence score of the classification
     */
    categories: Array<'Violent' | 'Non-violent Illegal Acts' | 'Sexual Content or Sexual Acts' | 'PII' | 'Suicide & Self-Harm' | 'Unethical Acts' | 'Politically Sensitive Topics' | 'Copyright Violation' | 'Jailbreak' | 'None'>;
    /**
     * Label
     *
     * The classification label
     */
    label: 'Safe' | 'Unsafe' | 'Controversial';
};

/**
 * InputModel
 */
export type VideoPromptGeneratorInput = {
    /**
     * Custom Elements
     *
     * Custom technical elements (optional)
     */
    custom_elements?: string;
    /**
     * Style
     *
     * Style of the video prompt
     */
    style?: 'Minimalist' | 'Simple' | 'Detailed' | 'Descriptive' | 'Dynamic' | 'Cinematic' | 'Documentary' | 'Animation' | 'Action' | 'Experimental';
    /**
     * Camera Direction
     *
     * Camera direction
     */
    camera_direction?: 'None' | 'Zoom in' | 'Zoom out' | 'Pan left' | 'Pan right' | 'Tilt up' | 'Tilt down' | 'Orbital rotation' | 'Push in' | 'Pull out' | 'Track forward' | 'Track backward' | 'Spiral in' | 'Spiral out' | 'Arc movement' | 'Diagonal traverse' | 'Vertical rise' | 'Vertical descent';
    /**
     * Pacing
     *
     * Pacing rhythm
     */
    pacing?: 'None' | 'Slow burn' | 'Rhythmic pulse' | 'Frantic energy' | 'Ebb and flow' | 'Hypnotic drift' | 'Time-lapse rush' | 'Stop-motion staccato' | 'Gradual build' | 'Quick cut rhythm' | 'Long take meditation' | 'Jump cut energy' | 'Match cut flow' | 'Cross-dissolve dreamscape' | 'Parallel action' | 'Slow motion impact' | 'Ramping dynamics' | 'Montage tempo' | 'Continuous flow' | 'Episodic breaks';
    /**
     * Special Effects
     *
     * Special effects approach
     */
    special_effects?: 'None' | 'Practical effects' | 'CGI enhancement' | 'Analog glitches' | 'Light painting' | 'Projection mapping' | 'Nanosecond exposures' | 'Double exposure' | 'Smoke diffusion' | 'Lens flare artistry' | 'Particle systems' | 'Holographic overlay' | 'Chromatic aberration' | 'Digital distortion' | 'Wire removal' | 'Motion capture' | 'Miniature integration' | 'Weather simulation' | 'Color grading' | 'Mixed media composite' | 'Neural style transfer';
    /**
     * Image Url
     *
     * URL of an image to analyze and incorporate into the video prompt (optional)
     */
    image_url?: string;
    /**
     * Model
     *
     * Model to use
     */
    model?: 'anthropic/claude-3.5-sonnet' | 'anthropic/claude-3-5-haiku' | 'anthropic/claude-3-haiku' | 'google/gemini-2.5-flash-lite' | 'google/gemini-2.0-flash-001' | 'meta-llama/llama-3.2-1b-instruct' | 'meta-llama/llama-3.2-3b-instruct' | 'meta-llama/llama-3.1-8b-instruct' | 'meta-llama/llama-3.1-70b-instruct' | 'openai/gpt-4o-mini' | 'openai/gpt-4o' | 'deepseek/deepseek-r1';
    /**
     * Camera Style
     *
     * Camera movement style
     */
    camera_style?: 'None' | 'Steadicam flow' | 'Drone aerials' | 'Handheld urgency' | 'Crane elegance' | 'Dolly precision' | 'VR 360' | 'Multi-angle rig' | 'Static tripod' | 'Gimbal smoothness' | 'Slider motion' | 'Jib sweep' | 'POV immersion' | 'Time-slice array' | 'Macro extreme' | 'Tilt-shift miniature' | 'Snorricam character' | 'Whip pan dynamics' | 'Dutch angle tension' | 'Underwater housing' | 'Periscope lens';
    /**
     * Input Concept
     *
     * Core concept or thematic input for the video prompt
     */
    input_concept: string;
    /**
     * Prompt Length
     *
     * Length of the prompt
     */
    prompt_length?: 'Short' | 'Medium' | 'Long';
};

/**
 * OutputModel
 */
export type VideoPromptGeneratorOutput = {
    /**
     * Prompt
     *
     * Generated video prompt
     */
    prompt: string;
};
